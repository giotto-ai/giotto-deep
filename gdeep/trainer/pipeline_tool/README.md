# Pipeline tool

## Table of contents

[toc]

## Introduction

The field of machine learning is constantly evolving, with increasingly sophisticated models and ever-expanding datasets. Significant challenges can arise for professionals in the field, especially when it comes to training models that are too large to fit into the memory of a single GPU.

In this context, a tool has been developed to distribute a PyTorch machine learning model across multiple GPUs without altering the training process. The PyTorch model description is taken as input, each layer is interpreted independently, and the model is rewritten to handle the operations interdependencies. The result is a new model that can be automatically distributed (creating a pileline) across multiple GPUs that does not affects the results quality.

In the following, this tool will be presented in detail, along with the benefits it can provide to machine learning professionals seeking to train large and complex models.

## How it works

### First step

To run the tool, you have to provide some parameters:

- Number of GPUs: If the user does not specify the number of GPUs to use, the tool will automatically detect the available GPUs on the machine running the command. In this case, the model will be trained using all detected GPUs to improve performance.
- PyTorch Model: The user must provide a PyTorch model that only uses functions and modules coming from the PyTorch API. In other words, the model should not incorporate custom functions unknown to the PyTorch API. However, it is entirely possible to create custom layers using combinations of functions (always from the PyTorch API).
- Shapes of the input and output: This will be needed to profile [memory usage](#model-splitting).

The first step is adding the following imports in your project:

```python
from torch.distributed.pipeline.sync import Pipe 
from pipeline_tool.pipeline_tool import SkippableTracing
```

Once you have defined the desired model, you can process it by following these steps:

```python
N_GPUs = 2
trace = SkippableTracing(N_GPUs, model, input_shape, output_shape)
graph_model = trace.get_modules()
```

Here the model is traced using [torch.fx](https://pytorch.org/docs/stable/fx.html) to obtain the GraphModule. This allows us to determine, for each module, its type, parameters, functions (e.g., convolution, activation, multiplication) and their links to others modules.

Bellow an example of how is treated a simple model : 

![03_simple_model_dep](img/03_simple_model_dep.png)

In this basic example, we have a model composed exclusively of PyTorch modules. To describe them accurately, we utilize the trace generated by torch fx. 

The generated trace appears as follows:

```bash
Opcode          Name            Target         
placeholder     x               x              
call_module     linear1         linear1        
call_module     activation      activation     
call_module     linear2         linear2        
call_module     softmax         softmax        
output          output          output  
```

This trace allows us to identify each generated layer and provides the following information:

- Opcode: Indicates the type of operation performed by the layer.
- Name: Corresponds to the name of the function or operation performed by the layer.
- Target: Represents the name of the layer as it appears in the description of the PyTorch model.

Thus, the trace provides a detailed view of the operations performed by each layer, making it easier to understand and analyze the model.

```bash
Name            Module declaration
linear1         Linear(in_features=100, out_features=200, bias=True)
activation      ReLU()         
linear2         Linear(in_features=200, out_features=10, bias=True)
softmax         Softmax(dim=None)
```

The retrieval, analysis, and management of all this information enable the generation of a file containing a new model ready for pipelined training on N GPUs.

### Complex Models

Unfortunately, a model is never limited to a simple linear sequence of modules taking the output of the previous operation as input... More complex models exist, and it is necessary to handle all possible cases, to trace the model correctly so that it is faithfully reproduced without omitting certain operations.

As a result, it is necessary to distinguish PyTorch modules from other elements.

We analyze the model received as a parameter and store the elements by their names in a dictionary, which we use to create a correspondence table with the names given by the trace.

We can then iterate over the generated trace to differentiate five types of layers:

1. <u>**Module**</u>: These need to be initialized and thus require their description to be retrieved from the original model.
2. <u>**Function**</u>: These correspond to simple PyTorch functions executed between tensors or on a tensor (e.g., additions, dimension reductions, etc.).
3. <u>**Getitem**</u>: These appear in the trace when only a portion of a result in the form of a list needs to be retrieved (e.g., index 1 of a list or the first return value of a function).
4. <u>**Getattr**</u>: These correspond to retrieving an attribute of a tensor.
5. <u>**Propagation**</u>: These appear in the trace to propagate tensors to other layers.

#### call_function

Let's explore the concept of call_function with the widely known ResNet model.

When we examine the generated trace, we notice a new opcode, distinct from the one we previously discussed in the [first step](#first-step) : 

```bash
Opcode               Name                           Arguments                     
placeholder          x                              ()                            
call_module          flow_0_conv1                   (x,)                          
[...]  
call_module          flow_0_avgpool                 (flow_0_layer4_1_relu_1,)   
# ############################################################################################
call_function        flatten                        (flow_0_avgpool, 1) 
# ############################################################################################
call_module          flow_0_fc                      (flatten,)                    
call_module          flow_1                         (flow_0_fc,)                  
output               output                         (flow_1,)    
```
*Notice that we also have access to the input arguments of each layer.*

call_functions are treated differently from call_modules and consequently generate distinct code. Therefore, each call_function is declared as a Torch module that exclusively performs the necessary operation. In the case of the previous trace, let's consider the declaration of the call_function `flatten`:

```python
class flatten_layer(nn.Module):
    def forward(self, input):
        ret = torch.flatten(input, 1)
        return ret
```

Functions do not necessitate an initialization function. Instead, our tool seeks out the appropriate Torch function based on the name provided in the trace. For instance, when working with the instantiated ResNet18 model, the function "flatten" already exists within the Torch API.

The trace allows us to identify the arguments passed to this function. In the case above, the inputs are the output of the previous layer and the integer "1".

#### Propagation

As discussed in the section on [complex models](#complex-models) there are instances where we need to transmit the output of one layer to others that are not inherently connected to it. To facilitate this process, PyTorch provides a useful decorator called "skippable." This decorator introduces two key features:

1. `stash`: This feature permits us to store a specific value with an associated name, allowing for convenient retrieval later.

2. `pop`: With this functionality,

Let's get a look into an example trace to have a better understanding::

```bash
Opcode               Name                           Arguments                     
placeholder          x                              ()                            
call_module          flow_0_conv1                   (x,)                          
[...]              
call_module          flow_0_maxpool                 (flow_0_relu,)                
call_module          flow_0_layer1_0_conv1          (flow_0_maxpool,)             
call_module          flow_0_layer1_0_bn1            (flow_0_layer1_0_conv1,)      
call_module          flow_0_layer1_0_relu           (flow_0_layer1_0_bn1,)        
call_module          flow_0_layer1_0_conv2          (flow_0_layer1_0_relu,)       
call_module          flow_0_layer1_0_bn2            (flow_0_layer1_0_conv2,) 
#############################################################################################
call_function        add                            (flow_0_layer1_0_bn2, flow_0_maxpool)
#############################################################################################
call_module          flow_0_layer1_0_relu_1         (add,)                        
[...]          
call_module          flow_0_fc                      (flatten,)                    
call_module          flow_1                         (flow_0_fc,)                  
output               output                         (flow_1,)    
```

The call_function surrounded have two name in input : 
- flow_0_layer1_0_bn2, which directly stems from the previous layer.
- flow_0_maxpool, originating from an earlier layer in the model.

Our tool is designed to establish connections between layers and retain information about the arguments derived from prior layers. 

Consequently, when utilizing the skippable decorator in the generated code:

```python
[...]

@skippable(stash=['flow_0_maxpool_to_add'])
class flow_0_maxpool_layer(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.fc = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    def forward(self, input):
        ret = self.fc(input)
        yield stash('flow_0_maxpool_to_add', ret)
        return ret

[...]

class flow_0_layer1_0_bn2_layer(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.fc = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    def forward(self, input):
        ret = self.fc(input)
        return ret

@skippable(pop=['flow_0_maxpool_to_add'])
class add_layer(nn.Module):
    def forward(self, input):
        flow_0_maxpool = yield pop('flow_0_maxpool_to_add')
        ret = torch.add(input, flow_0_maxpool)
        return ret
    
[...]
```
We ensure that the dependencies between layers are properly preserved.


#### Getitem

Within the trace, certain call_function entries contain the term "getitem" in their names. This indicates that these are not conventional functions but rather indicate the need to access a specific index within a result. Consider the following trace as an example:

```bash
[...]
call_function        getattr_1            (add_3, 'shape')              
call_function        getitem_4            (getattr_1, 0)                
[...]
```

Here, we notice the presence of a getitem operation, which is applied to the result of the previous layer. If we were to translate this trace, it would resemble something like add_3.shape[0] (for an explanation of getattr, please refer to the [next point](#getattr)).

The challenge with getitem lies in the limitation of the Torch API, which does not allow the propagation of non-tensor values. Consequently, we must concatenate the getitem operation to the layer from which we require the value, rather than creating an independent layer that cannot effectively transmit its output.

#### GetAttr

There are two distinct types of `getattr` operations: 

1. **call_function with the Name "getattr"**: These instances occur when an attribute of modules needs to be accessed..
   
   In the provided trace:

   ```bash
   [...]
   call_function        getattr_1                                         (add_3, 'shape') 
   [...]
   call_module          model_pooling_layer_scaled_dot_product_attention  (expand, add_3, add_3)  
   ```

   As previously mentioned, we cannot propagate non-tensor values. The presence of getattr indicates the need to access a specific attribute within a module. In the trace above, the tensor add_3 possesses an attribute "shape" that will be utilized. In such cases, we refrain from creating new modules; instead, we reference the relevant attribute of the tensor when it is passed as a parameter.

   Here's an illustrative example of generated code to elucidate this approach:

   ```python
   [...]
   @skippable(stash=['add_3_to_expand', 'add_3_to_model_pooling_layer_scaled_dot_product_attention'], pop=['add_2_to_add_3'])
   class add_3_layer(nn.Module):
       def forward(self, input):
           add_2 = yield pop('add_2_to_add_3')
           ret = torch.add(input, add_2)
           yield stash('add_3_to_expand', ret)
           yield stash('add_3_to_model_pooling_layer_scaled_dot_product_attention', ret)
           return ret
   [...]
   @skippable(pop=['add_3_to_expand'])
   class expand_layer(nn.Module):
       def forward(self, input):
           add_3 = yield pop('add_3_to_expand')
           ret = input.expand(add_3.shape[0], -1, -1)
           return ret
   ```


2. **get_attr with the Opcode "get_attr"**: These occurrences arise when a private attribute of a user-created class is requested.

   In the provided trace:
   ```bash
   get_attr       model_pooling_layer_query           ()
   ```

   We only have the name of the attribute, and it needs to be initialized to propagate or utilize it, we create a module that initializes the attribute based on the provided information. We search for the attribute on the given model and recreate it identically.

   Here's an example of code to illustrate this process:
   
   ```python
   class model_pooling_layer_query_layer(nn.Module):
       def __init__(self) -> None:
           super().__init__()
           self.fc = nn.parameter.Parameter(torch.Tensor(1, 16), requires_grad=True)
       def forward(self, input):
           ret = self.fc
           return ret
   ```

#### MultiHeadAttention processing

Unpredictable management is, however, necessary for MultiHeadAttention. During the module declaration retrieval phase, it is impossible to retrieve those of the MultiHeadAttention. Therefore, the user must provide a dictionary containing the description of all the parameters and their values for the MultiHeadAttention of their model during the tool's initialization.

At a minimum, the following parameters must be provided for a MultiHead:

- embed_dim
- num_heads

And the initialization would be changed to:

```python
configs = [{'embed_dim': hidden_val, 'num_heads': heads_val, 'dropout': 0.1, 'batch_first': True},
            {'embed_dim': hidden_val, 'num_heads': heads_val, 'dropout': 0.1, 'batch_first': True},
            {'embed_dim': hidden_val, 'num_heads': heads_val, 'dropout': 0.1, 'batch_first': True},
            {'embed_dim': hidden_val, 'num_heads': heads_val, 'dropout': 0.1, 'batch_first': True},
            {'embed_dim': hidden_val, 'num_heads': heads_val, 'dropout': 0.1, 'batch_first': True}]
nb_gpus = 2
trace = SkippableTracing(nb_gpus, model, configs)
model_pipe = trace.get_modules()
```

### Model splitting

Now that we are capable of creating a model that can be distributed across multiple GPUs, the question arises: how do we split it intelligently? Currently, the tool proceeds with a somewhat naive approach. We create a dummy dataset to pass through the model and perform a training run. This allows us to measure the memory loads on all GPUs.

Initially, the tool divides the layers into two equal parts (in terms of the number of layers) and conducts these memory load measurements.
If the load is not evenly distributed, we re-write the model (moving layers around) and iterate the dummy run, until we achieving uniform  distribution on N GPUs.


## Pipeline Tool x Giotto Deep
The Pipeline tool is seamlessly integrated into Giotto-Deep's trainer, requiring no changes to their API. To activate the pipeline mode and configure MultiHeadAttention if your model includes it, simply provide a boolean flag and the MHA configuration.

Here's an example: 

```python
trainer = Trainer(wrapped_model, [dl_train, dl_train], loss_function, writer) 

configs = [{'embed_dim': 16, 'num_heads': 8, 'dropout': 0.1, 'batch_first': True},
        {'embed_dim': 16, 'num_heads': 8, 'dropout': 0.1, 'batch_first': True},
        {'embed_dim': 16, 'num_heads': 8, 'dropout': 0.1, 'batch_first': True},
        {'embed_dim': 16, 'num_heads': 8, 'dropout': 0.1, 'batch_first': True},
        {'embed_dim': 16, 'num_heads': 8, 'dropout': 0.1, 'batch_first': True}]

n_epoch = 1

trainer.train(Adam, n_epoch, pipeline_train=True, config_mha=configs, nb_chunks=2)
```

### Example
To experiment with Giotto Deep training using the Pipeline tool in your environment, two example scripts have been provided. Navigate to Giotto's examples folder and run either "pipeline_basic_image.py" or "pipeline_orbit5k.py" with the --pipeline argument to enable the pipeline mode, or without it for regular training.

## Profiling

## Improvements
1. Although repartition is currently performed, it is unnecessary when the model fits within a single GPU. The process should automatically avoid splitting when feasible, requiring an initial run on the largest GPU and an error-handling mechanism.
2. Replace the rudimentary repartition method with a more efficient approach, such as employing a dichotomous search.
3. Incorporate profiling results into the documentation for better performance analysis.
4. Explore the possibility of eliminating the requirement for a config_mha when users opt for the default MHA. In cases where most users utilize the same MHA, removing the mandatory configuration can simplify the process.