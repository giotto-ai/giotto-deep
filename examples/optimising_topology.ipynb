{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Differentiating persistent diagrams\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "The goal of this notebook is to showcase the possibility in giotto-deep to differeentiate persistence diagrams with respect to the pointcloud.\n",
    "\n",
    "Here below I will describe the theory behind the differentiation algorithm.\n",
    "\n",
    "## Notation\n",
    "\n",
    " - $K$ is the simplicial complex\n",
    "\n",
    " - $p$ is the number of finite bars (generated by *positive simplices* and eliminated by *negative simplices*)\n",
    "\n",
    " - $q$ is the number of infinitely persistent features.\n",
    "\n",
    "This holds: $|K| = 2p + q$\n",
    "\n",
    "## Description of the problem\n",
    "\n",
    "Let us focus on VR, as the other filtrations are done similarly.\n",
    "\n",
    "The first step is to notice that a filtration can be seen as a map \n",
    "$\\Phi: A \\to \\mathbb R^{|K|} $. \n",
    "\n",
    "Given a point cloud with n points and d features, the space A can be seen as \n",
    "$A=(\\mathbb R^d)^n$. \n",
    "\n",
    "The map $\\Phi$ is given explicitly: given X a point cloud (i.e. an element of $(\\mathbb R^d)^n$), \n",
    "\n",
    "$\\Phi_{\\sigma}(X)=max_{i,j \\in \\sigma}||x_i-x_j||.$ \n",
    "\n",
    "This is the $\\sigma$ component of the whole $\\Phi$.\n",
    "\n",
    "Here sigma is a simplex, i.e. a coordinate of $\\mathbb R^{|K|}$.\n",
    "\n",
    "A persistent diagram requires:\n",
    "1. The identification of $\\mathbb R^{|K|} =(\\mathbb R^2)^p \\times \\mathbb R^q$\n",
    "2. The pairing of positive with negative simplices and the identification of unpaired positive simples.\n",
    "\n",
    "Hence \n",
    "\n",
    "$Pers:Filt_K \\subset \\mathbb R^{|K|} \\to (\\mathbb R^2)^p \\times \\mathbb R^q, \\Phi(X) \\mapsto D = \\cup_i^p (\\Phi_{\\sigma_{i_1}}(X) , \\Phi_{\\sigma_{i_2}}(X) ) \\times \\cup_j^q (\\Phi_{\\sigma_j}(X),+\\infty).$\n",
    "\n",
    "Of course, $|K|=2p + q$.\n",
    "\n",
    "Finally, we can also define persistence functions: they are functions like $E: (\\mathbb R^2)^p \\times \\mathbb R^q \\to \\mathbb R$, invariant under permutations of $p$ and $q$ .\n",
    "e.g.\n",
    "$E(D)=\\sum_i^p|d_i-b_i|^2$\n",
    "\n",
    "We can now define a loss function $L:= E.Pers.\\Phi : A \\to \\mathbb R, A = (\\mathbb R^d)^n$ as before.\n",
    "\n",
    "Can we compute the gradient of $L$ with respect to the point cloud? Observe that Pers is merely a permutation of the coordinates, thus its partial derivatives w.r.t. the filtration are either 1 or 0.\n",
    "Thus, since all the components of $L$ are differentiable, so is $L$ by Leibnitz rule.\n",
    "One can implement it is Pytorch using `autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from itertools import chain, combinations\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from gtda.homology import VietorisRipsPersistence as vrp\n",
    "from gtda.homology import FlagserPersistence as flp\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gdeep.utility.optimization import PersistenceGradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Build the point cloud\n",
    "We start with a random 2D point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "hom_dim = (0, 1, 2)\n",
    "Xp = torch.rand((10, 4), requires_grad=False)\n",
    "X_arr = Xp.detach().numpy().copy()\n",
    "df = pd.DataFrame(Xp, columns=[\"x\" + str(jj) for jj in range(len(Xp[0]))])\n",
    "\n",
    "px.scatter(df, x=\"x0\", y=\"x1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SGD over topology!\n",
    "\n",
    "To gradient descend (for the moment, the algorithm is not stochastic) over the topological loss function $L=-\\sum_i^p |\\epsilon_{i2}-\\epsilon_{i1}|+ \\lambda \\sum_{x \\in X} ||x||_2^2$ it is enough to initialise the \n",
    "`PersistenceGradient` class and run the `SGD()` method. For clarity, $\\epsilon_{i1}$ is the filtration value of the $i$-th *positive simplex*, while $\\epsilon_{i2}$ of the $i$-th *negative simplex*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pg = PersistenceGradient(\n",
    "    homology_dimensions=hom_dim, zeta=0.1, max_edge_length=0.5, collapse_edges=False\n",
    ")\n",
    "fig, fig3d, loss_val = pg.sgd(Xp, 0.07, 32)\n",
    "\n",
    "# plot of the evolution over the GD iterations\n",
    "fig.show()\n",
    "# plot of the evolution over the GD iterations in 3D\n",
    "# fig3d.show()\n",
    "# plot of the loss function\n",
    "px.line(loss_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    Xp.detach(), columns=[\"x\" + str(jj) for jj in range(len(Xp.detach()[0]))]\n",
    ")\n",
    "\n",
    "px.scatter(df, x=\"x0\", y=\"x1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comparison between persistence diagrams\n",
    "Plot the PD at the beginning and at the end of the optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot persistence diagram\n",
    "vr = vrp(homology_dimensions=hom_dim)\n",
    "\n",
    "try:\n",
    "    vr.fit_transform_plot([X_arr])\n",
    "except ValueError:\n",
    "    warnings.warn(\"Most likely the array is empty...\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    vr.fit_transform_plot([Xp.detach()])\n",
    "except ValueError:\n",
    "    warnings.warn(\"Most likely the array is empty...\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application to weighterd graphs\n",
    "\n",
    "The algorithm can be applied as is to weighted graphs as well.\n",
    "\n",
    "Import the weighted graph as a square tensor, where the entry $(i,j)$ is the weight of the edge $i \\to j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = torch.rand((11, 11))  # simulate the weighted graph\n",
    "dist_arr = dist.detach().numpy().copy()\n",
    "pg = PersistenceGradient(\n",
    "    homology_dimensions=hom_dim, zeta=0.0, collapse_edges=False, metric=\"precomputed\"\n",
    ")\n",
    "fig, fig3d, loss_val = pg.sgd(dist, 0.1, 10)\n",
    "# plot of the loss function\n",
    "px.line(loss_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persistent homology before the optimisation\n",
    "vr = vrp(metric=\"precomputed\", homology_dimensions=hom_dim)\n",
    "vr.fit_transform_plot([dist_arr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persistent homology after the optimisation\n",
    "try:\n",
    "    vr.fit_transform_plot([dist.detach().numpy()])\n",
    "except ValueError:\n",
    "    warnings.warn(\"Most likely the array is empty...\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directed graphs\n",
    "The algorithm also works for directed graphs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 11\n",
    "dist = torch.rand((n, n)) + 1  # simulate the weighted directed graph\n",
    "dist = dist * (torch.ones(n, n) - torch.eye(n, n))\n",
    "dist_arr = dist.detach().numpy().copy()\n",
    "pg = PersistenceGradient(\n",
    "    homology_dimensions=hom_dim,\n",
    "    zeta=0.01,\n",
    "    collapse_edges=False,\n",
    "    metric=\"precomputed\",\n",
    "    directed=True,\n",
    ")\n",
    "fig, fig3d, loss_val = pg.sgd(dist, 0.1, 10)\n",
    "# plot of the loss function\n",
    "px.line(loss_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persistent homology before the optimisation\n",
    "fp = flp(homology_dimensions=hom_dim)\n",
    "try:\n",
    "    fp.fit_transform_plot([dist_arr])\n",
    "except ValueError:\n",
    "    warnings.warn(\"Most likely the array is empty...\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persistent homology after the optimisation\n",
    "try:\n",
    "    fp.fit_transform_plot([dist.detach().numpy()])\n",
    "except ValueError:\n",
    "    warnings.warn(\"Most likely the array is empty...\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
