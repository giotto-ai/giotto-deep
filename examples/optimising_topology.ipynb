{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from gtda.homology import VietorisRipsPersistence as vrp\n",
    "from gtda.homology import FlagserPersistence as flp\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import pandas as pd\n",
    "from itertools import chain, combinations\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gdeep.utility.optimisation import PersistenceGradient\n",
    "import warnings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Differentiating persistent diagrams\n",
    "\n",
    "## Notation\n",
    "\n",
    " - $K$ is the simplicial complex\n",
    "\n",
    " - $p$ is the number of finite bars (generated by *positive simplices* and eliminated by *negative simplices*)\n",
    "\n",
    " - $q$ is the number of infinitely persistent features.\n",
    "\n",
    "This holds: $|K| = 2p + q$\n",
    "\n",
    "## Description of the problem\n",
    "\n",
    "Let us focus on VR, as the other filtrations are done similarly.\n",
    "\n",
    "The first step is to notice that a filtration can be seen as a map \n",
    "$\\Phi: A \\to \\mathbb R^{|K|} $. \n",
    "\n",
    "Given a point cloud with n points and d features, the space A can be seen as \n",
    "$A=(\\mathbb R^d)^n$. \n",
    "\n",
    "The map $\\Phi$ is given explicitly: given X a point cloud (i.e. an element of $(\\mathbb R^d)^n$), \n",
    "\n",
    "$\\Phi_{\\sigma}(X)=max_{i,j \\in \\sigma}||x_i-x_j||.$ \n",
    "\n",
    "This is the $\\sigma$ component of the whole $\\Phi$.\n",
    "\n",
    "Here sigma is a simplex, i.e. a coordinate of $\\mathbb R^{|K|}$.\n",
    "\n",
    "A persistent diagram requires:\n",
    "1. The identification of $\\mathbb R^{|K|} =(\\mathbb R^2)^p \\times \\mathbb R^q$\n",
    "2. The pairing of positive with negative simplices and the identification of unpaired positive simples.\n",
    "\n",
    "Hence \n",
    "\n",
    "$Pers:Filt_K \\subset \\mathbb R^{|K|} \\to (\\mathbb R^2)^p \\times \\mathbb R^q, \\Phi(X) \\mapsto D = \\cup_i^p (\\Phi_{\\sigma_{i_1}}(X) , \\Phi_{\\sigma_{i_2}}(X) ) \\times \\cup_j^q (\\Phi_{\\sigma_j}(X),+\\infty).$\n",
    "\n",
    "Of course, $|K|=2p + q$.\n",
    "\n",
    "Finally, we can also define persistence functions: they are functions like $E: (\\mathbb R^2)^p \\times \\mathbb R^q \\to \\mathbb R$, invariant under permutations of $p$ and $q$ .\n",
    "e.g.\n",
    "$E(D)=\\sum_i^p|d_i-b_i|^2$\n",
    "\n",
    "We can now define a loss function $L:= E.Pers.\\Phi : A \\to \\mathbb R, A = (\\mathbb R^d)^n$ as before.\n",
    "\n",
    "Can we compute the gradient of $L$ with respect to the point cloud? Observe that Pers is merely a permutation of the coordinates, thus its partial derivatives w.r.t. the filtration are either 1 or 0.\n",
    "Thus, since all the components of $L$ are differentiable, so is $L$ by Leibnitz rule.\n",
    "One can implement it is Pytorch using `autograd`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Build the point cloud\n",
    "We start with a random 2D point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "hom_dim = (0, 1, 2)\n",
    "Xp = torch.rand((10, 4), requires_grad=False)\n",
    "X_arr = Xp.detach().numpy().copy()\n",
    "df = pd.DataFrame(Xp, columns=[\"x\" + str(jj) for jj in range(len(Xp[0]))])\n",
    "\n",
    "px.scatter(df, x=\"x0\", y=\"x1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SGD over topology!\n",
    "\n",
    "To gradient descend (for the moment, the algorithm is not stochastic) over the topological loss function $L=-\\sum_i^p |\\epsilon_{i2}-\\epsilon_{i1}|+ \\lambda \\sum_{x \\in X} ||x||_2^2$ it is enough to initialise the \n",
    "`PersistenceGradient` class and run the `SGD()` method. For clarity, $\\epsilon_{i1}$ is the filtration value of the $i$-th *positive simplex*, while $\\epsilon_{i2}$ of the $i$-th *negative simplex*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pg = PersistenceGradient(\n",
    "    homology_dimensions=hom_dim, zeta=0.1, max_edge_length=0.5, collapse_edges=False\n",
    ")\n",
    "fig, fig3d, loss_val = pg.sgd(Xp, 0.07, 32)\n",
    "\n",
    "# plot of the evolution over the GD iterations\n",
    "fig.show()\n",
    "# plot of the evolution over the GD iterations in 3D\n",
    "# fig3d.show()\n",
    "# plot of the loss function\n",
    "px.line(loss_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    Xp.detach(), columns=[\"x\" + str(jj) for jj in range(len(Xp.detach()[0]))]\n",
    ")\n",
    "\n",
    "px.scatter(df, x=\"x0\", y=\"x1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comparison between persistence diagrams\n",
    "Plot the PD at the beginning and at the end of the optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot persistence diagram\n",
    "vr = vrp(homology_dimensions=hom_dim)\n",
    "\n",
    "try:\n",
    "    vr.fit_transform_plot([X_arr])\n",
    "except ValueError:\n",
    "    warnings.warn(\"Most likely the array is empty...\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    vr.fit_transform_plot([Xp.detach()])\n",
    "except ValueError:\n",
    "    warnings.warn(\"Most likely the array is empty...\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application to weighterd graphs\n",
    "\n",
    "The algorithm can be applied as is to weighted graphs as well.\n",
    "\n",
    "Import the weighted graph as a square tensor, where the entry $(i,j)$ is the weight of the edge $i \\to j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = torch.rand((11, 11))  # simulate the weighted graph\n",
    "dist_arr = dist.detach().numpy().copy()\n",
    "pg = PersistenceGradient(\n",
    "    homology_dimensions=hom_dim, zeta=0.0, collapse_edges=False, metric=\"precomputed\"\n",
    ")\n",
    "fig, fig3d, loss_val = pg.sgd(dist, 0.1, 10)\n",
    "# plot of the loss function\n",
    "px.line(loss_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persistent homology before the optimisation\n",
    "vr = vrp(metric=\"precomputed\", homology_dimensions=hom_dim)\n",
    "vr.fit_transform_plot([dist_arr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persistent homology after the optimisation\n",
    "try:\n",
    "    vr.fit_transform_plot([dist.detach().numpy()])\n",
    "except ValueError:\n",
    "    warnings.warn(\"Most likely the array is empty...\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directed graphs\n",
    "The algorithm also works for directed graphs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 11\n",
    "dist = torch.rand((n, n)) + 1  # simulate the weighted directed graph\n",
    "dist = dist * (torch.ones(n, n) - torch.eye(n, n))\n",
    "dist_arr = dist.detach().numpy().copy()\n",
    "pg = PersistenceGradient(\n",
    "    homology_dimensions=hom_dim,\n",
    "    zeta=0.01,\n",
    "    collapse_edges=False,\n",
    "    metric=\"precomputed\",\n",
    "    directed=True,\n",
    ")\n",
    "fig, fig3d, loss_val = pg.sgd(dist, 0.1, 10)\n",
    "# plot of the loss function\n",
    "px.line(loss_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persistent homology before the optimisation\n",
    "fp = flp(homology_dimensions=hom_dim)\n",
    "try:\n",
    "    fp.fit_transform_plot([dist_arr])\n",
    "except ValueError:\n",
    "    warnings.warn(\"Most likely the array is empty...\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persistent homology after the optimisation\n",
    "try:\n",
    "    fp.fit_transform_plot([dist.detach().numpy()])\n",
    "except ValueError:\n",
    "    warnings.warn(\"Most likely the array is empty...\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}