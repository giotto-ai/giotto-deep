{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython  # type: ignore\n",
    "get_ipython().magic('load_ext autoreload')\n",
    "get_ipython().magic('autoreload 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotmap import DotMap\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Import the PyTorch modules\n",
    "import torch  # type: ignore\n",
    "from torch import nn  # type: ignore\n",
    "from torch.optim import SGD, Adam, RMSprop, AdamW  # type: ignore\n",
    "\n",
    "# Import Tensorflow writer\n",
    "from torch.utils.tensorboard import SummaryWriter  # type: ignore\n",
    "\n",
    "from transformers.optimization import get_cosine_with_hard_restarts_schedule_with_warmup, get_constant_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "# Import the giotto-deep modules\n",
    "from gdeep.data import OrbitsGenerator, DataLoaderKwargs\n",
    "from gdeep.topology_layers import Persformer\n",
    "from gdeep.pipeline import Pipeline\n",
    "from gdeep.search import Gridsearch\n",
    "import json\n",
    "\n",
    "from optuna.pruners import MedianPruner, NopPruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Configs\n",
    "config_data = DotMap({\n",
    "    'batch_size_train': 32,\n",
    "    'num_orbits_per_class': 1_000,\n",
    "    'validation_percentage': 0.0,\n",
    "    'test_percentage': 0.0,\n",
    "    'num_jobs': 8,\n",
    "    'dynamical_system': 'classical_convention',\n",
    "    'homology_dimensions': (0, 1),\n",
    "    'dtype': 'float32',\n",
    "    'arbitrary_precision': False\n",
    "})\n",
    "\n",
    "\n",
    "config_model = DotMap({\n",
    "    'implementation': 'Old_SetTransformer', # SetTransformer, PersFormer,\n",
    "    # PytorchTransformer, DeepSet, X-Transformer\n",
    "    'dim_input': 2 + len(config_data.homology_dimensions) if len(config_data.homology_dimensions) > 1 else 2,\n",
    "    'num_outputs': 1,  # for classification tasks this should be 1\n",
    "    'num_classes': 5,  # number of classes\n",
    "    'dim_hidden': 64,\n",
    "    'num_heads': 4,\n",
    "    'num_induced_points': 32,\n",
    "    'layer_norm': False,  # use layer norm\n",
    "    'simplified_layer_norm': False,  #Xu, J., et al. Understanding and improving layer normalization.\n",
    "    'pre_layer_norm': False,\n",
    "    'layer_norm_pooling': False,\n",
    "    'num_layers_encoder': 2,\n",
    "    'num_layers_decoder': 3,\n",
    "    'attention_type': \"induced_attention\",\n",
    "    'activation': \"gelu\",\n",
    "    'dropout_enc': 0.0,\n",
    "    'dropout_dec': 0.0,\n",
    "    'optimizer': Adam,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_epochs': 200,\n",
    "    'pooling_type': \"attention\",\n",
    "    'weight_decay': 0.00,\n",
    "    'n_accumulated_grads': 0,\n",
    "    'bias_attention': \"True\",\n",
    "    'warmup': 0.02,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the data loader\n",
    "\n",
    "\n",
    "dataloaders_dicts = DataLoaderKwargs(train_kwargs = {\"batch_size\":\n",
    "                                                        config_data.batch_size_train,},\n",
    "                                     val_kwargs = {\"batch_size\": 4},\n",
    "                                     test_kwargs = {\"batch_size\": 3})\n",
    "\n",
    "og = OrbitsGenerator(num_orbits_per_class=config_data.num_orbits_per_class,\n",
    "                     homology_dimensions = config_data.homology_dimensions,\n",
    "                     validation_percentage=config_data.validation_percentage,\n",
    "                     test_percentage=config_data.test_percentage,\n",
    "                     n_jobs=config_data.num_jobs,\n",
    "                     dynamical_system = config_data.dynamical_system,\n",
    "                     dtype=config_data.dtype,\n",
    "                     arbitrary_precision=config_data.arbitrary_precision,\n",
    "                     )\n",
    "\n",
    "if config_data.arbitrary_precision:\n",
    "    orbits = np.load(os.path.join('data', 'orbit5k_arbitrary_precision.npy'))\n",
    "    og.orbits_from_array(orbits)\n",
    "\n",
    "if len(config_data.homology_dimensions) == 0:\n",
    "    dl_train, _, _ = og.get_dataloader_orbits(dataloaders_dicts)\n",
    "else:\n",
    "    dl_train, _, _ = og.get_dataloader_persistence_diagrams(dataloaders_dicts)\n",
    "\n",
    "\n",
    "\n",
    "# Define the model    \n",
    "if config_model.implementation == \"Old_SetTransformer\":\n",
    "    # initialize SetTransformer model\n",
    "\n",
    "    model = Persformer(dim_input=config_model.dim_input, dim_output=5,\n",
    "                           num_inds=config_model.num_induced_points,\n",
    "                           dim_hidden=config_model.dim_hidden,\n",
    "                           num_heads=str(config_model.num_heads),\n",
    "                           layer_norm=str(config_model.layer_norm),  # use layer norm\n",
    "                           pre_layer_norm=str(config_model.pre_layer_norm),\n",
    "                           simplified_layer_norm=str(config_model.simplified_layer_norm),\n",
    "                           dropout_enc=config_model.dropout_enc,\n",
    "                           dropout_dec=config_model.dropout_dec,\n",
    "                           num_layer_enc=config_model.num_layers_encoder,\n",
    "                           num_layer_dec=config_model.num_layers_decoder,\n",
    "                           activation=config_model.activation,\n",
    "                           bias_attention=config_model.bias_attention,\n",
    "                           attention_type=config_model.attention_type,\n",
    "                           layer_norm_pooling=str(config_model.layer_norm_pooling))\n",
    "else:\n",
    "    raise Exception(\"Unknown Implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if config_data.dtype == \"float64\":\n",
    "    print(\"Use float64 model\")\n",
    "    model = model.double()\n",
    "else:\n",
    "    print(\"use float32 model\")\n",
    "    print(config_model)\n",
    "    print(config_data)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do training and validation\n",
    "\n",
    "# initialise loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize the Tensorflow writer\n",
    "#writer = SummaryWriter(comment=json.dumps(config_model.toDict())\\\n",
    "#                                + json.dumps(config_data.toDict()))\n",
    "writer = SummaryWriter(comment=config_model.implementation)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "# initialise pipeline class\n",
    "pipe = Pipeline(model, [dl_train, None], loss_fn, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "# pipe.train(config_model.optimizer,\n",
    "#            config_model.num_epochs,\n",
    "#            cross_validation=False,\n",
    "#            optimizers_param={\"lr\": config_model.learning_rate,\n",
    "#            \"weight_decay\": config_model.weight_decay},\n",
    "#            n_accumulated_grads=config_model.n_accumulated_grads,\n",
    "#            lr_scheduler=get_cosine_schedule_with_warmup,  #get_constant_schedule_with_warmup,  #get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "#            scheduler_params = {\"num_warmup_steps\": int(config_model.warmup * config_model.num_epochs),\n",
    "#                                \"num_training_steps\": config_model.num_epochs,},\n",
    "#                                #\"num_cycles\": 1},\n",
    "#            store_grad_layer_hist=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep training\n",
    "#pipe.train(Adam, 100, False, keep_training=True, store_grad_layer_hist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridsearch\n",
    "\n",
    "# initialise gridsearch\n",
    "pruner = NopPruner()\n",
    "search = Gridsearch(pipe, search_metric=\"accuracy\", n_trials=50, best_not_last=False, pruner=pruner)\n",
    "\n",
    "# dictionaries of hyperparameters\n",
    "optimizers_params = {\"lr\": [1e-3, 1e-0, None, True],\n",
    "                      \"weight_decay\": [0.0001, 0.2, None, True] }\n",
    "dataloaders_params = {\"batch_size\": [8, 16, 2]}\n",
    "models_hyperparams = {\"n_layer_enc\": [2, 4],\n",
    "                      \"n_layer_dec\": [1, 5],\n",
    "                      \"num_heads\": [\"2\", \"4\", \"8\"],\n",
    "                      \"hidden_dim\": [\"16\", \"32\", \"64\"],\n",
    "                      \"dropout\": [0.0, 0.5, 0.05],\n",
    "                      \"layer_norm\": [\"True\", \"False\"],\n",
    "                      \"bias_attention\": [\"True\", \"False\"]}#,\n",
    "                      #'pre_layer_norm': [\"True\", \"False\"]}\n",
    "    \n",
    "scheduler_params = {\"num_warmup_steps\": int(0.1 * config_model.num_epochs),  #(int) – The number of steps for the warmup phase.\n",
    "                    \"num_training_steps\": config_model.num_epochs, #(int) – The total number of training steps.\n",
    "                    \"num_cycles\": [1, 3, 1]}\n",
    "\n",
    "# # starting the gridsearch\n",
    "# search.start((AdamW,), n_epochs=config_model.num_epochs, cross_validation=False,\n",
    "#             optimizers_params=optimizers_params,\n",
    "#             dataloaders_params=dataloaders_params,\n",
    "#             models_hyperparams=models_hyperparams, lr_scheduler=get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "#             scheduler_params=scheduler_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
