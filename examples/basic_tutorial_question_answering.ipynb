{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: Question answering\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial provides you with the basic functioning of *giotto-deep* API.\n",
    "\n",
    "## Scope\n",
    "\n",
    "The example described in this tutorial is the one of question answering. A trained model would be able to **find** the answer inside a given *context*. Hence, we are not building models that can generate new sentences to answer an abstract question: rather, our models read a text (a.k.a. *context*) and try to answer a given question based on the information found in the context.\n",
    "\n",
    "## Content\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. creation of a dataset\n",
    " 2. creation of a model\n",
    " 3. define metrics and losses\n",
    " 4. train the model\n",
    " 5. try to answer a question\n",
    " 6. extract some features of the network to study the attention maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import copy\n",
    "\n",
    "from torch.nn import Transformer\n",
    "from torch.optim import Adam, SparseAdam, SGD\n",
    "import numpy as np\n",
    "from gtda.diagrams import BettiCurve\n",
    "from gtda.plotting import plot_betti_surfaces\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from gdeep.models import FFNet\n",
    "from gdeep.visualization import persistence_diagrams_of_activations\n",
    "from gdeep.data.datasets import DatasetBuilder\n",
    "from gdeep.trainer import Trainer\n",
    "from gdeep.models import ModelExtractor\n",
    "from gdeep.utility import DEVICE\n",
    "from gdeep.data import PreprocessingPipeline\n",
    "from gdeep.data import TransformingDataset\n",
    "from gdeep.data.preprocessors import Normalization, TokenizerQA\n",
    "from gdeep.data.datasets import DataLoaderBuilder\n",
    "from gdeep.visualization import Visualiser\n",
    "from gdeep.search import GiottoSummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to visualize and analyze the results of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/examples` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualization results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = GiottoSummaryWriter()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset\n",
    "\n",
    "In giotto-deep one can wrte a few lines to get the most famous datasets: in the next cell you will see how simple it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd = DatasetBuilder(name=\"SQuAD2\", convert_to_map_dataset=True)\n",
    "ds_tr_str, ds_val_str, ds_ts_str = bd.build()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An item of the dataset contains a context and a question whose answer can be found within that context. The correct answer as well as the starting token are also provided: check the output of the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before preprocessing: \\n\", ds_tr_str[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required preprocessing\n",
    "\n",
    "Neural networks cannot direcly deal with strings. We have first to preprocess the dataset in three main ways:\n",
    " 1. Tokenise the strings into its words\n",
    " 2. Build a vocabulary out of these words\n",
    " 3. Embed each word into a vector, so that each sentence becomes a list of vectors\n",
    "\n",
    "The first two steps are performed by the `TokenizerQA`. The embedding will be added directly as layers to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = TokenizerQA()\n",
    "\n",
    "# in case you need to combine multiple preprocessing:\n",
    "# ppp = PreprocessingPipeline(((PreprocessTextData(), IdentityTransform(), TextDataset),\n",
    "#                             (Normalisation(), IdentityTransform(), BasicDataset)))\n",
    "\n",
    "\n",
    "tokenizer.fit_to_dataset(ds_tr_str)\n",
    "transformed_textds = tokenizer.attach_transform_to_dataset(ds_tr_str)\n",
    "\n",
    "transformed_textts = tokenizer.attach_transform_to_dataset(\n",
    "    ds_val_str\n",
    ")  # this has been fitted on the train set!\n",
    "\n",
    "print(\"After the preprocessing: \\n\", transformed_textds[0])\n",
    "\n",
    "# the only part of the training/test set we are interested in\n",
    "train_indices = list(range(64 * 2))\n",
    "test_indices = list(range(64 * 1))\n",
    "\n",
    "dl_tr2, dl_ts2, _ = DataLoaderBuilder((transformed_textds, transformed_textts)).build(\n",
    "    (\n",
    "        {\"batch_size\": 16, \"sampler\": SubsetRandomSampler(train_indices)},\n",
    "        {\"batch_size\": 16, \"sampler\": SubsetRandomSampler(test_indices)},\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train your model\n",
    "\n",
    "The model for QA shall accept as input the context and the question and return the probabilities for the initial and final token of the answer in the input context. The output then, shall be a pair of logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# my simple transformer model\n",
    "class QATransformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim):\n",
    "        super(QATransformer, self).__init__()\n",
    "        self.transformer = Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=2,\n",
    "            num_encoder_layers=1,\n",
    "            num_decoder_layers=1,\n",
    "            dim_feedforward=512,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        self.embedding_src = nn.Embedding(src_vocab_size, embed_dim, sparse=True)\n",
    "        self.embedding_tgt = nn.Embedding(tgt_vocab_size, embed_dim, sparse=True)\n",
    "        self.generator = nn.Linear(embed_dim, 2)\n",
    "\n",
    "    def forward(self, ctx, qst):\n",
    "        # print(src.shape, tgt.shape)\n",
    "        ctx_emb = self.embedding_src(ctx).permute(1, 0, 2)\n",
    "        qst_emb = self.embedding_tgt(qst).permute(1, 0, 2)\n",
    "        # print(src_emb.shape, tgt_emb.shape)\n",
    "        self.outs = self.transformer(qst_emb, ctx_emb).permute(1, 0, 2)\n",
    "        # print(outs.shape)\n",
    "        logits = self.generator(self.outs)\n",
    "        return logits\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        \"\"\"this is needed to make sure that the \n",
    "        non-leaf nodes do not\n",
    "        interfere with copy.deepcopy()\n",
    "        \"\"\"\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        memo[id(self)] = result\n",
    "        for k, v in self.__dict__.items():\n",
    "            setattr(result, k, copy.deepcopy(v, memo))\n",
    "        return result\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"this method is used only at the inference step\"\"\"\n",
    "        return self.transformer.encoder(self.embedding_src(src), src_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        \"\"\"this method is used only at the inference step\"\"\"\n",
    "        return self.transformer.decoder(self.embedding_tgt(tgt), memory, tgt_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(tokenizer.vocabulary)\n",
    "tgt_vocab_size = len(tokenizer.vocabulary)\n",
    "emb_size = 64\n",
    "\n",
    "model = QATransformer(src_vocab_size, tgt_vocab_size, emb_size)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "This loss function is a adapted version of the Cross Entropy for the trnasformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(output_of_network, label_of_dataloader):\n",
    "    # print(output_of_network.shape, label_of_dataloader.shape)\n",
    "    tgt_out = label_of_dataloader\n",
    "    logits = output_of_network\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    return cel(logits, tgt_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!\n",
    "\n",
    "We are fnally there! We have defined the model, transformed the dataset so that it is manageable by standard layers and we have also adapted the loss function. We are ready to start the training: in giotto-deep, it is a matter of a few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a pipeline class with the model, dataloaders loss_fn and tensorboard writer\n",
    "pipe = Trainer(model, (dl_tr2, dl_ts2), loss_fn, writer)\n",
    "\n",
    "# train the model\n",
    "pipe.train(SGD, 3, False, {\"lr\": 0.01}, {\"batch_size\": 16})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering questions!\n",
    "\n",
    "Here we have a question and its associated context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = next(iter(ds_val_str))\n",
    "bb[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Get the vocabulary and numericize the question and context to then input both to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocabulary and tokenizer\n",
    "voc = tokenizer.vocabulary\n",
    "context = tokenizer.tokenizer(bb[0])\n",
    "question = tokenizer.tokenizer(bb[1])\n",
    "\n",
    "# get the indexes in the vocabulary of the tokens\n",
    "context_idx = torch.tensor(list(map(voc.__getitem__, context)))\n",
    "question_idx = torch.tensor(list(map(voc.__getitem__, question)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = next(iter(dl_tr2))\n",
    "pad_fn = lambda length_to_pad, item: torch.cat(\n",
    "    [item, tokenizer.pad_item * torch.ones(length_to_pad - item.shape[0])]\n",
    ").to(torch.long)\n",
    "\n",
    "# these tensors are ready to be fitted into the model\n",
    "length_to_pad = aa[0][0].shape[-1]  # context length\n",
    "context_ready_for_model = pad_fn(length_to_pad, context_idx)\n",
    "length_to_pad = aa[0][1].shape[-1]  # question length\n",
    "question_ready_for_model = pad_fn(length_to_pad, question_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the two tensors of context and question together and input them to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = [context_ready_for_model.view(1, -1).to(DEVICE), \n",
    "              question_ready_for_model.view(1,-1).to(DEVICE)]\n",
    "\n",
    "out = pipe.model(*input_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output corresponds to the digits for the start and end tokens of the answer. It is now time to extract them with `torch.argmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_idx = torch.argmax(out, dim=1)\n",
    "\n",
    "# simple code to convert the model's answer into words\n",
    "try:\n",
    "    if answer_idx[0][1] > answer_idx[0][0]:\n",
    "        print(\n",
    "            \"The model proposes: '\",\n",
    "            \" \".join(context[answer_idx[0][0] : answer_idx[0][1]]),\n",
    "            \"...'\",\n",
    "        )\n",
    "    else:\n",
    "        print(\"The model proposes: '\", context[answer_idx[0][0]], \"...'\")\n",
    "except IndexError:\n",
    "    print(\"The model was not able to find the answer.\")\n",
    "print(\"The actual answer was: '\" + bb[2][0] + \"'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract inner data from your models\n",
    "\n",
    "In this section we are extracting, for the same input as above, the attention maps.\n",
    "\n",
    "Such matrices are creating a relationsip between the question and the context, highlighting the words that most captured the transformer attention. Such maps are really useful to interpret the model results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the model extractor\n",
    "ex = ModelExtractor(pipe.model, loss_fn)\n",
    "\n",
    "# getting the names of the layers\n",
    "layer_names = ex.get_layers_param().keys()\n",
    "\n",
    "print(\"Let's extract the activations of the first attention layer: \", next(iter(layer_names)))\n",
    "self_attention = ex.get_activations(input_list)[-5:-3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the tensor! First, load th visualizer\n",
    "vs = Visualiser(pipe)\n",
    "vs.plot_self_attention(self_attention, context, question, figsize=(20, 20));\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "If you have trained the model with very few epochs and only a small subset of data, you would have probably obtained almost random results: Can you improve teh model and interpret these attention maps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise your model interactively\n",
    "\n",
    "One final note about visualising and inspecting transformer models: it is possible, in giotto-deep, to plot an interactive model graph in tensdorboard, so that you can eviscerate the inner working of the transformer visually and demistify these poewerful models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.visualization import Visualiser\n",
    "\n",
    "vs = Visualiser(pipe)\n",
    "\n",
    "vs.plot_interactive_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
