{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: translation\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial provides you with the basic functionalities of *giotto-deep* API.\n",
    "\n",
    "The example described in this tutorial is the one of translation following a sequence-to-sequence transformer model: all teh details can be found [here](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf). \n",
    "\n",
    "## Probabilistic modelling\n",
    "\n",
    "The basic idea would be to predict the target sentence $y$ given source sentence $x$, more precisely you ant to predict $P(y|x)$. But this is very hard to do directly, since one would need a lot of data to have a reasonable estimate of this conditional probability.\n",
    "Hence, one would ratehr factor the probability using the chain rule, then predict each factor:\n",
    "\n",
    "$$\n",
    "P(y_1,...,y_N|x) = \\prod_i^N P(y_i|y_{<i},x)\n",
    "$$\n",
    "\n",
    "In the NLP language, this corresponds to predicting the next token of the output sequence given the already predicted tokens and the input sequence.\n",
    "\n",
    "## Scope\n",
    "\n",
    "You will build your own transformer model, train it, and use it to translate German sentences to English!\n",
    "\n",
    "Here is an example of what a translation task is about:\n",
    "\n",
    " - German sentence: 'Ich mag Pizza.'\n",
    " - English translation: 'I like pizza.'\n",
    "\n",
    "## The plan for this tutorial\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. creation of a dataset\n",
    " 2. preprocessing of the dataset\n",
    " 3. creation of a model\n",
    " 4. definition of the metrics and losses\n",
    " 5. trainining of the model\n",
    " 6. using the model to translate some sentences\n",
    " 7. (extra) extract some features of the network for interpretability analysis\n",
    " \n",
    "Let's start with importing the requried libraries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from gtda.diagrams import BettiCurve\n",
    "from gtda.plotting import plot_betti_surfaces\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.nn import Transformer\n",
    "from torch.optim import Adam, SparseAdam, SGD\n",
    "import torchtext\n",
    "\n",
    "# our special guests!\n",
    "from gdeep.models import FFNet\n",
    "from gdeep.visualization import persistence_diagrams_of_activations\n",
    "from gdeep.data.datasets import DatasetBuilder\n",
    "from gdeep.trainer import Trainer\n",
    "from gdeep.data import TransformingDataset\n",
    "from gdeep.data.preprocessors import TokenizerTranslation\n",
    "from gdeep.data.datasets import DataLoaderBuilder\n",
    "from gdeep.models import ModelExtractor\n",
    "from gdeep.analysis.interpretability import Interpreter\n",
    "from gdeep.visualization import Visualiser\n",
    "from gdeep.search import GiottoSummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the results of your models, you need to start tensorboard. All data about the model, the training, the hyperparameters... will be stored there.\n",
    "\n",
    "## How to start tensorboard\n",
    "On the terminal, move inside the `/examples` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training step to visualise all the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = GiottoSummaryWriter()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset\n",
    "\n",
    "We propose to use the [Multi30k](https://github.com/multi30k/dataset) dataset: in giotto-deep it is very easy to import datasets! It is eough to write:\n",
    "\n",
    "```python\n",
    "# initialise the builder with the dataset name\n",
    "bd = DatasetBuilder(name=\"Multi30k\", convert_to_map_dataset=True)\n",
    "\n",
    "# build the datasets\n",
    "ds_tr_str, ds_val_str, ds_ts_str = bd.build()\n",
    "\n",
    "```\n",
    "\n",
    "### Disclaimer: the next cell is a temporary fix to a third-party download link issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary cell, due to a failure of theird party link: we are required to download the dataset manually!\n",
    "# yout would expect to simply run:\n",
    "# bd = DatasetBuilder(name=\"Multi30k\", convert_to_map_dataset=True)\n",
    "\n",
    "# and then\n",
    "# ds_tr_str, ds_val_str, ds_ts_str = bd.build()\n",
    "\n",
    "# instead we are doing this mess:\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "from typing import List\n",
    "\n",
    "from torchdata.datapipes.iter import FileOpener, FileLister, StreamReader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from gdeep.utility import DEFAULT_DOWNLOAD_DIR\n",
    "\n",
    "\n",
    "base_path = os.path.join(DEFAULT_DOWNLOAD_DIR, \"Multi30k\")\n",
    "url_de = 'https://github.com/multi30k/dataset/raw/master/data/task1/raw/train.de.gz'\n",
    "url_en = 'https://github.com/multi30k/dataset/raw/master/data/task1/raw/train.en.gz'\n",
    "torchtext.utils.download_from_url(url_en, path=os.path.join(base_path, \"train.en.gz\"))\n",
    "torchtext.utils.download_from_url(url_de, path=os.path.join(base_path, \"train.de.gz\"))\n",
    "url_de = 'https://github.com/multi30k/dataset/raw/master/data/task1/raw/val.de.gz'\n",
    "url_en = 'https://github.com/multi30k/dataset/raw/master/data/task1/raw/val.en.gz'\n",
    "torchtext.utils.download_from_url(url_en, path=os.path.join(base_path, \"val.en.gz\"))\n",
    "torchtext.utils.download_from_url(url_de, path=os.path.join(base_path, \"val.de.gz\"))\n",
    "url_de = 'https://github.com/multi30k/dataset/raw/master/data/task1/raw/test_2016_flickr.de.gz'\n",
    "url_en = 'https://github.com/multi30k/dataset/raw/master/data/task1/raw/test_2016_flickr.en.gz'\n",
    "torchtext.utils.download_from_url(url_en, path=os.path.join(base_path, \"test.en.gz\"))\n",
    "torchtext.utils.download_from_url(url_de, path=os.path.join(base_path, \"test.de.gz\"))\n",
    "\n",
    "# we store the data in the RAM\n",
    "in_memory_data = []\n",
    "\n",
    "\n",
    "for item in [\"train.de\", \"train.en\", \"val.de\", \"val.en\", \"test.de\", \"test.en\"]:\n",
    "    with gzip.open(os.path.join(base_path, item+\".gz\"), 'rb') as f_in:\n",
    "        with open(os.path.join(base_path, item), 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "    with open(os.path.join(base_path, item), 'r', encoding=\"utf8\") as f:\n",
    "        in_memory_data.append(f.read())\n",
    "\n",
    "assert len(in_memory_data) == 6\n",
    "assert len(in_memory_data[0].split(\"\\n\")) == len(in_memory_data[1].split(\"\\n\"))\n",
    "assert len(in_memory_data[2].split(\"\\n\")) == len(in_memory_data[3].split(\"\\n\"))\n",
    "\n",
    "class MyMulti30k(Dataset):\n",
    "    \"\"\"Temporary Multi30k Dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_tuple: List[List[str]]) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data:\n",
    "                list of data\n",
    "        \"\"\"\n",
    "        \n",
    "        assert len(data_tuple) == 2, \"expected 2 lists corresponding to the two languages\"\n",
    "        self.data = list(zip(data_tuple[0].split(\"\\n\"), data_tuple[1].split(\"\\n\")))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "ds_tr_str, ds_val_str, ds_ts_str = MyMulti30k(in_memory_data[:2]), MyMulti30k(in_memory_data[2:4]), MyMulti30k(in_memory_data[4:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a list of pairs of sentences: the German sentence and its English translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before preprocessing: \\n\", ds_tr_str[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required preprocessing\n",
    "\n",
    "Neural networks cannot direcly deal with strings. We have first to preprocess the dataset in three main ways:\n",
    " 1. Tokenise each string into its words (and maybe adjust each word to remove plurals, interjections, capital letters...)\n",
    " 2. Build a vocabulary out of these tokens (each modified word of point a. is called a token)\n",
    " 3. Embed each token into a vector, so that each sentence becomes a list of vectors\n",
    "\n",
    "The **first two steps** are performed by the `TokenizerTranslation` class. The embedding will be added directly to the model (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# install german tokenizer\n",
    "!{sys.executable} -m spacy download de_core_news_sm\n",
    "\n",
    "# get the german tokenizer\n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "\n",
    "# initialise the giotto-deep tokenizer\n",
    "tokenizer = TokenizerTranslation(tokenizer=de_tokenizer)\n",
    "\n",
    "# fit the tokenizer to the dataset (note that te vocabularies will be automatically built in this case\n",
    "tokenizer.fit_to_dataset(ds_tr_str)\n",
    "\n",
    "# prprocess the dataset\n",
    "transformed_textds = tokenizer.attach_transform_to_dataset(ds_tr_str)\n",
    "transformed_textts = tokenizer.attach_transform_to_dataset(ds_val_str)\n",
    "\n",
    "print(\"After the preprocessing: \\n\", transformed_textds[0])\n",
    "\n",
    "# subsample the training and test datasets\n",
    "train_indices = list(range(64*2))\n",
    "test_indices = list(range(64*1))\n",
    "\n",
    "dl_tr, dl_val, _ = DataLoaderBuilder((transformed_textds, \n",
    "                                      transformed_textts)).build(({\"batch_size\":16, \n",
    "                                                                   \"sampler\":SubsetRandomSampler(train_indices)},{\"batch_size\":16, \n",
    "                                                                                                                  \"sampler\":SubsetRandomSampler(test_indices)}\n",
    "                                                                 ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, now the data is not in string format anymore: rather, the sentences have each been transformed to a `torch.Tensor` of type `long`. Each of these numbers represents the index in the vocabulary of the associated token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your model\n",
    "\n",
    "The model we play with is a simple transformer model with two embedding layers (for the German and English sentence) followed by a single transformer layer. \n",
    "\n",
    "The input of this model is assumed to be a tensors. The tensor is of the form `shape(transformed_textds[0][0])`. \n",
    "\n",
    "Of course, as in all neural networks, data are inputted to the model  in batches: hence, the first dimension will be the batch size, then a dimension to choose the source or target sentence, and the last dimension will be about the actual tokenisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my simple transformer model\n",
    "class TranslationTransformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim):\n",
    "        super(TranslationTransformer, self).__init__()\n",
    "        self.transformer = Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=2,\n",
    "            num_encoder_layers=1,\n",
    "            num_decoder_layers=1,\n",
    "            dim_feedforward=512,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        self.embedding_src = nn.Embedding(src_vocab_size, embed_dim, sparse=True)\n",
    "        self.embedding_tgt = nn.Embedding(tgt_vocab_size, embed_dim, sparse=True)\n",
    "        self.generator = nn.Linear(embed_dim, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.embedding_src(src).permute(1, 0, 2)\n",
    "        tgt_emb = self.embedding_tgt(tgt).permute(1, 0, 2)\n",
    "        self.outs = self.transformer(src_emb, tgt_emb).permute(1, 0, 2)\n",
    "        logits = self.generator(self.outs)\n",
    "        return logits\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"this method is used only at the inference step.\n",
    "        This method runs the data through the encoder of the\n",
    "        transformer\"\"\"\n",
    "        return self.transformer.encoder(self.embedding_src(src), src_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        \"\"\"this method is used only at the inference step\n",
    "        This method runs the data through the decoder of the\n",
    "        transformer\"\"\"\n",
    "        return self.transformer.decoder(self.embedding_tgt(tgt), memory, tgt_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialise the model as follows:\n",
    " - we need to set the maximum vocabulary size to fix the `Embedding` architectures\n",
    " - we need to set the embedding dimension\n",
    " - initialise the model class with the needed parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(tokenizer.vocabulary)\n",
    "tgt_vocab_size = len(tokenizer.vocabulary_target)\n",
    "emb_dim = 64\n",
    "\n",
    "model = TranslationTransformer(src_vocab_size, tgt_vocab_size, emb_dim)\n",
    "X = next(iter(dl_tr))\n",
    "# a datum\n",
    "# assert model(X[0]).argmax(2).shape == X[1].shape\n",
    "print(\"This is our model: \\n\", model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "This loss function is an adapted version of the Cross Entropy Loss for the transformer architecture we just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, tgt_out):\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    return cel(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we are: all is set upo and we are ready to train the model.\n",
    "\n",
    "# Trainig the model\n",
    "\n",
    "In giotto deep all is done via a `Trainer`, meaning a class taht takes care of the training and validation steps, storing the intermediate results to tensorboard, using (or not) cross validation, ... basically everythin you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a pipeline class with the model, dataloaders loss_fn and tensorboard writer\n",
    "pipe = Trainer(model, (dl_tr, dl_val), loss_fn, writer)\n",
    "\n",
    "# train the model\n",
    "pipe.train(\n",
    "    optimizer=SGD,\n",
    "    n_epochs=2,\n",
    "    cross_validation=False,\n",
    "    optimizers_param={\"lr\": 0.01},\n",
    "    dataloaders_param={\"batch_size\": 16},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation!\n",
    "\n",
    "So the model has been trained and now it is time to translate a sentece. Let's take thee following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de, en = next(iter(ds_tr_str))\n",
    "print(de, \"\\n\", en)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Get the vocabulary and numericize the German sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = tokenizer.vocabulary\n",
    "# sent = str.lower(de).split()\n",
    "# de_sentence = list(map(voc.__getitem__,sent))\n",
    "de_sentence, en_sentence = tokenizer((de, en))[0]\n",
    "de_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to build a couple of auxiliary functiosn to help us translate. The logic is to use the transformer encoder and decoder layers directly. Let's have a quick reminder o how things work:\n",
    "\n",
    "<img src=\"./images/translation_transformer.png\" alt=\"drawing\" width=\"400\" class=\"center\"/>\n",
    "\n",
    "The above drawing is a good representation of our architecture. During training, the English and German sentence are inputted from \"below\". Then, once the model is used in inference, there is an important change ongoing. Basically, the full input sentence (the German one in our case) is given as input to the encoder: the output (the probablities) are then greedily transformed into a signle token: that token is then used in the \"Outputs\" below: together with the Input sentence, the next word is predicted (at the top) and used again as input in the \"Outputs\" below. Hence, word after word, you get the translation of the original sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.utility import DEVICE\n",
    "\n",
    "pad_item = 0\n",
    "max_len = len(en_sentence)\n",
    "\n",
    "\n",
    "def greedy_decode(model, src, src_mask, max_len):\n",
    "    \"\"\"function to generate output sequence using greedy algorithm\"\"\"\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    # ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long)\n",
    "    ys = torch.ones(*src.shape).fill_(pad_item).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len - 1):\n",
    "        # print(\"before:\", ys)\n",
    "        out = model.decode(ys, memory, None)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        next_word = torch.argmax(prob, dim=1)\n",
    "        next_word = next_word[i + 1].item()  # the shift\n",
    "        ys[0, i] = next_word\n",
    "        # print(\"after:\", ys)\n",
    "    return ys\n",
    "\n",
    "\n",
    "def translate(model: torch.nn.Module, dl_ts_item):\n",
    "    \"\"\"actual function to translate input sentence into target language\"\"\"\n",
    "    model.eval()\n",
    "    voc = tokenizer.vocabulary_target\n",
    "    src = dl_ts_item.to(DEVICE)\n",
    "    num_tokens = src.shape[1]\n",
    "    src_mask = None\n",
    "    tgt_tokens_tensor = greedy_decode(model, src, src_mask, max_len).flatten().to(DEVICE)\n",
    "    tgt_tokens = tgt_tokens_tensor.tolist()\n",
    "    # print(tgt_tokens)\n",
    "    return \" \".join(voc.lookup_tokens(tgt_tokens))\n",
    "\n",
    "\n",
    "# translation!\n",
    "print(\"German sentence: \", de)\n",
    "print(\"English translation: \", translate(pipe.model, de_sentence.reshape(1, -1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: interpretability\n",
    "\n",
    "In the next section we show you how to run some simple intepretability tools, like plotting thee attention matrices of the transformer. \n",
    "\n",
    "The first step is to extract the values of the attention activations given an input `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.models import ModelExtractor\n",
    "\n",
    "# the candidate datum\n",
    "x = [de_sentence.view(1,-1).to(DEVICE), en_sentence.view(1,-1).to(DEVICE)]  # only one pair of sentences, shapd as a batch of dim = 1\n",
    "\n",
    "# the model extractor\n",
    "ex = ModelExtractor(pipe.model, loss_fn)\n",
    "\n",
    "# getting the names of the layers\n",
    "layer_names = ex.get_layers_param().keys()\n",
    "\n",
    "print(\"Let's extract the activations of the first attention layer: \", next(iter(layer_names)))\n",
    "self_attention = ex.get_activations(x)[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the tensor! First, load th visualizer\n",
    "vs = Visualiser(pipe)\n",
    "vs.plot_self_attention(self_attention, tokenizer.tokenizer(en), de_tokenizer(de), figsize=(20, 20));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have just run this notebooks, you would most likely have gotten some funny nonsensical answer: consider that you trained a very simple model on a very small subset of data for only two epochs.\n",
    "\n",
    "## Challenge\n",
    "\n",
    "Starting from this simple notebook, do you think you can enlarge the dataset, the model, the training epochs and get a decent translator? Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (WIP) Huggingface transformers\n",
    "\n",
    "If you are actually interested in a pretrained transformers that works directly with a few lines of code, `giotto-deep` supports Hugginface transformers.\n",
    "\n",
    "The next section explains how to run one such transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import ...\n",
    "\n",
    "# take T5\n",
    "\n",
    "# finetune on the Multi30k\n",
    "\n",
    "# go to TB to see the results + screenshot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
