{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: text classification\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial provides you with a basic tutorial on *text classification* using *giotto-deep*.\n",
    "\n",
    "## Scope\n",
    "\n",
    "Text classification, sometimes called **sentiment analysis** consist is classifying text excerpts into predefined classes. For example, one may try to classify movies reviews into five classes with differet level of the author enjoyment:\n",
    "\n",
    "| Movie review | liking score |\n",
    "|----|-----|\n",
    "| This movie makes me cry! It's so moving! | 5/5 |\n",
    "|I hate this movie: bad acting and no plot! Definitely not recommended. | 1/5 |\n",
    "\n",
    "## Content\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. Creation of a dataset and preprocessing\n",
    " 2. Creation of a model\n",
    " 3. Define metrics and losses\n",
    " 4. Run benchmarks\n",
    " 5. Visualise results interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from gtda.diagrams import BettiCurve\n",
    "from gtda.plotting import plot_betti_surfaces\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim import Adam, SparseAdam, SGD\n",
    "from torchvision import transforms\n",
    "\n",
    "from gdeep.models import FFNet\n",
    "from gdeep.visualization import persistence_diagrams_of_activations\n",
    "from gdeep.data.datasets import DatasetBuilder\n",
    "from gdeep.data import PreprocessingPipeline\n",
    "from gdeep.data import TransformingDataset\n",
    "from gdeep.data.preprocessors import Normalization, TokenizerTextClassification\n",
    "from gdeep.data.datasets import DataLoaderBuilder\n",
    "from gdeep.trainer import Trainer\n",
    "from gdeep.analysis.interpretability import Interpreter\n",
    "from gdeep.visualization import Visualiser\n",
    "from gdeep.models import ModelExtractor\n",
    "from gdeep.search import GiottoSummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the results of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/examples` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualization results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = GiottoSummaryWriter()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset\n",
    "\n",
    "In this section we will import the [AG_NEWS](https://pytorch.org/text/stable/datasets.html#ag-news) dataset. This dataset contains Thomson-Reuters news and they are classified into four macro categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many time we get an IterableDataset which is good for memory consumption, but cannot be subsampled!\n",
    "# we can entire batches and sample them using the conversion to map type:\n",
    "bd = DatasetBuilder(name=\"AG_NEWS\", convert_to_map_dataset=True)\n",
    "ds_tr_str, ds_val_str, ds_ts_str = bd.build()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is presented as a tuple of the form `(label, text)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before the preprocessing: \\n\", ds_tr_str[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required preprocessing\n",
    "\n",
    "Neural networks cannot direcly deal with strings. We have first to preprocess the dataset in three main ways:\n",
    " 1. Tokenise each string into its words (and maybe adjust each word to remove plurals, interjections, capital letters...)\n",
    " 2. Build a vocabulary out of these tokens (each modified word of point a. is called a token)\n",
    " 3. Embed each token into a vector, so that each sentence becomes a list of vectors\n",
    "\n",
    "The **first two steps** are performed by the `TokenizerTextClassification` class. The embedding will be added directly to the model (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptd = TokenizerTextClassification()\n",
    "\n",
    "# in case you need to combine multiple preprocessing:\n",
    "# ppp = PreprocessingPipeline(((PreprocessTextData(), IdentityTransform(), TextDataset),\n",
    "#                             (Normalisation(), IdentityTransform(), BasicDataset)))\n",
    "\n",
    "\n",
    "ptd.fit_to_dataset(ds_tr_str)\n",
    "transformed_textds = ptd.attach_transform_to_dataset(ds_tr_str)\n",
    "\n",
    "transformed_textts = ptd.attach_transform_to_dataset(ds_val_str)\n",
    "\n",
    "print(\"After the preprocessing: \\n\", transformed_textds[0])\n",
    "\n",
    "# the only part of the training/test set we are interested in\n",
    "train_indices = list(range(64 * 20))\n",
    "test_indices = list(range(64 * 10))\n",
    "\n",
    "dl_tr2, dl_ts2, _ = DataLoaderBuilder((transformed_textds, transformed_textts)).build(\n",
    "    (\n",
    "        {\"batch_size\": 16, \"sampler\": SubsetRandomSampler(train_indices)},\n",
    "        {\"batch_size\": 16, \"sampler\": SubsetRandomSampler(test_indices)},\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train your model\n",
    "\n",
    "Our model is going to be a simple embedding layer (i.e. a layer whose goal is to transform list of tokens into vectors) , followed by a simple feed-forward layer.\n",
    "\n",
    "Furthermore, since the output of the embedding layer has three dimensions, we need to remove one before the tensor can be fed to the final feed-forward layer: this is done - in this very simple tutorial - via an average operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        mean = torch.mean(embedded, dim=1)\n",
    "        return self.fc(mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we initialise the model parameters and the model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(ptd.vocabulary)\n",
    "emsize = 64\n",
    "# print(vocab_size, emsize)\n",
    "model = TextClassificationModel(vocab_size, emsize, 4)\n",
    "print(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "We are finally ready to train our model: we have already built a dataset (that is compatible with the model). We also know that the task is a classification task, hence we can use the CrossEntropy loss!\n",
    "\n",
    "Let's see how in giotto-deep, in only a couple of lines, one can run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "pipe = Trainer(model, (dl_tr2,), loss_fn, writer)\n",
    "\n",
    "# train the model\n",
    "pipe.train(SGD, 7, False, {\"lr\": 0.01}, {\"batch_size\": 20})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simply use interpretability tools\n",
    "\n",
    "In the next two cells we use interpretability tool on the model to see what and how our model has learned. Basically, we want to understand how each word in a sentence contributes to the classification choice.\n",
    "\n",
    "But first, how about sending to the tensorboard the model structure an interactive inspection? It's as simple as writing the code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = Visualiser(pipe)\n",
    "vs.plot_interactive_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretability!\n",
    "\n",
    "inter = Interpreter(pipe.model, method=\"LayerIntegratedGradients\")\n",
    "\n",
    "inter.interpret_text(\n",
    "    \"I am writing about money and business\",\n",
    "    0,\n",
    "    ptd.vocabulary,\n",
    "    ptd.tokenizer,\n",
    "    layer=pipe.model.embedding,\n",
    "    n_steps=500,\n",
    "    return_convergence_delta=True\n",
    ")\n",
    "\n",
    "\n",
    "vs.plot_interpreter_text(inter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
