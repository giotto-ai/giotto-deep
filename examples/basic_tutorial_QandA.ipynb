{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: Question answer\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial provides you with the basic functioning of *giotto-deep* API.\n",
    "\n",
    "The example described in this tutorial is the one of question answer.\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. creation of a dataset\n",
    " 2. creation of a model\n",
    " 3. define metrics and losses\n",
    " 4. train the model\n",
    " 5. extract some features of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPUs...\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from gdeep.models import FFNet\n",
    "\n",
    "from gdeep.visualisation import  persistence_diagrams_of_activations\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gdeep.data import TorchDataLoader\n",
    "from gdeep.pipeline import Pipeline\n",
    "\n",
    "from gtda.diagrams import BettiCurve\n",
    "\n",
    "from gtda.plotting import plot_betti_surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the reuslts of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualisation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# the only part of the training set we are interested in\n",
    "train_indices = list(range(32*10))\n",
    "\n",
    "dl = TorchDataLoader(name=\"SQuAD2\", convert_to_map_dataset=True)\n",
    "dl_tr_str, dl_ts_str = dl.build_dataloaders(sampler=SubsetRandomSampler(train_indices), batch_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a context and a question whose answer can be found within that context. The correct answer as well as the starting characters are also provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The group changed their name to Destiny\\'s Child in 1996, based upon a passage in the Book of Isaiah. In 1997, Destiny\\'s Child released their major label debut song \"Killing Time\" on the soundtrack to the 1997 film, Men in Black. The following year, the group released their self-titled debut album, scoring their first major hit \"No, No, No\". The album established the group as a viable act in the music industry, with moderate sales and winning the group three Soul Train Lady of Soul Awards for Best R&B/Soul Album of the Year, Best R&B/Soul or Rap New Artist, and Best R&B/Soul Single for \"No, No, No\". The group released their multi-platinum second album The Writing\\'s on the Wall in 1999. The record features some of the group\\'s most widely known songs such as \"Bills, Bills, Bills\", the group\\'s first number-one single, \"Jumpin\\' Jumpin\\'\" and \"Say My Name\", which became their most successful song at the time, and would remain one of their signature songs. \"Say My Name\" won the Best R&B Performance by a Duo or Group with Vocals and the Best R&B Song at the 43rd Annual Grammy Awards. The Writing\\'s on the Wall sold more than eight million copies worldwide. During this time, BeyoncÃ© recorded a duet with Marc Nelson, an original member of Boyz II Men, on the song \"After All Is Said and Done\" for the soundtrack to the 1999 film, The Best Man.',),\n",
       " (\"Beyonce's group changed their name to Destiny's Child in what year?\",),\n",
       " [('1996',)],\n",
       " [tensor([51])]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datum = next(iter(dl_tr_str))\n",
    "datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, it tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kind of problems can, in principle, be solved algorithmically.',),\n",
       " ('What field of computer science analyzes all possible algorithms in aggregate to determine the resource requirements needed to solve to a given problem?  ',),\n",
       " [('computational complexity theory',),\n",
       "  ('computational complexity theory',),\n",
       "  ('computational complexity theory',)],\n",
       " [tensor([161]), tensor([161]), tensor([161])]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datum = next(iter(dl_ts_str))\n",
    "datum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required preprocessing\n",
    "\n",
    "Neural networks cannot direcly deal with strings. We have first to preprocess the dataset in three main ways:\n",
    " 1. Tokenise the strings into its words\n",
    " 2. Build a vocabulary out of these words\n",
    " 3. Embed each word into a vector, so that each sentence becomes a list of vectors\n",
    "\n",
    "The first two steps are performed by the `PreprocessTextQA`. The embedding will be added directly to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.data import PreprocessTextQA\n",
    "\n",
    "prec = PreprocessTextQA((dl_tr_str, dl_ts_str))\n",
    "\n",
    "(dl_tr, dl_ts) = prec.build_dataloaders(batch_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[1828,   90, 4077,  ..., 2567, 2567, 2567],\n",
       "          [ 133,  568,   25,  ..., 2567, 2567, 2567]],\n",
       " \n",
       "         [[ 518,   41,   28,  ..., 2567, 2567, 2567],\n",
       "          [ 117,   64,   17,  ..., 2567, 2567, 2567]],\n",
       " \n",
       "         [[ 778,   72,   33,  ..., 2567, 2567, 2567],\n",
       "          [ 117,  158, 1133,  ..., 2567, 2567, 2567]]]),\n",
       " tensor([[103, 104],\n",
       "         [178, 180],\n",
       "         [ 17,  20]])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = next(iter(dl_tr))\n",
    "aa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train your model\n",
    "\n",
    "The model for QA shall accept as input the context and the question and return the probabilities for the initial and final token of the answer in the input context. The output than, is a pair of logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Transformer\n",
    "from torch.optim import Adam, SparseAdam, SGD\n",
    "import copy\n",
    "# my simple transformer model\n",
    "class QATransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim):\n",
    "        super(QATransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=embed_dim,\n",
    "                                       nhead=2,\n",
    "                                       num_encoder_layers=1,\n",
    "                                       num_decoder_layers=1,\n",
    "                                       dim_feedforward=512,\n",
    "                                       dropout=0.1)\n",
    "        self.embedding_src = nn.Embedding(src_vocab_size, embed_dim, sparse=True)\n",
    "        self.embedding_tgt = nn.Embedding(tgt_vocab_size, embed_dim, sparse=True)\n",
    "        self.generator = nn.Linear(embed_dim, 2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #print(X.shape)\n",
    "        src = X[:,0,:]\n",
    "        tgt = X[:,1,:]\n",
    "        #print(src.shape, tgt.shape)\n",
    "        src_emb = self.embedding_src(src)\n",
    "        tgt_emb = self.embedding_tgt(tgt)\n",
    "        #print(src_emb.shape, tgt_emb.shape)\n",
    "        self.outs = self.transformer(src_emb, tgt_emb)\n",
    "        #print(outs.shape)\n",
    "        logits = self.generator(self.outs)\n",
    "        #print(logits.shape)\n",
    "        #out = torch.topk(logits, k=1, dim=2).indices.reshape(-1,44)\n",
    "        #print(out, out.shape)\n",
    "        return logits\n",
    "    \n",
    "    def __deepcopy__(self, memo):\n",
    "        \"\"\"this is needed to make sure that the \n",
    "        non-leaf nodes do not\n",
    "        interfere with copy.deepcopy()\n",
    "        \"\"\"\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        memo[id(self)] = result\n",
    "        for k, v in self.__dict__.items():\n",
    "            setattr(result, k, copy.deepcopy(v, memo))\n",
    "        return result\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"this method is used only at the inference step\"\"\"\n",
    "        return self.transformer.encoder(\n",
    "                            self.embedding_src(src), src_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        \"\"\"this method is used only at the inference step\"\"\"\n",
    "        return self.transformer.decoder(\n",
    "                          self.embedding_tgt(tgt), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QATransformer(\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=64, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=64, bias=True)\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=64, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=64, bias=True)\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (embedding_src): Embedding(5500, 64, sparse=True)\n",
      "  (embedding_tgt): Embedding(5500, 64, sparse=True)\n",
      "  (generator): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5500\n",
    "\n",
    "src_vocab_size = vocab_size\n",
    "tgt_vocab_size = vocab_size\n",
    "emb_size = 64\n",
    "\n",
    "model = QATransformer(src_vocab_size, tgt_vocab_size, emb_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "This loss function is a adapted version of the Cross Entropy for the trnasformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_fn(output_of_network, label_of_dataloader):\n",
    "    #print(output_of_network.shape, label_of_dataloader.shape)\n",
    "    tgt_out = label_of_dataloader\n",
    "    #print(tgt_out)\n",
    "    logits = output_of_network\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    return cel(logits, tgt_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 5.765003 \tEpoch training accuracy: 1.17%                                                            \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matteocaorsi/Desktop/new_dir/giotto-deep/gdeep/pipeline/pipeline.py:358: UserWarning:\n",
      "\n",
      "Cannot store data in the PR curve\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " Accuracy: 0.00%,                 Avg loss: 5.579422 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 5.559848 \tEpoch training accuracy: 1.17%                                                             \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " Accuracy: 0.00%,                 Avg loss: 5.424694 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 5.460648 \tEpoch training accuracy: 1.17%                                                            \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " Accuracy: 1.56%,                 Avg loss: 5.428532 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.428531527519226, 1.5625)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare a pipeline class with the model, dataloaders loss_fn and tensorboard writer\n",
    "pipe = Pipeline(model, (dl_tr, dl_ts), loss_fn, writer)\n",
    "\n",
    "# train the model\n",
    "pipe.train(SGD, 3, False, {\"lr\":0.01}, {\"batch_size\":16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering questions!\n",
    "\n",
    "Here we have a question and its associated context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Computational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.',),\n",
       " ('What is a manual application of mathematical steps?',)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb = next(iter(dl_ts_str))\n",
    "bb[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Get the vocabulary and numericize the question and context to then input both to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = prec.vocabulary\n",
    "context = prec.tokenizer(bb[0][0])\n",
    "question = prec.tokenizer(bb[1][0])\n",
    "\n",
    "# get the indexes in the vocabulary of the tokens\n",
    "context_idx = torch.tensor(list(map(voc.__getitem__,context)))\n",
    "question_idx = torch.tensor(list(map(voc.__getitem__,question)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_to_pad = aa[0].shape[-1]\n",
    "pad_fn = lambda item : torch.cat([item, prec.pad_item * torch.ones(length_to_pad - item.shape[0])])\n",
    "\n",
    "# these tansors are ready for be fitted into teh model\n",
    "context_ready_for_model = pad_fn(context_idx)\n",
    "question_ready_for_model = pad_fn(question_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the two tensors of context and question together and input them to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.stack((context_ready_for_model, question_ready_for_model)).reshape(1,*aa[0].shape[1:]).long()\n",
    "out = model(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is the ligits for the start and end tokens of the answer. It is now time to extract them with `torch.argmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model was not able to find the answer.\n",
      "The actual answer was: ''\n"
     ]
    }
   ],
   "source": [
    "answer_idx = torch.argmax(out, dim=1)\n",
    "\n",
    "try:\n",
    "    if answer_idx[0][1] > answer_idx[0][0]:\n",
    "        print(\"The model proposes: '\", context[answer_idx[0][0]:answer_idx[0][1]],\"...'\")\n",
    "    else:\n",
    "        print(\"The model proposes: '\", context[answer_idx[0][0]],\"...'\")\n",
    "except IndexError:\n",
    "    print(\"The model was not able to find the answer.\")\n",
    "print(\"The actual answer was: '\" + bb[2][0][0]+\"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract inner data from your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.encoder.layers.0.self_attn.in_proj_weight torch.Size([192, 64])\n",
      "transformer.encoder.layers.0.self_attn.in_proj_bias torch.Size([192])\n",
      "transformer.encoder.layers.0.self_attn.out_proj.weight torch.Size([64, 64])\n",
      "transformer.encoder.layers.0.self_attn.out_proj.bias torch.Size([64])\n",
      "transformer.encoder.layers.0.linear1.weight torch.Size([512, 64])\n",
      "transformer.encoder.layers.0.linear1.bias torch.Size([512])\n",
      "transformer.encoder.layers.0.linear2.weight torch.Size([64, 512])\n",
      "transformer.encoder.layers.0.linear2.bias torch.Size([64])\n",
      "transformer.encoder.layers.0.norm1.weight torch.Size([64])\n",
      "transformer.encoder.layers.0.norm1.bias torch.Size([64])\n",
      "transformer.encoder.layers.0.norm2.weight torch.Size([64])\n",
      "transformer.encoder.layers.0.norm2.bias torch.Size([64])\n",
      "transformer.encoder.norm.weight torch.Size([64])\n",
      "transformer.encoder.norm.bias torch.Size([64])\n",
      "transformer.decoder.layers.0.self_attn.in_proj_weight torch.Size([192, 64])\n",
      "transformer.decoder.layers.0.self_attn.in_proj_bias torch.Size([192])\n",
      "transformer.decoder.layers.0.self_attn.out_proj.weight torch.Size([64, 64])\n",
      "transformer.decoder.layers.0.self_attn.out_proj.bias torch.Size([64])\n",
      "transformer.decoder.layers.0.multihead_attn.in_proj_weight torch.Size([192, 64])\n",
      "transformer.decoder.layers.0.multihead_attn.in_proj_bias torch.Size([192])\n",
      "transformer.decoder.layers.0.multihead_attn.out_proj.weight torch.Size([64, 64])\n",
      "transformer.decoder.layers.0.multihead_attn.out_proj.bias torch.Size([64])\n",
      "transformer.decoder.layers.0.linear1.weight torch.Size([512, 64])\n",
      "transformer.decoder.layers.0.linear1.bias torch.Size([512])\n",
      "transformer.decoder.layers.0.linear2.weight torch.Size([64, 512])\n",
      "transformer.decoder.layers.0.linear2.bias torch.Size([64])\n",
      "transformer.decoder.layers.0.norm1.weight torch.Size([64])\n",
      "transformer.decoder.layers.0.norm1.bias torch.Size([64])\n",
      "transformer.decoder.layers.0.norm2.weight torch.Size([64])\n",
      "transformer.decoder.layers.0.norm2.bias torch.Size([64])\n",
      "transformer.decoder.layers.0.norm3.weight torch.Size([64])\n",
      "transformer.decoder.layers.0.norm3.bias torch.Size([64])\n",
      "transformer.decoder.norm.weight torch.Size([64])\n",
      "transformer.decoder.norm.bias torch.Size([64])\n",
      "embedding_src.weight torch.Size([5500, 64])\n",
      "embedding_tgt.weight torch.Size([5500, 64])\n",
      "generator.weight torch.Size([2, 64])\n",
      "generator.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "from gdeep.models import ModelExtractor\n",
    "\n",
    "me = ModelExtractor(pipe.model, loss_fn)\n",
    "\n",
    "lista = me.get_layers_param()\n",
    "\n",
    "for k, item in lista.items():\n",
    "    print(k,item.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cpu\")\n",
    "x = next(iter(dl_tr))[0]\n",
    "pipe.model.eval()\n",
    "pipe.model(x.to(DEVICE))\n",
    "\n",
    "list_activations = me.get_activations(x)\n",
    "len(list_activations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(dl_tr))[0][0]\n",
    "if x.dtype is not torch.int64:\n",
    "    res = me.get_decision_boundary(x, n_epochs=1)\n",
    "    res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, target = next(iter(dl_tr))\n",
    "if x.dtype is torch.float:\n",
    "    for gradient in me.get_gradients(x, target=target)[1]:\n",
    "        print(gradient.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise activations and other topological aspects of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.visualisation import Visualiser\n",
    "\n",
    "vs = Visualiser(pipe)\n",
    "\n",
    "vs.plot_data_model()\n",
    "#vs.plot_activations(x)\n",
    "#vs.plot_persistence_diagrams(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
