{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdbddbfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml==5.4.1 in /usr/local/lib/python3.7/dist-packages (5.4.1)\n",
      "Using GPU!\n",
      "No TPUs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use float32 model\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6216005430221558  \tBatch training accuracy:  20.2125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 8s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 19.335938%,                 Avg loss: 1.613257 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6150968685150147  \tBatch training accuracy:  20.175  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 8s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 20.068359%,                 Avg loss: 1.613575 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.614751015663147  \tBatch training accuracy:  19.675  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 8s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 20.068359%,                 Avg loss: 1.610720 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.614335199356079  \tBatch training accuracy:  19.35  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 8s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 19.824219%,                 Avg loss: 1.611950 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6124178409576415  \tBatch training accuracy:  19.9125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 8s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 19.335938%,                 Avg loss: 1.610145 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6127348966598511  \tBatch training accuracy:  19.787499999999998  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 8s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 19.335938%,                 Avg loss: 1.609569 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6118319635391236  \tBatch training accuracy:  20.375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 19.335938%,                 Avg loss: 1.608431 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.611092167854309  \tBatch training accuracy:  20.200000000000003  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 19.824219%,                 Avg loss: 1.608709 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6110915126800538  \tBatch training accuracy:  20.3125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 18.457031%,                 Avg loss: 1.612467 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6108729248046876  \tBatch training accuracy:  20.724999999999998  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 18.457031%,                 Avg loss: 1.608770 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6100341339111328  \tBatch training accuracy:  19.9875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 19.824219%,                 Avg loss: 1.610164 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.610324785232544  \tBatch training accuracy:  20.849999999999998  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 20.849609%,                 Avg loss: 1.607998 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6090868530273437  \tBatch training accuracy:  21.65  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 19.824219%,                 Avg loss: 1.609744 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6095269708633422  \tBatch training accuracy:  20.4  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 19.824219%,                 Avg loss: 1.607451 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6087070989608765  \tBatch training accuracy:  20.7375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 21.142578%,                 Avg loss: 1.610104 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6085113744735717  \tBatch training accuracy:  20.2625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 19.824219%,                 Avg loss: 1.616110 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6134148197174072  \tBatch training accuracy:  19.412499999999998  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 21.386719%,                 Avg loss: 1.611691 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6104428596496583  \tBatch training accuracy:  20.3375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 17.822266%,                 Avg loss: 1.607301 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6087466621398925  \tBatch training accuracy:  20.7875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 20.507812%,                 Avg loss: 1.608480 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6090056867599487  \tBatch training accuracy:  20.25  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 19.580078%,                 Avg loss: 1.603824 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.5961093816757201  \tBatch training accuracy:  21.224999999999998  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 21.386719%,                 Avg loss: 1.592799 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.594988609313965  \tBatch training accuracy:  21.6  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 21.191406%,                 Avg loss: 1.593831 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.594850869178772  \tBatch training accuracy:  22.25  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 19.335938%,                 Avg loss: 1.610636 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6087908458709717  \tBatch training accuracy:  20.625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 18.457031%,                 Avg loss: 1.624786 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6085267324447632  \tBatch training accuracy:  20.8  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 21.679688%,                 Avg loss: 1.568529 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6035407390594483  \tBatch training accuracy:  21.6125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 19.335938%,                 Avg loss: 1.598094 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6005182733535768  \tBatch training accuracy:  21.9375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 17.773438%,                 Avg loss: 1.592812 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.5928796310424804  \tBatch training accuracy:  22.125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 29.052734%,                 Avg loss: 1.587277 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.596505509376526  \tBatch training accuracy:  21.5625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 20.605469%,                 Avg loss: 1.589973 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.5817528533935548  \tBatch training accuracy:  23.9875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 23.779297%,                 Avg loss: 1.531149 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.5857862472534179  \tBatch training accuracy:  22.875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 21.142578%,                 Avg loss: 1.592362 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.5892115306854249  \tBatch training accuracy:  23.0375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 21.191406%,                 Avg loss: 1.590893 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.595103982925415  \tBatch training accuracy:  22.0  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 24.804688%,                 Avg loss: 1.561104 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.588373203277588  \tBatch training accuracy:  22.6  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 19.921875%,                 Avg loss: 1.589626 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.5615481281280517  \tBatch training accuracy:  26.625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 22.656250%,                 Avg loss: 1.534884 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.5396662187576293  \tBatch training accuracy:  27.700000000000003  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 35.644531%,                 Avg loss: 1.461043 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.5197690839767457  \tBatch training accuracy:  29.925  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 32.861328%,                 Avg loss: 1.506123 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.477287645339966  \tBatch training accuracy:  31.95  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 26.806641%,                 Avg loss: 1.481340 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.4522870378494264  \tBatch training accuracy:  33.7125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 42.871094%,                 Avg loss: 1.329428 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.3817278242111206  \tBatch training accuracy:  37.262499999999996  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 30.175781%,                 Avg loss: 1.430013 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.324362663269043  \tBatch training accuracy:  40.9375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 46.435547%,                 Avg loss: 1.187387 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.2663970241546632  \tBatch training accuracy:  43.0875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 40.087891%,                 Avg loss: 1.289507 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.2207831788063048  \tBatch training accuracy:  46.6  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 54.150391%,                 Avg loss: 1.068173 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.1474088649749756  \tBatch training accuracy:  49.625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 54.248047%,                 Avg loss: 0.988650 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.8299323153495789  \tBatch training accuracy:  65.325  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 71.142578%,                 Avg loss: 0.700080 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.6636040644645691  \tBatch training accuracy:  73.625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 73.925781%,                 Avg loss: 0.616038 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.5940992736816406  \tBatch training accuracy:  76.4  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 76.513672%,                 Avg loss: 0.537681 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.5418457462787628  \tBatch training accuracy:  78.77499999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 79.638672%,                 Avg loss: 0.480249 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.5056758148670196  \tBatch training accuracy:  80.2375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 79.736328%,                 Avg loss: 0.473283 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.4446865304708481  \tBatch training accuracy:  82.175  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 83.154297%,                 Avg loss: 0.370967 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.3887522344589233  \tBatch training accuracy:  84.6375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 84.667969%,                 Avg loss: 0.341864 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.38460434901714324  \tBatch training accuracy:  84.5  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 85.058594%,                 Avg loss: 0.318826 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.34230856573581697  \tBatch training accuracy:  85.925  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.035156%,                 Avg loss: 0.291548 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.3743042770624161  \tBatch training accuracy:  84.6875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 85.888672%,                 Avg loss: 0.301399 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.3387613149881363  \tBatch training accuracy:  86.0125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.328125%,                 Avg loss: 0.284635 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.3278849651813507  \tBatch training accuracy:  86.5625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.181641%,                 Avg loss: 0.278813 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.3111267784833908  \tBatch training accuracy:  86.95  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.425781%,                 Avg loss: 0.264820 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.30119827032089236  \tBatch training accuracy:  87.8375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.060547%,                 Avg loss: 0.275057 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.3024398438930512  \tBatch training accuracy:  88.0625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.304688%,                 Avg loss: 0.246452 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.29363064479827883  \tBatch training accuracy:  87.7625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.718750%,                 Avg loss: 0.258752 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.29654616153240204  \tBatch training accuracy:  88.08749999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.255859%,                 Avg loss: 0.257502 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.3178663486242294  \tBatch training accuracy:  87.5  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.402344%,                 Avg loss: 0.249619 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.3045624693632126  \tBatch training accuracy:  87.575  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.816406%,                 Avg loss: 0.274376 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.276860204577446  \tBatch training accuracy:  88.4875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.158203%,                 Avg loss: 0.249218 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.28963599717617033  \tBatch training accuracy:  88.5625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.816406%,                 Avg loss: 0.267357 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2863060779571533  \tBatch training accuracy:  88.275  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 85.839844%,                 Avg loss: 0.278916 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2821277708411217  \tBatch training accuracy:  88.3125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.621094%,                 Avg loss: 0.266948 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.31428318792581555  \tBatch training accuracy:  87.35000000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.060547%,                 Avg loss: 0.282885 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.28905612874031067  \tBatch training accuracy:  88.3375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.304688%,                 Avg loss: 0.257334 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.27382597714662554  \tBatch training accuracy:  88.52499999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.304688%,                 Avg loss: 0.257021 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2768097150325775  \tBatch training accuracy:  88.8375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.158203%,                 Avg loss: 0.249186 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2912584776878357  \tBatch training accuracy:  88.03750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 12s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.500000%,                 Avg loss: 0.239187 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.26615413904190066  \tBatch training accuracy:  88.97500000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 11s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.890625%,                 Avg loss: 0.237091 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.29086628544330595  \tBatch training accuracy:  87.925  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 12s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.328125%,                 Avg loss: 0.268998 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.28356041538715365  \tBatch training accuracy:  88.1375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 11s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.962891%,                 Avg loss: 0.261233 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2748874925374985  \tBatch training accuracy:  88.8375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 13s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.500000%,                 Avg loss: 0.248742 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2759519997835159  \tBatch training accuracy:  88.8625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 11s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 83.544922%,                 Avg loss: 0.417138 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2826721316576004  \tBatch training accuracy:  88.7625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 10s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.939453%,                 Avg loss: 0.239499 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2766648679971695  \tBatch training accuracy:  89.03750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 10s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 85.839844%,                 Avg loss: 0.282536 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2677702242732048  \tBatch training accuracy:  89.2  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.744141%,                 Avg loss: 0.228656 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2635212434530258  \tBatch training accuracy:  88.9625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.227927 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.26257677280902864  \tBatch training accuracy:  89.0  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.060547%,                 Avg loss: 0.261809 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2944650138616562  \tBatch training accuracy:  88.44999999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.207031%,                 Avg loss: 0.248026 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2951957216262817  \tBatch training accuracy:  88.35  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 84.423828%,                 Avg loss: 0.325061 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.41649793648719785  \tBatch training accuracy:  83.9875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.132812%,                 Avg loss: 0.269170 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.29459971296787263  \tBatch training accuracy:  87.7125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.060547%,                 Avg loss: 0.262020 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.26709985089302063  \tBatch training accuracy:  88.8375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.255859%,                 Avg loss: 0.242638 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2664553482532501  \tBatch training accuracy:  88.8625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.060547%,                 Avg loss: 0.241068 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.26308012664318087  \tBatch training accuracy:  89.075  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.816406%,                 Avg loss: 0.265358 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.264799569606781  \tBatch training accuracy:  89.47500000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.134766%,                 Avg loss: 0.233794 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.273958946287632  \tBatch training accuracy:  88.675  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.744141%,                 Avg loss: 0.247078 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.26899628269672393  \tBatch training accuracy:  88.94999999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.939453%,                 Avg loss: 0.231665 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.26271047979593276  \tBatch training accuracy:  89.45  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.207031%,                 Avg loss: 0.256804 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2656544625163078  \tBatch training accuracy:  89.0125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.500000%,                 Avg loss: 0.229842 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.27804272389411927  \tBatch training accuracy:  88.575  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.720703%,                 Avg loss: 0.226590 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.26393785667419434  \tBatch training accuracy:  89.2875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.744141%,                 Avg loss: 0.241194 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2672300555706024  \tBatch training accuracy:  88.7  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.353516%,                 Avg loss: 0.244314 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2613816214799881  \tBatch training accuracy:  89.47500000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.939453%,                 Avg loss: 0.225061 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2522837753891945  \tBatch training accuracy:  89.53750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.402344%,                 Avg loss: 0.244869 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.25705128943920136  \tBatch training accuracy:  89.53750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.376953%,                 Avg loss: 0.270209 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2556734910607338  \tBatch training accuracy:  89.3125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.226204 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.25633421593904493  \tBatch training accuracy:  89.53750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.230469%,                 Avg loss: 0.267886 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2626953721046448  \tBatch training accuracy:  89.2  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.744141%,                 Avg loss: 0.229176 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.25676638579368594  \tBatch training accuracy:  89.4625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.353516%,                 Avg loss: 0.248452 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.26520579087734225  \tBatch training accuracy:  89.0  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.183594%,                 Avg loss: 0.219919 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2657002116441727  \tBatch training accuracy:  89.0  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.238038 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2748488854765892  \tBatch training accuracy:  88.925  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.695312%,                 Avg loss: 0.234908 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2612940809726715  \tBatch training accuracy:  89.35  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.304688%,                 Avg loss: 0.251863 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.26151902168989183  \tBatch training accuracy:  89.25  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.255859%,                 Avg loss: 0.264020 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2629008027911186  \tBatch training accuracy:  89.225  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.085938%,                 Avg loss: 0.224032 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24915443909168244  \tBatch training accuracy:  89.825  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.572266%,                 Avg loss: 0.284988 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.30146824979782105  \tBatch training accuracy:  88.225  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.988281%,                 Avg loss: 0.245877 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2600345772504806  \tBatch training accuracy:  89.3  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.962891%,                 Avg loss: 0.268591 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2495206117630005  \tBatch training accuracy:  89.625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.841797%,                 Avg loss: 0.237901 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.34774558812379835  \tBatch training accuracy:  85.975  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 82.128906%,                 Avg loss: 0.383187 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.33225692880153657  \tBatch training accuracy:  86.825  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.476562%,                 Avg loss: 0.225821 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2698598802089691  \tBatch training accuracy:  88.9375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.865234%,                 Avg loss: 0.262701 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.25956294596195223  \tBatch training accuracy:  88.925  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.939453%,                 Avg loss: 0.232522 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.28066275370121  \tBatch training accuracy:  88.9  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.328125%,                 Avg loss: 0.267461 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2661861910223961  \tBatch training accuracy:  89.1375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.060547%,                 Avg loss: 0.244484 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.25548000282049177  \tBatch training accuracy:  89.55  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.646484%,                 Avg loss: 0.238600 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.26114083647727965  \tBatch training accuracy:  89.3875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.474609%,                 Avg loss: 0.274407 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2526713156104088  \tBatch training accuracy:  89.4875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.792969%,                 Avg loss: 0.231074 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2566035677790642  \tBatch training accuracy:  89.45  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.646484%,                 Avg loss: 0.227402 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2554566735625267  \tBatch training accuracy:  89.3  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.207031%,                 Avg loss: 0.245645 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24877647721767426  \tBatch training accuracy:  89.51249999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.792969%,                 Avg loss: 0.230496 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2552876845598221  \tBatch training accuracy:  89.4625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.525391%,                 Avg loss: 0.225703 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.25745215320587156  \tBatch training accuracy:  89.3  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.914062%,                 Avg loss: 0.263597 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.25828944450616836  \tBatch training accuracy:  89.075  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.085938%,                 Avg loss: 0.226837 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2521224193572998  \tBatch training accuracy:  89.53750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.500000%,                 Avg loss: 0.243173 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2516186751127243  \tBatch training accuracy:  89.51249999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.841797%,                 Avg loss: 0.218073 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24657334876060485  \tBatch training accuracy:  89.85  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.207031%,                 Avg loss: 0.255727 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2505407946705818  \tBatch training accuracy:  89.55  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.378906%,                 Avg loss: 0.223984 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2692840910553932  \tBatch training accuracy:  88.925  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.718750%,                 Avg loss: 0.255869 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2602419185042381  \tBatch training accuracy:  89.3125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.962891%,                 Avg loss: 0.266266 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2522758481502533  \tBatch training accuracy:  89.6375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.718750%,                 Avg loss: 0.258559 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2509120451211929  \tBatch training accuracy:  89.3875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.890625%,                 Avg loss: 0.236602 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24756472635269164  \tBatch training accuracy:  89.17500000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.109375%,                 Avg loss: 0.238672 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2708867303729057  \tBatch training accuracy:  88.8875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.523438%,                 Avg loss: 0.273300 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.26518817311525344  \tBatch training accuracy:  89.3  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.451172%,                 Avg loss: 0.227876 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.25453597933053973  \tBatch training accuracy:  89.2625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.744141%,                 Avg loss: 0.243283 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.25555471891164777  \tBatch training accuracy:  89.4125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.427734%,                 Avg loss: 0.213763 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24528796869516373  \tBatch training accuracy:  89.4625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.841797%,                 Avg loss: 0.221035 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.256934679210186  \tBatch training accuracy:  89.225  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.572266%,                 Avg loss: 0.265430 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24710726195573807  \tBatch training accuracy:  89.7375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.085938%,                 Avg loss: 0.231482 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.28142256164550783  \tBatch training accuracy:  88.3125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.158203%,                 Avg loss: 0.269353 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2690873839855194  \tBatch training accuracy:  88.775  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.378906%,                 Avg loss: 0.221439 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2813328234553337  \tBatch training accuracy:  88.61250000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.548828%,                 Avg loss: 0.259899 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.28331443351507185  \tBatch training accuracy:  88.6875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.476562%,                 Avg loss: 0.223116 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2527099221944809  \tBatch training accuracy:  89.64999999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.695312%,                 Avg loss: 0.242448 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2457084292769432  \tBatch training accuracy:  89.67500000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.720703%,                 Avg loss: 0.220422 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24088710248470308  \tBatch training accuracy:  89.97500000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.841797%,                 Avg loss: 0.237209 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24238807183504105  \tBatch training accuracy:  89.8625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.646484%,                 Avg loss: 0.241301 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24947437399625777  \tBatch training accuracy:  89.71249999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.841797%,                 Avg loss: 0.241262 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2481391761302948  \tBatch training accuracy:  89.60000000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.229299 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24614216440916062  \tBatch training accuracy:  89.7625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.695312%,                 Avg loss: 0.240396 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24348892760276794  \tBatch training accuracy:  89.825  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.330078%,                 Avg loss: 0.218561 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2453680775165558  \tBatch training accuracy:  89.8625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.646484%,                 Avg loss: 0.231092 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24225042957067489  \tBatch training accuracy:  89.8  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.244850 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "No TPUs\n",
      "\n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.425781%,                 Avg loss: 0.269393 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24396318036317827  \tBatch training accuracy:  89.55  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.281250%,                 Avg loss: 0.216629 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2510164825320244  \tBatch training accuracy:  89.53750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.646484%,                 Avg loss: 0.239512 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2515523295402527  \tBatch training accuracy:  89.4375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.207031%,                 Avg loss: 0.236231 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24300710874795914  \tBatch training accuracy:  89.7875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.353516%,                 Avg loss: 0.250357 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24648522794246674  \tBatch training accuracy:  89.8625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.085938%,                 Avg loss: 0.226614 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2439373136162758  \tBatch training accuracy:  89.58749999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.330078%,                 Avg loss: 0.239064 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2498701531291008  \tBatch training accuracy:  89.75  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.476562%,                 Avg loss: 0.224654 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24523676121234894  \tBatch training accuracy:  89.53750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.255859%,                 Avg loss: 0.239056 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.25797923231124875  \tBatch training accuracy:  89.3  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.623047%,                 Avg loss: 0.216987 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2972400385141373  \tBatch training accuracy:  88.1625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.451172%,                 Avg loss: 0.249011 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24633330327272415  \tBatch training accuracy:  89.7375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.227460 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2380280156135559  \tBatch training accuracy:  89.9875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.134766%,                 Avg loss: 0.231651 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.26275197917222975  \tBatch training accuracy:  89.1625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 83.886719%,                 Avg loss: 0.379217 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.26973940938711166  \tBatch training accuracy:  89.1125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.792969%,                 Avg loss: 0.233941 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2604418439865112  \tBatch training accuracy:  89.4375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.597656%,                 Avg loss: 0.223544 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2489344100356102  \tBatch training accuracy:  89.4375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.232422%,                 Avg loss: 0.217991 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.260120955824852  \tBatch training accuracy:  89.1125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.223879 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24407127177715301  \tBatch training accuracy:  89.725  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.451172%,                 Avg loss: 0.239579 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24114194267988204  \tBatch training accuracy:  89.9125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.988281%,                 Avg loss: 0.225551 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2407158653140068  \tBatch training accuracy:  90.025  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.890625%,                 Avg loss: 0.237217 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24225358664989471  \tBatch training accuracy:  89.8125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.792969%,                 Avg loss: 0.236765 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.252681282222271  \tBatch training accuracy:  89.8  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 10s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.378906%,                 Avg loss: 0.218611 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23920040142536164  \tBatch training accuracy:  89.85  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.330078%,                 Avg loss: 0.216046 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23399967634677887  \tBatch training accuracy:  90.2625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.792969%,                 Avg loss: 0.243736 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24366509240865708  \tBatch training accuracy:  89.8875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.304688%,                 Avg loss: 0.239774 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2537172817587853  \tBatch training accuracy:  89.64999999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.988281%,                 Avg loss: 0.224076 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24338355642557144  \tBatch training accuracy:  89.6875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.597656%,                 Avg loss: 0.232521 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24238888132572173  \tBatch training accuracy:  89.9  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.816406%,                 Avg loss: 0.252119 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24685370481014252  \tBatch training accuracy:  89.8625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.816406%,                 Avg loss: 0.282899 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2474664239883423  \tBatch training accuracy:  89.6375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.427734%,                 Avg loss: 0.217277 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23991747879981995  \tBatch training accuracy:  89.725  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.281250%,                 Avg loss: 0.209436 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24472768026590347  \tBatch training accuracy:  89.58749999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.234089 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24890250247716902  \tBatch training accuracy:  89.7625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.695312%,                 Avg loss: 0.235030 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23963953042030334  \tBatch training accuracy:  89.9625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 85.205078%,                 Avg loss: 0.275808 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2558136565685272  \tBatch training accuracy:  89.21249999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.234709 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.25498937249183656  \tBatch training accuracy:  89.5625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.623047%,                 Avg loss: 0.213232 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2370909338593483  \tBatch training accuracy:  90.10000000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.085938%,                 Avg loss: 0.243682 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24674150043725968  \tBatch training accuracy:  90.10000000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.134766%,                 Avg loss: 0.247454 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24690452784299852  \tBatch training accuracy:  89.8375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.646484%,                 Avg loss: 0.248659 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.3619509326219559  \tBatch training accuracy:  86.325  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 82.031250%,                 Avg loss: 0.378863 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.27919202822446826  \tBatch training accuracy:  88.53750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.597656%,                 Avg loss: 0.243698 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2529892050027847  \tBatch training accuracy:  89.60000000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.671875%,                 Avg loss: 0.215061 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.25016659587621687  \tBatch training accuracy:  89.75  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.744141%,                 Avg loss: 0.232779 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23978270953893663  \tBatch training accuracy:  90.08749999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.890625%,                 Avg loss: 0.224275 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23479279404878617  \tBatch training accuracy:  90.3875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.085938%,                 Avg loss: 0.211762 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23624466633796692  \tBatch training accuracy:  90.1375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.597656%,                 Avg loss: 0.229335 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24200708001852037  \tBatch training accuracy:  89.875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.476562%,                 Avg loss: 0.213197 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22831126546859742  \tBatch training accuracy:  90.25  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.402344%,                 Avg loss: 0.226690 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2343031598329544  \tBatch training accuracy:  90.45  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.231909 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2678422469496727  \tBatch training accuracy:  89.1125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.255859%,                 Avg loss: 0.256550 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24914919006824493  \tBatch training accuracy:  89.60000000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.525391%,                 Avg loss: 0.218869 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23684529316425323  \tBatch training accuracy:  90.03750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.988281%,                 Avg loss: 0.220607 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23802280288934707  \tBatch training accuracy:  89.85  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.353516%,                 Avg loss: 0.238521 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23777509197592736  \tBatch training accuracy:  89.825  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.427734%,                 Avg loss: 0.213320 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23331478261947633  \tBatch training accuracy:  90.1125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.232422%,                 Avg loss: 0.219296 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2385042240023613  \tBatch training accuracy:  89.6625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.623047%,                 Avg loss: 0.209061 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24312332421541213  \tBatch training accuracy:  89.625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.476562%,                 Avg loss: 0.213894 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24098343521356583  \tBatch training accuracy:  89.71249999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.525391%,                 Avg loss: 0.209650 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23497976851463317  \tBatch training accuracy:  90.14999999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.525391%,                 Avg loss: 0.215188 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24254119688272477  \tBatch training accuracy:  89.625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.353516%,                 Avg loss: 0.245438 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2436701915860176  \tBatch training accuracy:  89.95  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.402344%,                 Avg loss: 0.236997 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2418968321084976  \tBatch training accuracy:  89.8625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.427734%,                 Avg loss: 0.213217 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23717132890224457  \tBatch training accuracy:  90.21249999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.695312%,                 Avg loss: 0.223265 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23781117212772368  \tBatch training accuracy:  90.1625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.330078%,                 Avg loss: 0.220632 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2348297256231308  \tBatch training accuracy:  90.10000000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.134766%,                 Avg loss: 0.226779 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24592567026615142  \tBatch training accuracy:  89.67500000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.183594%,                 Avg loss: 0.224816 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24024102479219436  \tBatch training accuracy:  90.1625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.474609%,                 Avg loss: 0.253757 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24023266869783402  \tBatch training accuracy:  89.71249999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.988281%,                 Avg loss: 0.225350 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.235181094288826  \tBatch training accuracy:  89.95  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.353516%,                 Avg loss: 0.232210 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24946986830234527  \tBatch training accuracy:  89.2875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.281250%,                 Avg loss: 0.242883 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.26431252211332323  \tBatch training accuracy:  89.275  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.841797%,                 Avg loss: 0.236121 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.26354482251405714  \tBatch training accuracy:  89.08749999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.402344%,                 Avg loss: 0.238926 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24226511961221694  \tBatch training accuracy:  89.525  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.769531%,                 Avg loss: 0.213518 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2375709342956543  \tBatch training accuracy:  90.3625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.939453%,                 Avg loss: 0.222204 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2359508686065674  \tBatch training accuracy:  90.0625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.988281%,                 Avg loss: 0.225170 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24204111778736115  \tBatch training accuracy:  89.7875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.085938%,                 Avg loss: 0.221997 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24400420367717743  \tBatch training accuracy:  89.8625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.230090 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24238055884838106  \tBatch training accuracy:  89.875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.281250%,                 Avg loss: 0.225909 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2470449994802475  \tBatch training accuracy:  90.1375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.841797%,                 Avg loss: 0.220404 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23488381099700928  \tBatch training accuracy:  90.075  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.939453%,                 Avg loss: 0.217305 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2353588980436325  \tBatch training accuracy:  90.08749999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.988281%,                 Avg loss: 0.231659 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22939918732643128  \tBatch training accuracy:  90.225  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.890625%,                 Avg loss: 0.227526 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23673315709829332  \tBatch training accuracy:  90.21249999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.378906%,                 Avg loss: 0.227469 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24818203157186508  \tBatch training accuracy:  89.875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.304688%,                 Avg loss: 0.245782 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.25750026297569273  \tBatch training accuracy:  89.2875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.425781%,                 Avg loss: 0.277100 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24787277764081955  \tBatch training accuracy:  89.6375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.939453%,                 Avg loss: 0.239956 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24451964384317398  \tBatch training accuracy:  89.8375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.988281%,                 Avg loss: 0.233975 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24137563437223433  \tBatch training accuracy:  89.9375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.255859%,                 Avg loss: 0.266519 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.317393607378006  \tBatch training accuracy:  87.91250000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.744141%,                 Avg loss: 0.235762 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.27664014089107514  \tBatch training accuracy:  88.91250000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.476562%,                 Avg loss: 0.215117 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24931868237257004  \tBatch training accuracy:  89.47500000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.818359%,                 Avg loss: 0.214073 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2448207967877388  \tBatch training accuracy:  90.075  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.134766%,                 Avg loss: 0.220588 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23001838451623915  \tBatch training accuracy:  90.2375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.353516%,                 Avg loss: 0.242636 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24460396784543992  \tBatch training accuracy:  89.8125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.476562%,                 Avg loss: 0.210078 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2294770896434784  \tBatch training accuracy:  90.35  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.744141%,                 Avg loss: 0.232128 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23258533698320388  \tBatch training accuracy:  90.01249999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.939453%,                 Avg loss: 0.233575 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24366001564264297  \tBatch training accuracy:  89.97500000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.574219%,                 Avg loss: 0.214562 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2384370222091675  \tBatch training accuracy:  90.3125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.353516%,                 Avg loss: 0.236658 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23177549612522125  \tBatch training accuracy:  90.1625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.330078%,                 Avg loss: 0.221324 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23871291893720628  \tBatch training accuracy:  89.9375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.279297%,                 Avg loss: 0.273106 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23099621510505677  \tBatch training accuracy:  90.0625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.525391%,                 Avg loss: 0.211357 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23150641858577728  \tBatch training accuracy:  90.1875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.427734%,                 Avg loss: 0.207773 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2361363172531128  \tBatch training accuracy:  90.125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.988281%,                 Avg loss: 0.220494 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2376784092783928  \tBatch training accuracy:  90.1875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.220688 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23563340401649474  \tBatch training accuracy:  89.9875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.085938%,                 Avg loss: 0.215124 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2304508373737335  \tBatch training accuracy:  90.2625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.427734%,                 Avg loss: 0.223386 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23318008780479432  \tBatch training accuracy:  90.325  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.427734%,                 Avg loss: 0.214635 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24062230223417283  \tBatch training accuracy:  90.01249999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.988281%,                 Avg loss: 0.225551 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22896470558643342  \tBatch training accuracy:  90.4125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.232422%,                 Avg loss: 0.212934 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22777631509304047  \tBatch training accuracy:  90.1375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.669922%,                 Avg loss: 0.269135 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24530103808641435  \tBatch training accuracy:  89.8125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 85.742188%,                 Avg loss: 0.289103 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2392744660973549  \tBatch training accuracy:  90.03750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.939453%,                 Avg loss: 0.231485 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24471824258565902  \tBatch training accuracy:  90.0625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.744141%,                 Avg loss: 0.225181 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23518932420015334  \tBatch training accuracy:  90.25  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.427734%,                 Avg loss: 0.219711 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2418823363184929  \tBatch training accuracy:  89.9625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.427734%,                 Avg loss: 0.215542 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23448986625671386  \tBatch training accuracy:  90.14999999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.988281%,                 Avg loss: 0.219489 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23764404743909837  \tBatch training accuracy:  90.1875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.890625%,                 Avg loss: 0.227630 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2366126770377159  \tBatch training accuracy:  90.0  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.402344%,                 Avg loss: 0.237084 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23144765231013298  \tBatch training accuracy:  90.2  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.427734%,                 Avg loss: 0.222598 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23871822273731233  \tBatch training accuracy:  90.25  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.962891%,                 Avg loss: 0.263499 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.35977596473693846  \tBatch training accuracy:  87.47500000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 47.509766%,                 Avg loss: 1.895761 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.39986256194114683  \tBatch training accuracy:  84.98750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.695312%,                 Avg loss: 0.237416 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2608370687961578  \tBatch training accuracy:  89.3125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.890625%,                 Avg loss: 0.229830 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.25436461770534513  \tBatch training accuracy:  89.3875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.988281%,                 Avg loss: 0.226463 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2459417051076889  \tBatch training accuracy:  89.64999999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.792969%,                 Avg loss: 0.238724 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2360620360970497  \tBatch training accuracy:  90.1125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.134766%,                 Avg loss: 0.223246 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2537187336087227  \tBatch training accuracy:  89.71249999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.623047%,                 Avg loss: 0.220207 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2413962464928627  \tBatch training accuracy:  89.85  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.451172%,                 Avg loss: 0.233234 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23117031073570252  \tBatch training accuracy:  90.1875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.353516%,                 Avg loss: 0.229767 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23359831845760345  \tBatch training accuracy:  90.4375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.085938%,                 Avg loss: 0.227115 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24613921970129013  \tBatch training accuracy:  89.725  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.330078%,                 Avg loss: 0.217996 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24066175085306168  \tBatch training accuracy:  90.05  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.476562%,                 Avg loss: 0.220729 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2370914059281349  \tBatch training accuracy:  89.85  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.623047%,                 Avg loss: 0.223262 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23591727113723754  \tBatch training accuracy:  90.1625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.695312%,                 Avg loss: 0.231063 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22888185095787048  \tBatch training accuracy:  90.25  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.574219%,                 Avg loss: 0.227401 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23076181679964067  \tBatch training accuracy:  90.3375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.890625%,                 Avg loss: 0.224548 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.226802683532238  \tBatch training accuracy:  90.4  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.939453%,                 Avg loss: 0.224361 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2311703894138336  \tBatch training accuracy:  90.3375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.695312%,                 Avg loss: 0.225985 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2272591912150383  \tBatch training accuracy:  90.6375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.890625%,                 Avg loss: 0.234603 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23671442645788193  \tBatch training accuracy:  89.9125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.451172%,                 Avg loss: 0.241899 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.235403824031353  \tBatch training accuracy:  90.2625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.255859%,                 Avg loss: 0.246434 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.239079265832901  \tBatch training accuracy:  89.9875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.744141%,                 Avg loss: 0.236938 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23459773576259613  \tBatch training accuracy:  90.1125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.548828%,                 Avg loss: 0.228986 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2272077696323395  \tBatch training accuracy:  90.5875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.134766%,                 Avg loss: 0.227815 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22559092491865157  \tBatch training accuracy:  90.64999999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.330078%,                 Avg loss: 0.222897 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22863285118341445  \tBatch training accuracy:  90.3375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.232422%,                 Avg loss: 0.228294 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22839649468660356  \tBatch training accuracy:  90.2375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.330078%,                 Avg loss: 0.218087 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23205323624610902  \tBatch training accuracy:  89.97500000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.378906%,                 Avg loss: 0.220803 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.3489401794075966  \tBatch training accuracy:  86.3625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 85.937500%,                 Avg loss: 0.304217 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.27849093413352966  \tBatch training accuracy:  88.52499999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.841797%,                 Avg loss: 0.237859 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.25146838372945785  \tBatch training accuracy:  89.53750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 84.667969%,                 Avg loss: 0.359595 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24553095400333405  \tBatch training accuracy:  89.8375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.230540 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23490119153261185  \tBatch training accuracy:  90.21249999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.841797%,                 Avg loss: 0.222469 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2303448430299759  \tBatch training accuracy:  90.4875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.476562%,                 Avg loss: 0.224501 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2289107846021652  \tBatch training accuracy:  90.1875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.525391%,                 Avg loss: 0.221895 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22446312057971954  \tBatch training accuracy:  90.8125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.427734%,                 Avg loss: 0.216928 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23251423102617264  \tBatch training accuracy:  90.275  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.183594%,                 Avg loss: 0.216459 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.26662377661466596  \tBatch training accuracy:  89.1375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 85.595703%,                 Avg loss: 0.297960 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24813069331645965  \tBatch training accuracy:  89.64999999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.744141%,                 Avg loss: 0.229880 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23632918453216553  \tBatch training accuracy:  90.21249999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.451172%,                 Avg loss: 0.238514 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2282456393837929  \tBatch training accuracy:  90.1375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.623047%,                 Avg loss: 0.215435 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22827842861413955  \tBatch training accuracy:  90.325  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.525391%,                 Avg loss: 0.218726 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2247988475561142  \tBatch training accuracy:  90.375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.183594%,                 Avg loss: 0.228223 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22671107375621796  \tBatch training accuracy:  90.2  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.330078%,                 Avg loss: 0.213859 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24170822042226792  \tBatch training accuracy:  89.925  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 84.423828%,                 Avg loss: 0.336336 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2372602455019951  \tBatch training accuracy:  90.05  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.525391%,                 Avg loss: 0.216777 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2271787285208702  \tBatch training accuracy:  90.225  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.353516%,                 Avg loss: 0.247349 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2308681458234787  \tBatch training accuracy:  90.21249999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.427734%,                 Avg loss: 0.217678 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22424906516075135  \tBatch training accuracy:  90.325  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.841797%,                 Avg loss: 0.234782 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22941369885206223  \tBatch training accuracy:  90.2625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.330078%,                 Avg loss: 0.214597 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22895578706264497  \tBatch training accuracy:  90.375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.085938%,                 Avg loss: 0.222260 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2319187272787094  \tBatch training accuracy:  90.25  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.574219%,                 Avg loss: 0.205988 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23049873977899552  \tBatch training accuracy:  90.4625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.744141%,                 Avg loss: 0.229447 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22473397564888  \tBatch training accuracy:  90.525  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.281250%,                 Avg loss: 0.219963 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.25180459958314894  \tBatch training accuracy:  89.925  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.597656%,                 Avg loss: 0.216483 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22944727832078934  \tBatch training accuracy:  90.2625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.134766%,                 Avg loss: 0.276230 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23841599774360656  \tBatch training accuracy:  90.01249999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.744141%,                 Avg loss: 0.226360 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22988339608907699  \tBatch training accuracy:  90.3125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.232422%,                 Avg loss: 0.222643 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22797349894046784  \tBatch training accuracy:  90.525  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.134766%,                 Avg loss: 0.225162 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2339803807735443  \tBatch training accuracy:  90.1125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.792969%,                 Avg loss: 0.217430 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22339716827869416  \tBatch training accuracy:  90.71249999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.085938%,                 Avg loss: 0.218845 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23257725316286088  \tBatch training accuracy:  90.25  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.792969%,                 Avg loss: 0.227774 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2325203874707222  \tBatch training accuracy:  90.075  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.818359%,                 Avg loss: 0.212101 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2586180694103241  \tBatch training accuracy:  89.3625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.207031%,                 Avg loss: 0.250994 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.25857154619693756  \tBatch training accuracy:  89.2375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.476562%,                 Avg loss: 0.224722 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2420267380475998  \tBatch training accuracy:  89.95  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.476562%,                 Avg loss: 0.211696 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24633033722639083  \tBatch training accuracy:  89.64999999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.988281%,                 Avg loss: 0.230519 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.229177077293396  \tBatch training accuracy:  90.3  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.939453%,                 Avg loss: 0.230172 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2249543228149414  \tBatch training accuracy:  90.9125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.792969%,                 Avg loss: 0.235627 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.229423630297184  \tBatch training accuracy:  90.14999999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.769531%,                 Avg loss: 0.206629 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22489304089546203  \tBatch training accuracy:  90.2875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.939453%,                 Avg loss: 0.224131 \n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23589094132184982  \tBatch training accuracy:  89.9625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.378906%,                 Avg loss: 0.226815 \n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2339802567958832  \tBatch training accuracy:  90.025  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.222041 \n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23161973217129708  \tBatch training accuracy:  90.125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.330078%,                 Avg loss: 0.216255 \n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2363782263994217  \tBatch training accuracy:  90.5875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.402344%,                 Avg loss: 0.237525 \n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.31010965448617933  \tBatch training accuracy:  88.175  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.669922%,                 Avg loss: 0.282108 \n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2776823053359985  \tBatch training accuracy:  89.075  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.085938%,                 Avg loss: 0.238291 \n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24290459328889846  \tBatch training accuracy:  89.875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.085938%,                 Avg loss: 0.223873 \n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2599659962654114  \tBatch training accuracy:  89.2625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.695312%,                 Avg loss: 0.231831 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2440299344062805  \tBatch training accuracy:  89.8625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.695312%,                 Avg loss: 0.240864 \n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23568364256620408  \tBatch training accuracy:  90.225  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.939453%,                 Avg loss: 0.223880 \n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2348210631608963  \tBatch training accuracy:  90.03750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.232422%,                 Avg loss: 0.221581 \n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22810528564453125  \tBatch training accuracy:  90.325  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.476562%,                 Avg loss: 0.211968 \n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2288865446448326  \tBatch training accuracy:  90.3625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.427734%,                 Avg loss: 0.212975 \n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22874632400274278  \tBatch training accuracy:  90.3  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.939453%,                 Avg loss: 0.231729 \n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2308657949566841  \tBatch training accuracy:  90.53750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.427734%,                 Avg loss: 0.221534 \n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22680078768730164  \tBatch training accuracy:  90.45  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.134766%,                 Avg loss: 0.227215 \n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.227304650247097  \tBatch training accuracy:  90.3  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.234750 \n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2319740149974823  \tBatch training accuracy:  90.17500000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.330078%,                 Avg loss: 0.213217 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22041955375671388  \tBatch training accuracy:  90.85  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.227500 \n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22877986919879914  \tBatch training accuracy:  90.3625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.183594%,                 Avg loss: 0.223130 \n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22292204481363295  \tBatch training accuracy:  90.6375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.330078%,                 Avg loss: 0.215639 \n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23739708125591277  \tBatch training accuracy:  90.1375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.255859%,                 Avg loss: 0.233497 \n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24038073015213013  \tBatch training accuracy:  89.95  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.500000%,                 Avg loss: 0.241495 \n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.26105214363336565  \tBatch training accuracy:  88.9625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.183594%,                 Avg loss: 0.230826 \n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23578479164838792  \tBatch training accuracy:  90.08749999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.500000%,                 Avg loss: 0.234753 \n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2289036014676094  \tBatch training accuracy:  89.9625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.988281%,                 Avg loss: 0.233690 \n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22599636989831925  \tBatch training accuracy:  90.2  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.085938%,                 Avg loss: 0.235929 \n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2251241798400879  \tBatch training accuracy:  90.45  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.597656%,                 Avg loss: 0.239750 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2319035792350769  \tBatch training accuracy:  90.3  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 89.013672%,                 Avg loss: 0.207554 \n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.26832918244600296  \tBatch training accuracy:  89.0125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.476562%,                 Avg loss: 0.241509 \n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23909773856401442  \tBatch training accuracy:  89.9625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.525391%,                 Avg loss: 0.233682 \n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2424307637810707  \tBatch training accuracy:  89.85  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.085938%,                 Avg loss: 0.218419 \n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23080372405052185  \tBatch training accuracy:  90.375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.183594%,                 Avg loss: 0.212613 \n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2275121010541916  \tBatch training accuracy:  90.375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.281250%,                 Avg loss: 0.219426 \n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22039377546310424  \tBatch training accuracy:  90.5  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.623047%,                 Avg loss: 0.221050 \n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22415852838754655  \tBatch training accuracy:  90.525  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.134766%,                 Avg loss: 0.234558 \n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22576450204849244  \tBatch training accuracy:  90.6375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.988281%,                 Avg loss: 0.221084 \n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2247667543888092  \tBatch training accuracy:  90.7  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.232422%,                 Avg loss: 0.212440 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24411741691827774  \tBatch training accuracy:  90.025  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.134766%,                 Avg loss: 0.226868 \n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22416394621133803  \tBatch training accuracy:  90.5  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.232422%,                 Avg loss: 0.223103 \n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22360394376516343  \tBatch training accuracy:  90.325  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.476562%,                 Avg loss: 0.209575 \n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22190360462665557  \tBatch training accuracy:  90.575  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.378906%,                 Avg loss: 0.218612 \n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2192448332309723  \tBatch training accuracy:  90.925  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.183594%,                 Avg loss: 0.211823 \n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22802463042736054  \tBatch training accuracy:  90.375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.427734%,                 Avg loss: 0.208179 \n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22558718186616897  \tBatch training accuracy:  90.3  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.476562%,                 Avg loss: 0.220352 \n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22489281886816026  \tBatch training accuracy:  90.53750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.218429 \n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22066257619857788  \tBatch training accuracy:  90.6625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.525391%,                 Avg loss: 0.216627 \n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23665121269226075  \tBatch training accuracy:  90.03750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.792969%,                 Avg loss: 0.222378 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24022674405574798  \tBatch training accuracy:  90.35  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.234892 \n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23126415151357652  \tBatch training accuracy:  90.275  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.792969%,                 Avg loss: 0.223310 \n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22716952282190322  \tBatch training accuracy:  90.575  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.623047%,                 Avg loss: 0.216295 \n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22247081190347673  \tBatch training accuracy:  90.53750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.134766%,                 Avg loss: 0.218340 \n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22584364622831343  \tBatch training accuracy:  90.2375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.769531%,                 Avg loss: 0.212105 \n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22071024107933043  \tBatch training accuracy:  90.675  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.988281%,                 Avg loss: 0.224666 \n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.21946031630039214  \tBatch training accuracy:  90.5625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.792969%,                 Avg loss: 0.236973 \n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2629188472032547  \tBatch training accuracy:  89.21249999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 85.986328%,                 Avg loss: 0.288904 \n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24589237755537033  \tBatch training accuracy:  89.6375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.233557 \n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23710906487703323  \tBatch training accuracy:  89.9375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.232422%,                 Avg loss: 0.226329 \n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23212389087677002  \tBatch training accuracy:  90.4875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.232422%,                 Avg loss: 0.216843 \n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22456171137094497  \tBatch training accuracy:  90.45  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.207031%,                 Avg loss: 0.260189 \n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22425921988487243  \tBatch training accuracy:  90.7  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.232422%,                 Avg loss: 0.224660 \n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22049597048759462  \tBatch training accuracy:  90.5875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.281250%,                 Avg loss: 0.221878 \n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22504295390844345  \tBatch training accuracy:  90.47500000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.818359%,                 Avg loss: 0.214621 \n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22072239923477172  \tBatch training accuracy:  90.4  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.671875%,                 Avg loss: 0.221732 \n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22225522750616072  \tBatch training accuracy:  90.9375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.378906%,                 Avg loss: 0.221228 \n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2259823296070099  \tBatch training accuracy:  90.425  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.841797%,                 Avg loss: 0.238834 \n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22403558272123336  \tBatch training accuracy:  90.325  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.476562%,                 Avg loss: 0.224660 \n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22282957631349565  \tBatch training accuracy:  90.375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.328125%,                 Avg loss: 0.311642 \n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2624634536504745  \tBatch training accuracy:  89.4875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.939453%,                 Avg loss: 0.229777 \n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23526872342824937  \tBatch training accuracy:  90.35  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.695312%,                 Avg loss: 0.233464 \n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22986902040243148  \tBatch training accuracy:  90.10000000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.451172%,                 Avg loss: 0.231654 \n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22779635068774223  \tBatch training accuracy:  90.51249999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.939453%,                 Avg loss: 0.223923 \n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22051204073429106  \tBatch training accuracy:  90.4375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.134766%,                 Avg loss: 0.217164 \n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2459342992901802  \tBatch training accuracy:  89.875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.574219%,                 Avg loss: 0.222112 \n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22546182477474214  \tBatch training accuracy:  90.225  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.085938%,                 Avg loss: 0.224926 \n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2228307437300682  \tBatch training accuracy:  90.64999999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.281250%,                 Avg loss: 0.216188 \n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.21951159805059434  \tBatch training accuracy:  90.73750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.228764 \n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.21730901789665222  \tBatch training accuracy:  90.9  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.229577 \n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.21689888733625412  \tBatch training accuracy:  90.64999999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.720703%,                 Avg loss: 0.213389 \n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.21878043347597123  \tBatch training accuracy:  90.53750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.304688%,                 Avg loss: 0.233959 \n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.21718426465988158  \tBatch training accuracy:  90.6875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.183594%,                 Avg loss: 0.214555 \n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22233990424871444  \tBatch training accuracy:  90.575  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.792969%,                 Avg loss: 0.218576 \n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2206052075624466  \tBatch training accuracy:  90.75  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.939453%,                 Avg loss: 0.237889 \n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.28204381084442137  \tBatch training accuracy:  88.425  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.451172%,                 Avg loss: 0.243123 \n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23711038756370545  \tBatch training accuracy:  90.025  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.232422%,                 Avg loss: 0.233454 \n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23073342472314834  \tBatch training accuracy:  90.4125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.744141%,                 Avg loss: 0.223167 \n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22480070233345031  \tBatch training accuracy:  90.2  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.281250%,                 Avg loss: 0.225836 \n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22123559081554414  \tBatch training accuracy:  90.5625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.232422%,                 Avg loss: 0.222295 \n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22123878753185272  \tBatch training accuracy:  90.5625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.183594%,                 Avg loss: 0.222700 \n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.21680544596910475  \tBatch training accuracy:  90.975  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.232422%,                 Avg loss: 0.212962 \n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2225583745241165  \tBatch training accuracy:  90.25  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.841797%,                 Avg loss: 0.233506 \n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22515312570333482  \tBatch training accuracy:  90.625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.212872 \n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.21950214385986327  \tBatch training accuracy:  90.5875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.330078%,                 Avg loss: 0.213261 \n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22310354870557786  \tBatch training accuracy:  90.4875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.769531%,                 Avg loss: 0.211381 \n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22056173491477965  \tBatch training accuracy:  90.53750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.451172%,                 Avg loss: 0.269965 \n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22984530329704284  \tBatch training accuracy:  90.525  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.720703%,                 Avg loss: 0.218998 \n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.21610124003887177  \tBatch training accuracy:  90.77499999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.281250%,                 Avg loss: 0.217910 \n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22198500627279283  \tBatch training accuracy:  90.625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.623047%,                 Avg loss: 0.220842 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24680459868907928  \tBatch training accuracy:  89.64999999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.183594%,                 Avg loss: 0.222745 \n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23569626367092134  \tBatch training accuracy:  90.14999999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 85.888672%,                 Avg loss: 0.258693 \n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24542271888256073  \tBatch training accuracy:  89.825  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.720703%,                 Avg loss: 0.223532 \n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23288845002651215  \tBatch training accuracy:  90.2875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.548828%,                 Avg loss: 0.245117 \n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24795089566707612  \tBatch training accuracy:  89.58749999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.232422%,                 Avg loss: 0.226215 \n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23067532658576964  \tBatch training accuracy:  90.3875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.183594%,                 Avg loss: 0.221937 \n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2216937758922577  \tBatch training accuracy:  90.7625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.330078%,                 Avg loss: 0.218987 \n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22230076652765274  \tBatch training accuracy:  90.95  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.792969%,                 Avg loss: 0.222638 \n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2271466549038887  \tBatch training accuracy:  90.3625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.427734%,                 Avg loss: 0.223177 \n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2221927269101143  \tBatch training accuracy:  90.5  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.183594%,                 Avg loss: 0.233223 \n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2202158812880516  \tBatch training accuracy:  90.51249999999999  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.402344%,                 Avg loss: 0.234131 \n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22367814105749131  \tBatch training accuracy:  90.55  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.183594%,                 Avg loss: 0.219896 \n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2133799232840538  \tBatch training accuracy:  90.9375  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.134766%,                 Avg loss: 0.234611 \n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22220648843050003  \tBatch training accuracy:  90.4125  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.060547%,                 Avg loss: 0.260903 \n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.21436787521839143  \tBatch training accuracy:  90.875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.109375%,                 Avg loss: 0.278703 \n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2171182016134262  \tBatch training accuracy:  90.825  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.281250%,                 Avg loss: 0.219404 \n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22274887996912002  \tBatch training accuracy:  90.725  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.548828%,                 Avg loss: 0.240953 \n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.21604393398761748  \tBatch training accuracy:  90.8  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.988281%,                 Avg loss: 0.223048 \n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.24314307886362077  \tBatch training accuracy:  90.25  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 87.744141%,                 Avg loss: 0.272827 \n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.3526023030281067  \tBatch training accuracy:  87.225  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 86.328125%,                 Avg loss: 0.280455 \n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.26229339849948885  \tBatch training accuracy:  89.625  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.085938%,                 Avg loss: 0.224637 \n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.23092023837566375  \tBatch training accuracy:  90.2  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.574219%,                 Avg loss: 0.214545 \n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.22793256032466888  \tBatch training accuracy:  90.425  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.183594%,                 Avg loss: 0.222371 \n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.21950681966543198  \tBatch training accuracy:  90.9875  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.037109%,                 Avg loss: 0.219984 \n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.21700482189655304  \tBatch training accuracy:  90.925  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.427734%,                 Avg loss: 0.217932 \n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  0.2168280678987503  \tBatch training accuracy:  90.73750000000001  \t[ 125 / 125 ]                     \n",
      "Time taken for this epoch: 9s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 88.330078%,                 Avg loss: 0.215989 \n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "No TPUs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-23b3a7cdd67c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    252\u001b[0m            optimizers_param={\"lr\": config_model.learning_rate,\n\u001b[1;32m    253\u001b[0m                              \"weight_decay\": config_model.weight_decay},\n\u001b[0;32m--> 254\u001b[0;31m            n_accumulated_grads=config_model.n_accumulated_grads)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;31m# %%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/gdrive/MyDrive/giotto-deep/gdeep/pipeline/pipeline.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, optimizer, n_epochs, cross_validation, optimizers_param, dataloaders_param, lr_scheduler, scheduler_params, optuna_params, profiling, k_folds, parallel_tpu, keep_training, store_grad_layer_hist, n_accumulated_grads, writer_tag)\u001b[0m\n\u001b[1;32m    458\u001b[0m                                                    \u001b[0mdl_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m                                                    \u001b[0mprof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_optuna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_metric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m                                                    trial, writer_tag)\n\u001b[0m\u001b[1;32m    461\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                 valloss, valacc = self.parallel_tpu_training_loops(n_epochs, dl_tr,\n",
      "\u001b[0;32m/root/gdrive/MyDrive/giotto-deep/gdeep/pipeline/pipeline.py\u001b[0m in \u001b[0;36m_training_loops\u001b[0;34m(self, n_epochs, dl_tr, dl_val, lr_scheduler, scheduler, prof, check_optuna, search_metric, trial, writer_tag)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_grad_layer_hist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/gdrive/MyDrive/giotto-deep/gdeep/pipeline/pipeline.py\u001b[0m in \u001b[0;36m_train_loop\u001b[0;34m(self, dl_tr, writer_tag)\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                 \u001b[0mt_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m                 print(\"Batch training loss: \", t_loss/(batch+1), \" \\tBatch training accuracy: \",\n\u001b[1;32m    138\u001b[0m                       \u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdl_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# for gridsearch\n",
    "\n",
    "!pip install pyyaml==5.4.1\n",
    "\n",
    "# %%\n",
    "from IPython import get_ipython  # type: ignore\n",
    "\n",
    "# %% \n",
    "get_ipython().magic('load_ext autoreload')\n",
    "get_ipython().magic('autoreload 2')\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "from dotmap import DotMap\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Import the PyTorch modules\n",
    "import torch  # type: ignore\n",
    "from torch import nn  # type: ignore\n",
    "from torch.optim import SGD, Adam, RMSprop  # type: ignore\n",
    "\n",
    "# Import Tensorflow writer\n",
    "from torch.utils.tensorboard import SummaryWriter  # type: ignore\n",
    "\n",
    "# Import modules from XTransformers\n",
    "from x_transformers.x_transformers import AttentionLayers, Encoder, ContinuousTransformerWrapper\n",
    "\n",
    "\n",
    "# Import the giotto-deep modules\n",
    "from gdeep.data import OrbitsGenerator, DataLoaderKwargs\n",
    "from gdeep.topology_layers import SetTransformer, PersFormer\n",
    "#from gdeep.topology_layers import AttentionPooling\n",
    "from gdeep.topology_layers import ISAB, PMA, SAB\n",
    "from gdeep.pipeline import Pipeline\n",
    "from gdeep.search import Gridsearch\n",
    "import json\n",
    "#from gdeep.search import Gridsearch\n",
    "\n",
    "from optuna.pruners import MedianPruner, NopPruner\n",
    "\n",
    "# %%\n",
    "\n",
    "#Configs\n",
    "config_data = DotMap({\n",
    "    'batch_size_train': 64,\n",
    "    'num_orbits_per_class': 2_000,\n",
    "    'validation_percentage': 0.0,\n",
    "    'test_percentage': 0.0,\n",
    "    'num_jobs': 2,\n",
    "    'dynamical_system': 'classical_convention',\n",
    "    'homology_dimensions': (0, 1),\n",
    "    'dtype': 'float32',\n",
    "    'arbitrary_precision': False\n",
    "})\n",
    "\n",
    "\n",
    "config_model = DotMap({\n",
    "    'implementation': 'Old_SetTransformer', # SetTransformer, PersFormer,\n",
    "    # PytorchTransformer, DeepSet, X-Transformer\n",
    "    'dim_input': 4,\n",
    "    'num_outputs': 1,  # for classification tasks this should be 1\n",
    "    'num_classes': 5,  # number of classes\n",
    "    'dim_hidden': 128,\n",
    "    'num_heads': 8,\n",
    "    'num_induced_points': 32,\n",
    "    'layer_norm': False,  # use layer norm\n",
    "    'pre_layer_norm': False,\n",
    "    'num_layers_encoder': 4,\n",
    "    'num_layers_decoder': 3,\n",
    "    'attention_type': \"induced_attention\",\n",
    "    'activation': nn.GELU,\n",
    "    'dropout': 0.2,\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'learning_rate': 1e-3,\n",
    "    'num_epochs': 1000,\n",
    "    'pooling_type': \"max\",\n",
    "    'weight_decay': 0.0,\n",
    "    'n_accumulated_grads': 0,\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# Define the data loader\n",
    "\n",
    "\n",
    "dataloaders_dicts = DataLoaderKwargs(train_kwargs = {\"batch_size\":\n",
    "                                                        config_data.batch_size_train,},\n",
    "                                     val_kwargs = {\"batch_size\": 4},\n",
    "                                     test_kwargs = {\"batch_size\": 3})\n",
    "\n",
    "og = OrbitsGenerator(num_orbits_per_class=config_data.num_orbits_per_class,\n",
    "                     homology_dimensions = config_data.homology_dimensions,\n",
    "                     validation_percentage=config_data.validation_percentage,\n",
    "                     test_percentage=config_data.test_percentage,\n",
    "                     n_jobs=config_data.num_jobs,\n",
    "                     dynamical_system = config_data.dynamical_system,\n",
    "                     dtype=config_data.dtype,\n",
    "                     arbitrary_precision=config_data.arbitrary_precision,\n",
    "                     )\n",
    "\n",
    "if config_data.arbitrary_precision:\n",
    "    orbits = np.load(os.path.join('data', 'orbit5k_arbitrary_precision.npy'))\n",
    "    og.orbits_from_array(orbits)\n",
    "\n",
    "if config_data.dim_input == 2:\n",
    "    dl_train, _, _ = og.get_dataloader_orbits(dataloaders_dicts)\n",
    "else:\n",
    "    dl_train, _, _ = og.get_dataloader_persistence_diagrams(dataloaders_dicts)\n",
    "\n",
    "\n",
    "\n",
    "# Define the model\n",
    "if config_model.implementation == 'SetTransformer':\n",
    "    model = SetTransformer(\n",
    "            dim_input=config_model.dim_input,\n",
    "            num_outputs=1,  # for classification tasks this should be 1\n",
    "            dim_output=config_model.num_classes,  # number of classes\n",
    "            dim_hidden=config_model.dim_hidden,\n",
    "            num_heads=config_model.num_heads,\n",
    "            num_inds=config_model.num_induced_points,\n",
    "            ln=config_model.layer_norm,  # use layer norm\n",
    "            n_layers_encoder=config_model.num_layers_encoder,\n",
    "            n_layers_decoder=config_model.num_layers_decoder,\n",
    "            attention_type=config_model.attention_type,\n",
    "            dropout=config_model.dropout\n",
    "    )\n",
    "\n",
    "elif config_model.implementation == 'PersFormer':\n",
    "    model = PersFormer(\n",
    "            dim_input=2,\n",
    "            dim_output=5,\n",
    "            n_layers=5,\n",
    "            hidden_size=32,\n",
    "            n_heads=4,\n",
    "            dropout=0.1,\n",
    "            layer_norm=True,\n",
    "            pre_layer_norm=False,\n",
    "            activation=nn.GELU,\n",
    "            attention_layer_type=\"self_attention\")\n",
    "\n",
    "elif config_model.implementation == 'PytorchTransformer':\n",
    "    model = PytorchTransformer(\n",
    "            dim_input=2,\n",
    "            dim_output=5,\n",
    "            hidden_size=64,\n",
    "            nhead=8,\n",
    "            activation='gelu',\n",
    "            norm_first=True,\n",
    "            num_layers=3,\n",
    "            dropout=0.0,\n",
    "    )\n",
    "elif config_model.implementation == 'DeepSet':\n",
    "    model = DeepSet(dim_input=2,\n",
    "                    dim_output=config_model.num_classes,\n",
    "                    dim_hidden=config_model.dim_hidden,\n",
    "                    n_layers_encoder=config_model.num_layers_encoder,\n",
    "                    n_layers_decoder=config_model.num_layers_decoder,\n",
    "                    pool=config_model.pooling_type).double()\n",
    "\n",
    "elif config_model.implementation == \"X-Transformer\":\n",
    "    model = \\\n",
    "    nn.Sequential(\n",
    "        ContinuousTransformerWrapper(\n",
    "            dim_in = 2,\n",
    "            use_pos_emb = True,\n",
    "            max_seq_len = None,\n",
    "            attn_layers = Encoder(\n",
    "                dim = config_model.dim_hidden,\n",
    "                depth = config_model.num_layers_encoder,\n",
    "                heads = config_model.num_heads,\n",
    "            ),\n",
    "        ),\n",
    "        AttentionPooling(hidden_dim = config_model.dim_hidden, q_length=1),\n",
    "        nn.Sequential(*[nn.Sequential(nn.Linear(config_model.dim_hidden,\n",
    "                            config_model.dim_hidden),\n",
    "                            nn.ReLU())\n",
    "                for _ in range(config_model.num_layers_decoder)]),\n",
    "        nn.Linear(config_model.dim_hidden, config_model.num_classes)\n",
    "    )\n",
    "    \n",
    "elif config_model.implementation == \"Old_SetTransformer\":\n",
    "    # initialize SetTransformer model\n",
    "    class SetTransformer(nn.Module):\n",
    "        \"\"\" Vanilla SetTransformer from\n",
    "        https://github.com/juho-lee/set_transformer/blob/master/main_pointcloud.py\n",
    "        \"\"\"\n",
    "        def __init__(\n",
    "            self,\n",
    "            dim_input=3,  # dimension of input data for each element in the set\n",
    "            num_outputs=1,\n",
    "            dim_output=40,  # number of classes\n",
    "            num_inds=32,  # number of induced points, see  Set Transformer paper\n",
    "            dim_hidden=128,\n",
    "            num_heads=4,\n",
    "            ln=False,  # use layer norm\n",
    "        ):\n",
    "            super(SetTransformer, self).__init__()\n",
    "            self.enc = nn.Sequential(\n",
    "                ISAB(dim_input, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "                ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "            )\n",
    "            self.dec = nn.Sequential(\n",
    "                nn.Dropout(),\n",
    "                PMA(dim_hidden, num_heads, num_outputs, ln=ln),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(dim_hidden, dim_output),\n",
    "            )\n",
    "\n",
    "        def forward(self, input):\n",
    "            return self.dec(self.enc(input)).squeeze()\n",
    "\n",
    "\n",
    "    model = SetTransformer(dim_input=4, dim_output=5)\n",
    "else:\n",
    "    raise Exception(\"Unknown Implementation\")\n",
    "# %%\n",
    "\n",
    "if config_data.dtype == \"float64\":\n",
    "    print(\"Use float64 model\")\n",
    "    model = model.double()\n",
    "else:\n",
    "    print(\"use float32 model\")\n",
    "\n",
    "# %%\n",
    "# Do training and validation\n",
    "\n",
    "# initialise loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize the Tensorflow writer\n",
    "#writer = SummaryWriter(comment=json.dumps(config_model.toDict())\\\n",
    "#                                + json.dumps(config_data.toDict()))\n",
    "writer = SummaryWriter(comment=config_model.implementation)\n",
    "\n",
    "# initialise pipeline class\n",
    "pipe = Pipeline(model, [dl_train, None], loss_fn, writer)\n",
    "# %%\n",
    "\n",
    "\n",
    "# train the model\n",
    "pipe.train(config_model.optimizer,\n",
    "           config_model.num_epochs,\n",
    "           cross_validation=False,\n",
    "           optimizers_param={\"lr\": config_model.learning_rate,\n",
    "                             \"weight_decay\": config_model.weight_decay},\n",
    "           n_accumulated_grads=config_model.n_accumulated_grads)\n",
    "\n",
    "# %%\n",
    "# keep training\n",
    "#pipe.train(Adam, 300, False, keep_training=True)\n",
    "\n",
    "# %%\n",
    "# %%\n",
    "# Gridsearch\n",
    "\n",
    "# initialise gridsearch\n",
    "pruner = NopPruner()\n",
    "search = Gridsearch(pipe, search_metric=\"accuracy\", n_trials=50, best_not_last=True, pruner=pruner)\n",
    "\n",
    "# dictionaries of hyperparameters\n",
    "optimizers_params = {\"lr\": [1e-6, 1e-3],\n",
    "                      \"weight_decay\": [0.0, 0.2] }\n",
    "dataloaders_params = {\"batch_size\": [32, 64, 16]}\n",
    "models_hyperparams = {\"n_layer_enc\": [2, 5],\n",
    "                      \"n_layer_dec\": [1, 5],\n",
    "                      \"num_heads\": [\"2\", \"4\", \"8\"],\n",
    "                      \"hidden_dim\": [\"16\", \"32\", \"64\"],\n",
    "                      \"dropout\": [0.0, 0.2],\n",
    "                      \"layer_norm\": [\"True\", \"False\"],\n",
    "                      'pre_layer_norm': [\"True\", \"False\"]}\n",
    "\n",
    "# starting the gridsearch\n",
    "#search.start((Adam,), n_epochs=500, cross_validation=False,\n",
    "#             optimizers_params=optimizers_params,\n",
    "#             dataloaders_params=dataloaders_params,\n",
    "#             models_hyperparams=models_hyperparams, lr_scheduler=None,\n",
    "#             scheduler_params=None)\n",
    "\n",
    "\n",
    "# %%\n",
    "#print(search.best_val_acc_gs, search.best_val_loss_gs)\n",
    "# %%\n",
    "#df_res = search._results()\n",
    "#df_res\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6c85e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.rand((2, 1000, 4)).double()\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f9e6bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "Number of pruned trials:  0\n",
      "Number of complete trials:  50\n",
      "Best trial:\n",
      "Metric Value for best trial:  99.10714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda-envs/giottodeep/lib/python3.8/site-packages/numpy/lib/function_base.py:2474: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in subtract\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "Parameters: %{x}<br>Parameters: %{y}<br>Correlation: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "x": [
          "lr",
          "batch_size",
          "dropout",
          "n_layer_enc",
          "n_layer_dec",
          "loss",
          "accuracy"
         ],
         "xaxis": "x",
         "y": [
          "lr",
          "batch_size",
          "dropout",
          "n_layer_enc",
          "n_layer_dec",
          "loss",
          "accuracy"
         ],
         "yaxis": "y",
         "z": [
          [
           1,
           0.3321793461835531,
           0.0904440217666608,
           0.08792457464781951,
           -0.1781623815223595,
           null,
           0.24389148551474943
          ],
          [
           0.33217934618355316,
           0.9999999999999998,
           0.11206889636935947,
           -0.02436777204559112,
           0.02155217086890601,
           null,
           -0.019212603122426414
          ],
          [
           0.0904440217666608,
           0.11206889636935947,
           0.9999999999999999,
           0.26285826902480613,
           0.1766546131963994,
           null,
           -0.05580527297616502
          ],
          [
           0.0879245746478195,
           -0.02436777204559112,
           0.26285826902480613,
           1,
           0.02517799662955688,
           null,
           0.09026802690199691
          ],
          [
           -0.17816238152235953,
           0.02155217086890601,
           0.1766546131963994,
           0.025177996629556883,
           0.9999999999999998,
           null,
           -0.07963620883241891
          ],
          [
           null,
           null,
           null,
           null,
           null,
           null,
           null
          ],
          [
           0.24389148551474943,
           -0.019212603122426414,
           -0.05580527297616503,
           0.09026802690199691,
           -0.07963620883241893,
           null,
           1
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "Correlation"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "scaleanchor": "y",
         "side": "top",
         "title": {
          "text": "Parameters"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Parameters"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"711b72fc-9d3b-44bf-b6a1-b3b8e17b6c56\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"711b72fc-9d3b-44bf-b6a1-b3b8e17b6c56\")) {                    Plotly.newPlot(                        \"711b72fc-9d3b-44bf-b6a1-b3b8e17b6c56\",                        [{\"coloraxis\": \"coloraxis\", \"hovertemplate\": \"Parameters: %{x}<br>Parameters: %{y}<br>Correlation: %{z}<extra></extra>\", \"name\": \"0\", \"type\": \"heatmap\", \"x\": [\"lr\", \"batch_size\", \"dropout\", \"n_layer_enc\", \"n_layer_dec\", \"loss\", \"accuracy\"], \"xaxis\": \"x\", \"y\": [\"lr\", \"batch_size\", \"dropout\", \"n_layer_enc\", \"n_layer_dec\", \"loss\", \"accuracy\"], \"yaxis\": \"y\", \"z\": [[1.0, 0.3321793461835531, 0.0904440217666608, 0.08792457464781951, -0.1781623815223595, null, 0.24389148551474943], [0.33217934618355316, 0.9999999999999998, 0.11206889636935947, -0.02436777204559112, 0.02155217086890601, null, -0.019212603122426414], [0.0904440217666608, 0.11206889636935947, 0.9999999999999999, 0.26285826902480613, 0.1766546131963994, null, -0.05580527297616502], [0.0879245746478195, -0.02436777204559112, 0.26285826902480613, 1.0, 0.02517799662955688, null, 0.09026802690199691], [-0.17816238152235953, 0.02155217086890601, 0.1766546131963994, 0.025177996629556883, 0.9999999999999998, null, -0.07963620883241891], [null, null, null, null, null, null, null], [0.24389148551474943, -0.019212603122426414, -0.05580527297616503, 0.09026802690199691, -0.07963620883241893, null, 1.0]]}],                        {\"coloraxis\": {\"colorbar\": {\"title\": {\"text\": \"Correlation\"}}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"constrain\": \"domain\", \"domain\": [0.0, 1.0], \"scaleanchor\": \"y\", \"side\": \"top\", \"title\": {\"text\": \"Parameters\"}}, \"yaxis\": {\"anchor\": \"x\", \"autorange\": \"reversed\", \"constrain\": \"domain\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Parameters\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('711b72fc-9d3b-44bf-b6a1-b3b8e17b6c56');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>n_layer_enc</th>\n",
       "      <th>n_layer_dec</th>\n",
       "      <th>num_heads</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>layer_norm</th>\n",
       "      <th>pre_layer_norm</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>32</td>\n",
       "      <td>0.158746</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>94.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>48</td>\n",
       "      <td>0.130351</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.710317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>32</td>\n",
       "      <td>0.044103</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>97.070312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>32</td>\n",
       "      <td>0.174644</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>63.769531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>64</td>\n",
       "      <td>0.016269</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.191406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>32</td>\n",
       "      <td>0.036563</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>89.160156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>32</td>\n",
       "      <td>0.126969</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>94.628906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>64</td>\n",
       "      <td>0.154917</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.582031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>32</td>\n",
       "      <td>0.125399</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>32</td>\n",
       "      <td>0.004410</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.777344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>48</td>\n",
       "      <td>0.078869</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.511905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>48</td>\n",
       "      <td>0.081688</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>97.519841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>48</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.412698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>48</td>\n",
       "      <td>0.068706</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.015873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>48</td>\n",
       "      <td>0.118253</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>64</td>\n",
       "      <td>0.131878</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>95.507812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>48</td>\n",
       "      <td>0.198995</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>97.519841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>48</td>\n",
       "      <td>0.111423</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>99.107143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>64</td>\n",
       "      <td>0.107427</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>95.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>48</td>\n",
       "      <td>0.145682</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>94.246032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>48</td>\n",
       "      <td>0.188350</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>48</td>\n",
       "      <td>0.194394</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.726190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>48</td>\n",
       "      <td>0.109942</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>48</td>\n",
       "      <td>0.103142</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>97.718254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>48</td>\n",
       "      <td>0.177623</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.412698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>64</td>\n",
       "      <td>0.144937</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>97.363281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>48</td>\n",
       "      <td>0.064298</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>97.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>48</td>\n",
       "      <td>0.106832</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>97.519841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>64</td>\n",
       "      <td>0.138439</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.484375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>48</td>\n",
       "      <td>0.119270</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>94.742063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>48</td>\n",
       "      <td>0.166125</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.031746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>48</td>\n",
       "      <td>0.092712</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.412698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>48</td>\n",
       "      <td>0.186484</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.115079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>48</td>\n",
       "      <td>0.160658</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>65.773810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>48</td>\n",
       "      <td>0.054597</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.329365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>48</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.130952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>32</td>\n",
       "      <td>0.089597</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>95.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>32</td>\n",
       "      <td>0.115180</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>94.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>64</td>\n",
       "      <td>0.135858</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>65.527344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>32</td>\n",
       "      <td>0.152652</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>97.363281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>48</td>\n",
       "      <td>0.109732</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>48.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>48</td>\n",
       "      <td>0.127127</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>48</td>\n",
       "      <td>0.096609</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>48</td>\n",
       "      <td>0.096068</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>48</td>\n",
       "      <td>0.080913</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>46.626984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>48</td>\n",
       "      <td>0.114384</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>20.039683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>48</td>\n",
       "      <td>0.033099</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.412698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>48</td>\n",
       "      <td>0.076171</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>92.658730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>32</td>\n",
       "      <td>0.103653</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.484375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>48</td>\n",
       "      <td>0.087275</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>97.222222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model  dataset optimizer        lr  batch_size   dropout  n_layer_enc  \\\n",
       "0   model  dataset      Adam  0.000425          32  0.158746            4   \n",
       "1   model  dataset      Adam  0.000946          48  0.130351            3   \n",
       "2   model  dataset      Adam  0.000823          32  0.044103            5   \n",
       "3   model  dataset      Adam  0.000339          32  0.174644            3   \n",
       "4   model  dataset      Adam  0.000835          64  0.016269            2   \n",
       "5   model  dataset      Adam  0.000078          32  0.036563            2   \n",
       "6   model  dataset      Adam  0.000218          32  0.126969            5   \n",
       "7   model  dataset      Adam  0.000901          64  0.154917            2   \n",
       "8   model  dataset      Adam  0.000550          32  0.125399            2   \n",
       "9   model  dataset      Adam  0.000493          32  0.004410            3   \n",
       "10  model  dataset      Adam  0.000670          48  0.078869            4   \n",
       "11  model  dataset      Adam  0.000668          48  0.081688            4   \n",
       "12  model  dataset      Adam  0.000975          48  0.091360            4   \n",
       "13  model  dataset      Adam  0.000731          48  0.068706            3   \n",
       "14  model  dataset      Adam  0.000997          48  0.118253            4   \n",
       "15  model  dataset      Adam  0.000997          64  0.131878            3   \n",
       "16  model  dataset      Adam  0.000942          48  0.198995            4   \n",
       "17  model  dataset      Adam  0.000791          48  0.111423            5   \n",
       "18  model  dataset      Adam  0.000778          64  0.107427            5   \n",
       "19  model  dataset      Adam  0.000666          48  0.145682            3   \n",
       "20  model  dataset      Adam  0.000890          48  0.188350            5   \n",
       "21  model  dataset      Adam  0.000871          48  0.194394            5   \n",
       "22  model  dataset      Adam  0.000981          48  0.109942            4   \n",
       "23  model  dataset      Adam  0.000999          48  0.103142            3   \n",
       "24  model  dataset      Adam  0.000749          48  0.177623            5   \n",
       "25  model  dataset      Adam  0.000908          64  0.144937            4   \n",
       "26  model  dataset      Adam  0.000588          48  0.064298            3   \n",
       "27  model  dataset      Adam  0.000771          48  0.106832            4   \n",
       "28  model  dataset      Adam  0.000948          64  0.138439            3   \n",
       "29  model  dataset      Adam  0.000391          48  0.119270            5   \n",
       "30  model  dataset      Adam  0.000798          48  0.166125            4   \n",
       "31  model  dataset      Adam  0.000868          48  0.092712            5   \n",
       "32  model  dataset      Adam  0.000910          48  0.186484            5   \n",
       "33  model  dataset      Adam  0.000839          48  0.160658            5   \n",
       "34  model  dataset      Adam  0.000993          48  0.054597            5   \n",
       "35  model  dataset      Adam  0.000837          48  0.121300            4   \n",
       "36  model  dataset      Adam  0.000720          32  0.089597            5   \n",
       "37  model  dataset      Adam  0.000296          32  0.115180            4   \n",
       "38  model  dataset      Adam  0.000059          64  0.135858            4   \n",
       "39  model  dataset      Adam  0.000936          32  0.152652            2   \n",
       "40  model  dataset      Adam  0.000137          48  0.109732            3   \n",
       "41  model  dataset      Adam  0.000869          48  0.127127            4   \n",
       "42  model  dataset      Adam  0.000961          48  0.096609            4   \n",
       "43  model  dataset      Adam  0.000942          48  0.096068            5   \n",
       "44  model  dataset      Adam  0.000814          48  0.080913            3   \n",
       "45  model  dataset      Adam  0.000995          48  0.114384            4   \n",
       "46  model  dataset      Adam  0.000882          48  0.033099            2   \n",
       "47  model  dataset      Adam  0.000944          48  0.076171            3   \n",
       "48  model  dataset      Adam  0.000993          32  0.103653            4   \n",
       "49  model  dataset      Adam  0.000593          48  0.087275            5   \n",
       "\n",
       "    n_layer_dec num_heads hidden_dim layer_norm pre_layer_norm  loss  \\\n",
       "0             4         4         64       True           True   inf   \n",
       "1             4         8         32       True          False   inf   \n",
       "2             2         8         16       True          False   inf   \n",
       "3             3         2         32      False           True   inf   \n",
       "4             1         2         16       True           True   inf   \n",
       "5             3         4         64       True          False   inf   \n",
       "6             5         8         16       True          False   inf   \n",
       "7             4         8         16      False          False   inf   \n",
       "8             1         2         64       True          False   inf   \n",
       "9             1         2         64      False          False   inf   \n",
       "10            5         8         32       True          False   inf   \n",
       "11            5         8         32       True          False   inf   \n",
       "12            4         8         32       True          False   inf   \n",
       "13            5         8         32       True          False   inf   \n",
       "14            4         8         32       True          False   inf   \n",
       "15            4         8         32       True          False   inf   \n",
       "16            3         4         32       True          False   inf   \n",
       "17            2         8         32      False           True   inf   \n",
       "18            2         8         32      False           True   inf   \n",
       "19            2         8         32      False           True   inf   \n",
       "20            2         4         32      False           True   inf   \n",
       "21            2         4         32      False           True   inf   \n",
       "22            4         8         32      False           True   inf   \n",
       "23            3         8         32      False           True   inf   \n",
       "24            2         4         32      False           True   inf   \n",
       "25            4         8         32      False           True   inf   \n",
       "26            3         8         32      False           True   inf   \n",
       "27            4         8         32      False           True   inf   \n",
       "28            3         8         32      False           True   inf   \n",
       "29            4         8         64       True          False   inf   \n",
       "30            5         8         32       True          False   inf   \n",
       "31            2         4         32      False           True   inf   \n",
       "32            2         4         32       True           True   inf   \n",
       "33            1         4         32      False           True   inf   \n",
       "34            3         4         16      False           True   inf   \n",
       "35            2         2         32       True          False   inf   \n",
       "36            3         4         32      False           True   inf   \n",
       "37            4         8         64       True          False   inf   \n",
       "38            4         8         16       True          False   inf   \n",
       "39            3         2         32      False           True   inf   \n",
       "40            4         8         16       True          False   inf   \n",
       "41            4         8         32      False           True   inf   \n",
       "42            1         8         32      False           True   inf   \n",
       "43            1         8         32      False           True   inf   \n",
       "44            1         8         32      False           True   inf   \n",
       "45            5         8         32       True          False   inf   \n",
       "46            1         2         64       True          False   inf   \n",
       "47            1         8         32      False           True   inf   \n",
       "48            4         8         32       True          False   inf   \n",
       "49            2         4         32      False           True   inf   \n",
       "\n",
       "     accuracy  \n",
       "0   94.921875  \n",
       "1   98.710317  \n",
       "2   97.070312  \n",
       "3   63.769531  \n",
       "4   96.191406  \n",
       "5   89.160156  \n",
       "6   94.628906  \n",
       "7   96.582031  \n",
       "8   96.875000  \n",
       "9   96.777344  \n",
       "10  98.511905  \n",
       "11  97.519841  \n",
       "12  98.412698  \n",
       "13  98.015873  \n",
       "14  98.611111  \n",
       "15  95.507812  \n",
       "16  97.519841  \n",
       "17  99.107143  \n",
       "18  95.312500  \n",
       "19  94.246032  \n",
       "20  98.611111  \n",
       "21  96.726190  \n",
       "22  98.611111  \n",
       "23  97.718254  \n",
       "24  98.412698  \n",
       "25  97.363281  \n",
       "26  97.916667  \n",
       "27  97.519841  \n",
       "28  96.484375  \n",
       "29  94.742063  \n",
       "30  96.031746  \n",
       "31  98.412698  \n",
       "32  98.115079  \n",
       "33  65.773810  \n",
       "34  96.329365  \n",
       "35  96.130952  \n",
       "36  95.703125  \n",
       "37  94.921875  \n",
       "38  65.527344  \n",
       "39  97.363281  \n",
       "40  48.809524  \n",
       "41  98.214286  \n",
       "42  98.611111  \n",
       "43  98.214286  \n",
       "44  46.626984  \n",
       "45  20.039683  \n",
       "46  98.412698  \n",
       "47  92.658730  \n",
       "48  96.484375  \n",
       "49  97.222222  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res = search._results()\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38906bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sympy\n",
      "  Downloading sympy-1.9-py3-none-any.whl (6.2 MB)\n",
      "\u001b[K     || 6.2 MB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mpmath>=0.19\n",
      "  Downloading mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
      "\u001b[K     || 532 kB 98.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy\n",
      "Successfully installed mpmath-1.2.1 sympy-1.9\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/jovyan/conda-envs/giottodeep/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64d5b8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch_optimizer as optim\n",
    "\n",
    "# model = ...\n",
    "optimizer = optim.Shampoo(\n",
    "    m.parameters(),\n",
    "    lr=1e-1,\n",
    "    momentum=0.0,\n",
    "    weight_decay=0.0,\n",
    "    epsilon=1e-4,\n",
    "    update_freq=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "753d5ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.to_csv('set_transformer_grid_search.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f68c8355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "Number of pruned trials:  0\n",
      "Number of complete trials:  50\n",
      "Best trial:\n",
      "Metric Value for best trial:  99.10714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda-envs/giottodeep/lib/python3.8/site-packages/numpy/lib/function_base.py:2474: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in subtract\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "Parameters: %{x}<br>Parameters: %{y}<br>Correlation: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "x": [
          "lr",
          "batch_size",
          "dropout",
          "n_layer_enc",
          "n_layer_dec",
          "loss",
          "accuracy"
         ],
         "xaxis": "x",
         "y": [
          "lr",
          "batch_size",
          "dropout",
          "n_layer_enc",
          "n_layer_dec",
          "loss",
          "accuracy"
         ],
         "yaxis": "y",
         "z": [
          [
           1,
           0.3321793461835531,
           0.0904440217666608,
           0.08792457464781951,
           -0.1781623815223595,
           null,
           0.24389148551474943
          ],
          [
           0.33217934618355316,
           0.9999999999999998,
           0.11206889636935947,
           -0.02436777204559112,
           0.02155217086890601,
           null,
           -0.019212603122426414
          ],
          [
           0.0904440217666608,
           0.11206889636935947,
           0.9999999999999999,
           0.26285826902480613,
           0.1766546131963994,
           null,
           -0.05580527297616502
          ],
          [
           0.0879245746478195,
           -0.02436777204559112,
           0.26285826902480613,
           1,
           0.02517799662955688,
           null,
           0.09026802690199691
          ],
          [
           -0.17816238152235953,
           0.02155217086890601,
           0.1766546131963994,
           0.025177996629556883,
           0.9999999999999998,
           null,
           -0.07963620883241891
          ],
          [
           null,
           null,
           null,
           null,
           null,
           null,
           null
          ],
          [
           0.24389148551474943,
           -0.019212603122426414,
           -0.05580527297616503,
           0.09026802690199691,
           -0.07963620883241893,
           null,
           1
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "Correlation"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "scaleanchor": "y",
         "side": "top",
         "title": {
          "text": "Parameters"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Parameters"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"f13c47d3-e1a8-481a-8d34-3dcce4c1e3b8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f13c47d3-e1a8-481a-8d34-3dcce4c1e3b8\")) {                    Plotly.newPlot(                        \"f13c47d3-e1a8-481a-8d34-3dcce4c1e3b8\",                        [{\"coloraxis\": \"coloraxis\", \"hovertemplate\": \"Parameters: %{x}<br>Parameters: %{y}<br>Correlation: %{z}<extra></extra>\", \"name\": \"0\", \"type\": \"heatmap\", \"x\": [\"lr\", \"batch_size\", \"dropout\", \"n_layer_enc\", \"n_layer_dec\", \"loss\", \"accuracy\"], \"xaxis\": \"x\", \"y\": [\"lr\", \"batch_size\", \"dropout\", \"n_layer_enc\", \"n_layer_dec\", \"loss\", \"accuracy\"], \"yaxis\": \"y\", \"z\": [[1.0, 0.3321793461835531, 0.0904440217666608, 0.08792457464781951, -0.1781623815223595, null, 0.24389148551474943], [0.33217934618355316, 0.9999999999999998, 0.11206889636935947, -0.02436777204559112, 0.02155217086890601, null, -0.019212603122426414], [0.0904440217666608, 0.11206889636935947, 0.9999999999999999, 0.26285826902480613, 0.1766546131963994, null, -0.05580527297616502], [0.0879245746478195, -0.02436777204559112, 0.26285826902480613, 1.0, 0.02517799662955688, null, 0.09026802690199691], [-0.17816238152235953, 0.02155217086890601, 0.1766546131963994, 0.025177996629556883, 0.9999999999999998, null, -0.07963620883241891], [null, null, null, null, null, null, null], [0.24389148551474943, -0.019212603122426414, -0.05580527297616503, 0.09026802690199691, -0.07963620883241893, null, 1.0]]}],                        {\"coloraxis\": {\"colorbar\": {\"title\": {\"text\": \"Correlation\"}}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"constrain\": \"domain\", \"domain\": [0.0, 1.0], \"scaleanchor\": \"y\", \"side\": \"top\", \"title\": {\"text\": \"Parameters\"}}, \"yaxis\": {\"anchor\": \"x\", \"autorange\": \"reversed\", \"constrain\": \"domain\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Parameters\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('f13c47d3-e1a8-481a-8d34-3dcce4c1e3b8');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>n_layer_enc</th>\n",
       "      <th>n_layer_dec</th>\n",
       "      <th>num_heads</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>layer_norm</th>\n",
       "      <th>pre_layer_norm</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>32</td>\n",
       "      <td>0.158746</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>94.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>48</td>\n",
       "      <td>0.130351</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.710317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>32</td>\n",
       "      <td>0.044103</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>97.070312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>32</td>\n",
       "      <td>0.174644</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>63.769531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>64</td>\n",
       "      <td>0.016269</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.191406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>32</td>\n",
       "      <td>0.036563</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>89.160156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>32</td>\n",
       "      <td>0.126969</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>94.628906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>64</td>\n",
       "      <td>0.154917</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.582031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>32</td>\n",
       "      <td>0.125399</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>32</td>\n",
       "      <td>0.004410</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.777344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>48</td>\n",
       "      <td>0.078869</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.511905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>48</td>\n",
       "      <td>0.081688</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>97.519841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>48</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.412698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>48</td>\n",
       "      <td>0.068706</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.015873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>48</td>\n",
       "      <td>0.118253</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>64</td>\n",
       "      <td>0.131878</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>95.507812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>48</td>\n",
       "      <td>0.198995</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>97.519841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>48</td>\n",
       "      <td>0.111423</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>99.107143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>64</td>\n",
       "      <td>0.107427</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>95.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>48</td>\n",
       "      <td>0.145682</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>94.246032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>48</td>\n",
       "      <td>0.188350</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>48</td>\n",
       "      <td>0.194394</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.726190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>48</td>\n",
       "      <td>0.109942</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>48</td>\n",
       "      <td>0.103142</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>97.718254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>48</td>\n",
       "      <td>0.177623</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.412698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>64</td>\n",
       "      <td>0.144937</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>97.363281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>48</td>\n",
       "      <td>0.064298</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>97.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>48</td>\n",
       "      <td>0.106832</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>97.519841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>64</td>\n",
       "      <td>0.138439</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.484375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>48</td>\n",
       "      <td>0.119270</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>94.742063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>48</td>\n",
       "      <td>0.166125</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.031746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>48</td>\n",
       "      <td>0.092712</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.412698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>48</td>\n",
       "      <td>0.186484</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.115079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>48</td>\n",
       "      <td>0.160658</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>65.773810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>48</td>\n",
       "      <td>0.054597</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.329365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>48</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.130952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>32</td>\n",
       "      <td>0.089597</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>95.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>32</td>\n",
       "      <td>0.115180</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>94.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>64</td>\n",
       "      <td>0.135858</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>65.527344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>32</td>\n",
       "      <td>0.152652</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>97.363281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>48</td>\n",
       "      <td>0.109732</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>48.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>48</td>\n",
       "      <td>0.127127</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>48</td>\n",
       "      <td>0.096609</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>48</td>\n",
       "      <td>0.096068</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>48</td>\n",
       "      <td>0.080913</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>46.626984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>48</td>\n",
       "      <td>0.114384</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>20.039683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>48</td>\n",
       "      <td>0.033099</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>98.412698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>48</td>\n",
       "      <td>0.076171</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>92.658730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>32</td>\n",
       "      <td>0.103653</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>inf</td>\n",
       "      <td>96.484375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>48</td>\n",
       "      <td>0.087275</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>inf</td>\n",
       "      <td>97.222222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model  dataset optimizer        lr  batch_size   dropout  n_layer_enc  \\\n",
       "0   model  dataset      Adam  0.000425          32  0.158746            4   \n",
       "1   model  dataset      Adam  0.000946          48  0.130351            3   \n",
       "2   model  dataset      Adam  0.000823          32  0.044103            5   \n",
       "3   model  dataset      Adam  0.000339          32  0.174644            3   \n",
       "4   model  dataset      Adam  0.000835          64  0.016269            2   \n",
       "5   model  dataset      Adam  0.000078          32  0.036563            2   \n",
       "6   model  dataset      Adam  0.000218          32  0.126969            5   \n",
       "7   model  dataset      Adam  0.000901          64  0.154917            2   \n",
       "8   model  dataset      Adam  0.000550          32  0.125399            2   \n",
       "9   model  dataset      Adam  0.000493          32  0.004410            3   \n",
       "10  model  dataset      Adam  0.000670          48  0.078869            4   \n",
       "11  model  dataset      Adam  0.000668          48  0.081688            4   \n",
       "12  model  dataset      Adam  0.000975          48  0.091360            4   \n",
       "13  model  dataset      Adam  0.000731          48  0.068706            3   \n",
       "14  model  dataset      Adam  0.000997          48  0.118253            4   \n",
       "15  model  dataset      Adam  0.000997          64  0.131878            3   \n",
       "16  model  dataset      Adam  0.000942          48  0.198995            4   \n",
       "17  model  dataset      Adam  0.000791          48  0.111423            5   \n",
       "18  model  dataset      Adam  0.000778          64  0.107427            5   \n",
       "19  model  dataset      Adam  0.000666          48  0.145682            3   \n",
       "20  model  dataset      Adam  0.000890          48  0.188350            5   \n",
       "21  model  dataset      Adam  0.000871          48  0.194394            5   \n",
       "22  model  dataset      Adam  0.000981          48  0.109942            4   \n",
       "23  model  dataset      Adam  0.000999          48  0.103142            3   \n",
       "24  model  dataset      Adam  0.000749          48  0.177623            5   \n",
       "25  model  dataset      Adam  0.000908          64  0.144937            4   \n",
       "26  model  dataset      Adam  0.000588          48  0.064298            3   \n",
       "27  model  dataset      Adam  0.000771          48  0.106832            4   \n",
       "28  model  dataset      Adam  0.000948          64  0.138439            3   \n",
       "29  model  dataset      Adam  0.000391          48  0.119270            5   \n",
       "30  model  dataset      Adam  0.000798          48  0.166125            4   \n",
       "31  model  dataset      Adam  0.000868          48  0.092712            5   \n",
       "32  model  dataset      Adam  0.000910          48  0.186484            5   \n",
       "33  model  dataset      Adam  0.000839          48  0.160658            5   \n",
       "34  model  dataset      Adam  0.000993          48  0.054597            5   \n",
       "35  model  dataset      Adam  0.000837          48  0.121300            4   \n",
       "36  model  dataset      Adam  0.000720          32  0.089597            5   \n",
       "37  model  dataset      Adam  0.000296          32  0.115180            4   \n",
       "38  model  dataset      Adam  0.000059          64  0.135858            4   \n",
       "39  model  dataset      Adam  0.000936          32  0.152652            2   \n",
       "40  model  dataset      Adam  0.000137          48  0.109732            3   \n",
       "41  model  dataset      Adam  0.000869          48  0.127127            4   \n",
       "42  model  dataset      Adam  0.000961          48  0.096609            4   \n",
       "43  model  dataset      Adam  0.000942          48  0.096068            5   \n",
       "44  model  dataset      Adam  0.000814          48  0.080913            3   \n",
       "45  model  dataset      Adam  0.000995          48  0.114384            4   \n",
       "46  model  dataset      Adam  0.000882          48  0.033099            2   \n",
       "47  model  dataset      Adam  0.000944          48  0.076171            3   \n",
       "48  model  dataset      Adam  0.000993          32  0.103653            4   \n",
       "49  model  dataset      Adam  0.000593          48  0.087275            5   \n",
       "\n",
       "    n_layer_dec num_heads hidden_dim layer_norm pre_layer_norm  loss  \\\n",
       "0             4         4         64       True           True   inf   \n",
       "1             4         8         32       True          False   inf   \n",
       "2             2         8         16       True          False   inf   \n",
       "3             3         2         32      False           True   inf   \n",
       "4             1         2         16       True           True   inf   \n",
       "5             3         4         64       True          False   inf   \n",
       "6             5         8         16       True          False   inf   \n",
       "7             4         8         16      False          False   inf   \n",
       "8             1         2         64       True          False   inf   \n",
       "9             1         2         64      False          False   inf   \n",
       "10            5         8         32       True          False   inf   \n",
       "11            5         8         32       True          False   inf   \n",
       "12            4         8         32       True          False   inf   \n",
       "13            5         8         32       True          False   inf   \n",
       "14            4         8         32       True          False   inf   \n",
       "15            4         8         32       True          False   inf   \n",
       "16            3         4         32       True          False   inf   \n",
       "17            2         8         32      False           True   inf   \n",
       "18            2         8         32      False           True   inf   \n",
       "19            2         8         32      False           True   inf   \n",
       "20            2         4         32      False           True   inf   \n",
       "21            2         4         32      False           True   inf   \n",
       "22            4         8         32      False           True   inf   \n",
       "23            3         8         32      False           True   inf   \n",
       "24            2         4         32      False           True   inf   \n",
       "25            4         8         32      False           True   inf   \n",
       "26            3         8         32      False           True   inf   \n",
       "27            4         8         32      False           True   inf   \n",
       "28            3         8         32      False           True   inf   \n",
       "29            4         8         64       True          False   inf   \n",
       "30            5         8         32       True          False   inf   \n",
       "31            2         4         32      False           True   inf   \n",
       "32            2         4         32       True           True   inf   \n",
       "33            1         4         32      False           True   inf   \n",
       "34            3         4         16      False           True   inf   \n",
       "35            2         2         32       True          False   inf   \n",
       "36            3         4         32      False           True   inf   \n",
       "37            4         8         64       True          False   inf   \n",
       "38            4         8         16       True          False   inf   \n",
       "39            3         2         32      False           True   inf   \n",
       "40            4         8         16       True          False   inf   \n",
       "41            4         8         32      False           True   inf   \n",
       "42            1         8         32      False           True   inf   \n",
       "43            1         8         32      False           True   inf   \n",
       "44            1         8         32      False           True   inf   \n",
       "45            5         8         32       True          False   inf   \n",
       "46            1         2         64       True          False   inf   \n",
       "47            1         8         32      False           True   inf   \n",
       "48            4         8         32       True          False   inf   \n",
       "49            2         4         32      False           True   inf   \n",
       "\n",
       "     accuracy  \n",
       "0   94.921875  \n",
       "1   98.710317  \n",
       "2   97.070312  \n",
       "3   63.769531  \n",
       "4   96.191406  \n",
       "5   89.160156  \n",
       "6   94.628906  \n",
       "7   96.582031  \n",
       "8   96.875000  \n",
       "9   96.777344  \n",
       "10  98.511905  \n",
       "11  97.519841  \n",
       "12  98.412698  \n",
       "13  98.015873  \n",
       "14  98.611111  \n",
       "15  95.507812  \n",
       "16  97.519841  \n",
       "17  99.107143  \n",
       "18  95.312500  \n",
       "19  94.246032  \n",
       "20  98.611111  \n",
       "21  96.726190  \n",
       "22  98.611111  \n",
       "23  97.718254  \n",
       "24  98.412698  \n",
       "25  97.363281  \n",
       "26  97.916667  \n",
       "27  97.519841  \n",
       "28  96.484375  \n",
       "29  94.742063  \n",
       "30  96.031746  \n",
       "31  98.412698  \n",
       "32  98.115079  \n",
       "33  65.773810  \n",
       "34  96.329365  \n",
       "35  96.130952  \n",
       "36  95.703125  \n",
       "37  94.921875  \n",
       "38  65.527344  \n",
       "39  97.363281  \n",
       "40  48.809524  \n",
       "41  98.214286  \n",
       "42  98.611111  \n",
       "43  98.214286  \n",
       "44  46.626984  \n",
       "45  20.039683  \n",
       "46  98.412698  \n",
       "47  92.658730  \n",
       "48  96.484375  \n",
       "49  97.222222  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res = search._results()\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4a5079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python [conda env:giottodeep]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
