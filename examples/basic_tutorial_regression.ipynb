{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: regression task\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial provides you with the basic example of regression.\n",
    "Regression tasks can be performed in *giotto-deep* as easily as classification ones.\n",
    "\n",
    "## Scope\n",
    "\n",
    "The scope of a regression task in similar to a classificaiton one, with the difference that the space of classes is infinite! Hence, we cannot really know how to extend the losses and metrics of classificaiton tasks directly to regressions ones: however, the models, the training, the optimisation,... remain basically the same!\n",
    "\n",
    "![img](./images/regression.jpeg)\n",
    "\n",
    "In this tutorial we will try to fit a line in a 3D space: not super exciting indeed, but then you can have fun and use these tools to fit the curves in the *forex market* for example!\n",
    "\n",
    "## Content\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. Creation of a dataset\n",
    " 2. Creation of a model\n",
    " 3. Define metrics and losses\n",
    " 4. Run trainig\n",
    " 5. Visualise results interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from gtda.diagrams import BettiCurve\n",
    "from gtda.plotting import plot_betti_surfaces\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "from gdeep.search import GiottoSummaryWriter\n",
    "from gdeep.models import FFNet\n",
    "from gdeep.visualization import persistence_diagrams_of_activations\n",
    "from gdeep.trainer import Trainer\n",
    "from gdeep.data.datasets import DatasetBuilder, FromArray, DataLoaderBuilder\n",
    "from gdeep.utility import DEVICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the results of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There you can run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/), after the training phase, to see all the visualization results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = GiottoSummaryWriter()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset\n",
    "\n",
    "The dataset that we create is a 3D dataset representing a noisy hyperplane. The `X` is generated at random while the `y` has a linear relation with `X`. The goal is to see if we can get `0.3` as linear coefficient once the model has been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(np.random.rand(100, 3), dtype=np.float32)\n",
    "y_train = np.array(\n",
    "    0.3 * np.array(list(map(sum, X_train))).reshape(-1, 1), dtype=np.float32\n",
    ")  # a hyperplane\n",
    "\n",
    "X_val = np.array(np.random.rand(50, 3), dtype=np.float32)\n",
    "y_val = np.array(\n",
    "    0.3 * np.array(list(map(sum, X_train))).reshape(-1, 1), dtype=np.float32\n",
    ")\n",
    "\n",
    "dl_builder = DataLoaderBuilder((FromArray(X_train, y_train), FromArray(X_val, y_val)))\n",
    "dl_tr, dl_val, _ = dl_builder.build(({\"batch_size\": 32}, {\"batch_size\": 16}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train your model\n",
    "\n",
    "The model is a simple feed-forward network: simple task, simple mdoel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model1, self).__init__()\n",
    "        self.seqmodel = FFNet(arch=[3, 5, 1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seqmodel(x)\n",
    "\n",
    "\n",
    "model = model1()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your metric\n",
    "\n",
    "In case of regression tasks, accuracy is not really a good metric. We propose here to compute the $L_1$ norm between the prediction and the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_norm(prediction, y):\n",
    "    return torch.norm(prediction - y, p=1).to(DEVICE)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "We are finally here: after having set up the dataset, the model, the metrics... we are now ready to put all of this together with giotto-deep into a `Trainer` class. Check the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "pipe = Trainer(model, (dl_tr, dl_val), loss_fn, writer, l1_norm)\n",
    "\n",
    "# train the model with learning rate scheduler\n",
    "pipe.train(\n",
    "    Adam,\n",
    "    3,\n",
    "    False,\n",
    "    lr_scheduler=ExponentialLR,\n",
    "    scheduler_params={\"gamma\": 0.9},\n",
    "    profiling=False,\n",
    "    store_grad_layer_hist=True,\n",
    "    writer_tag=\"line\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Checking the results\n",
    "\n",
    "Did we manage to get `0.3` after this first part of the training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.model(torch.tensor([[1, 1, 1]]).float().to(DEVICE))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's **train again** the model with cross validation: we just have to set the parameter `cross_validation = True`.\n",
    "\n",
    "The `keep_training = True` flag allows us to resume the training from the same scheduler, optimiser and trained model obtained at the end of the last training in the instance of the class `pipe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model with CV\n",
    "pipe.train(SGD, 3, cross_validation=True, keep_training=True)\n",
    "\n",
    "# since we used the keep training flag, the optimiser has not been modified compared to the previous training.\n",
    "print(pipe.optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did we manage this time to get `0.3`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "pipe.model(torch.tensor([[1, 1, 1]]).float().to(DEVICE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
