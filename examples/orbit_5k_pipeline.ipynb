{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial : Classifying Orbit5k with Persformer\n",
    "\n",
    "#### Authors : Nicolas Berkouk and Raphael Reinauer\n",
    "\n",
    "\n",
    "The Orbit5k dataset consists of 5000 subsets of $[0,1]^2$, each containing $1000$ points. For a given $\\rho \\in \\{2.5, 3.5, 4.0, 4.1,4.3\\}$, each subset $S^\\rho \\subset [0,1]^2 $ is generated randomly according to the following procedure:\n",
    "\n",
    "1. Start with a uniformly randomly sampled point $(x_0,y_0)\\in [0,1]^2$\n",
    "2. For $n \\geq 1$, generate $(x_{n+1},y_{n+1})$ inductively by : \n",
    "\n",
    "$$ x_{n+1} = x_{n} + \\rho y_n(1-y_n) ~~~\\text{mod }1$$\n",
    "$$ y_{n+1} = y_{n} + \\rho x_{n+1}(1-x_{n+1}) ~~~\\text{mod }1$$\n",
    "\n",
    "3. Define $S^\\rho := \\{(x_i,y_i) \\mid i = 0...999 \\}$\n",
    "\n",
    "We generate 1000 orbits for each value of $\\rho$. \n",
    "\n",
    "The classification problem is then to recover the parameter $\\rho$ from the persistence diagrams of $S^\\rho$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include necessary general imports\n",
    "import os\n",
    "from typing import Tuple\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Torch imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Gdeep imports \n",
    "\n",
    "from gdeep.data import PreprocessingPipeline\n",
    "from gdeep.data.datasets import PersistenceDiagramFromFiles\n",
    "from gdeep.data.datasets.base_dataloaders import (DataLoaderBuilder,\n",
    "                                                  DataLoaderParamsTuples)\n",
    "from gdeep.data.datasets.persistence_diagrams_from_graphs_builder import \\\n",
    "    PersistenceDiagramFromGraphBuilder\n",
    "from gdeep.data.persistence_diagrams.one_hot_persistence_diagram import (\n",
    "    OneHotEncodedPersistenceDiagram, collate_fn_persistence_diagrams)\n",
    "from gdeep.data.preprocessors import (\n",
    "    FilterPersistenceDiagramByHomologyDimension,\n",
    "    FilterPersistenceDiagramByLifetime, NormalizationPersistenceDiagram)\n",
    "from gdeep.search.hpo import GiottoSummaryWriter\n",
    "from gdeep.topology_layers import Persformer, PersformerConfig, PersformerWrapper\n",
    "from gdeep.topology_layers.persformer_config import PoolerType\n",
    "from gdeep.trainer.trainer import Trainer\n",
    "from gdeep.search import HyperParameterOptimization\n",
    "from gdeep.utility import DEFAULT_GRAPH_DIR, PoolerType\n",
    "from gdeep.utility.utils import autoreload_if_notebook\n",
    "from gdeep.analysis.interpretability import Interpreter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Subset\n",
    "from gdeep.visualization import Visualiser\n",
    "from gdeep.data.datasets import OrbitsGenerator, DataLoaderKwargs\n",
    "\n",
    "\n",
    "\n",
    "autoreload_if_notebook()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Orbit5K dataloaders thanks to gdeep helpers functions\n",
    "\n",
    "In the next cell, we define the configuration parameters of the Orbit5K dataset, and build the dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a configuration file with the parameters of the desired dataset\n",
    "@dataclass\n",
    "class Orbit5kConfig():\n",
    "    batch_size_train: int = 4\n",
    "    num_orbits_per_class: int = 32\n",
    "    validation_percentage: float = 0.0\n",
    "    test_percentage: float = 0.0\n",
    "    num_jobs: int = 8\n",
    "    dynamical_system: str = \"classical_convention\"\n",
    "    homology_dimensions: Tuple[int, int] = (0, 1)  # type: ignore\n",
    "    dtype: str = \"float32\"\n",
    "    arbitrary_precision: bool = False\n",
    "\n",
    "config_data = Orbit5kConfig()\n",
    "\n",
    "# Define the OrbitsGenerator Class with the above parameters    \n",
    "\n",
    "og = OrbitsGenerator(\n",
    "    num_orbits_per_class=config_data.num_orbits_per_class,\n",
    "    homology_dimensions=config_data.homology_dimensions,\n",
    "    validation_percentage=config_data.validation_percentage,\n",
    "    test_percentage=config_data.test_percentage,\n",
    "    n_jobs=config_data.num_jobs,\n",
    "    dynamical_system=config_data.dynamical_system,\n",
    "    dtype=config_data.dtype,\n",
    ")\n",
    "\n",
    "\n",
    "# Define the data loader\n",
    "\n",
    "dataloaders_dicts = DataLoaderKwargs(\n",
    "    train_kwargs={\"batch_size\": config_data.batch_size_train,},\n",
    "    val_kwargs={\"batch_size\": 4},\n",
    "    test_kwargs={\"batch_size\": 3},\n",
    ")\n",
    "\n",
    "if len(config_data.homology_dimensions) == 0:\n",
    "    dl_train, _, _ = og.get_dataloader_orbits(dataloaders_dicts)\n",
    "else:\n",
    "    dl_train, _, _ = og.get_dataloader_persistence_diagrams(dataloaders_dicts)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualise some different orbits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the orbits point clouds\n",
    "\n",
    "point_clouds = og.get_orbits()\n",
    "\n",
    "# For each rho value, plot one point cloud\n",
    "\n",
    "rho_values = [2.5, 3.5, 4.0, 4.1, 4.3]\n",
    "fig, ax = plt.subplots(ncols=len(rho_values), figsize = (20,3))\n",
    "\n",
    "for i in range(len(rho_values)):\n",
    "    x , y = point_clouds[i*config_data.num_orbits_per_class,:,0], point_clouds[i*config_data.num_orbits_per_class,:,1] \n",
    "    ax[i].scatter(x, y)\n",
    "    ax[i].set_title('Example of orbit for rho = ' + str(rho_values[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the Persformer achitecture\n",
    "\n",
    "To directly initialize the Persformer model, we wrap the Persformer class using `PersformerWrapper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model by using a Wrapper for the Persformer model\n",
    "\n",
    "wrapped_model = PersformerWrapper(\n",
    "    num_attention_layers=2,\n",
    "    num_attention_heads=8,\n",
    "    input_size= 2 + 2,\n",
    "    output_size=5,\n",
    "    pooler_type=PoolerType.ATTENTION,\n",
    "    hidden_size=16,\n",
    "    intermediate_size=16,\n",
    ")\n",
    "\n",
    "# Define the trainer \n",
    "\n",
    "writer = GiottoSummaryWriter()\n",
    "\n",
    "loss_function =  nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = Trainer(wrapped_model, [dl_train, dl_train], loss_function, writer) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are ready to train Persformer!\n",
    "\n",
    "### Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the results of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/examples` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualization results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model for one epoch\n",
    "\n",
    "n_epoch = 1\n",
    "\n",
    "trainer.train(Adam, n_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability : Saliency Maps for persistence diagrams\n",
    "\n",
    "One of the key feature of the Persformer achitecture, is that it is proven to satisfy a Universal Approximation Theorem. The authors of the [Persformer paper](https://arxiv.org/abs/2112.15210) also give evidence that this achitecture has less inductive bias than already existing deep neural networks architecture that can input persistence diagrams. This observation motivates the introduction of importance score of points in persistence diagrams in a classification task.\n",
    " \n",
    "The $\\textbf{Persformer}$ model for a classification problem is an almost everywhere differentiable function $F: \\mathcal D \\to \\mathbb{R}^m$, where $m$ is the number of classes and $\\mathcal D$ is the space of persistence diagrams. It maps a persistence diagram to the logits of the class probability. Let $d$ be the maximum homology dimension to be considered and let $x = (x_k)_{k\\in \\{ 1,\\ldots, n \\}}\\in (\\R^{2+d})^n$ be a persistence diagram and $i(x) = \\mathrm{argmax}_j F(x)_j$. The first two coordinates of $x_k \\in \\mathbb{R}^{2+d}$ are the birth and death coordinates and the last $d$ coordinates are the one-hot encoded homology dimensions. \n",
    "\n",
    "The *saliency map* of $F$ on $x$ is defined as \n",
    "$$ \\mathcal{S}_F(x) :=\n",
    "\\left (\\left\\|\\frac{\\partial F_{i(x)}(x)}{\\partial x_k}\\right\\|_2 \\right )_{k \\in \\{ 1,\\ldots, n \\}}\\in \\mathbb{R}_{\\geq 0}^n.\n",
    "$$\n",
    "\n",
    "Therefore, $\\mathcal{S}_F$ assigns to each point in a persistence diagram, a real value indicating how important a given point in the diagram is for the classification.\n",
    "\n",
    "With Giotto-deep, it is possible to compute saliency maps of a Persformer model with just a few lines of code. Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Interpreter class in Saliency mode\n",
    "\n",
    "inter = Interpreter(trainer.model, method=\"Saliency\")\n",
    "\n",
    "# Get a datum and its corresponding class\n",
    "\n",
    "batch = next(iter(dl_train))\n",
    "datum = batch[0][0].reshape(1, *(batch[0][0].shape))\n",
    "class_ = batch[1][0].item()\n",
    "\n",
    "# interpret the diagram\n",
    "x, attr = inter.interpret(x=datum, y=class_)\n",
    "\n",
    "# visualise the results\n",
    "vs = Visualiser(trainer)\n",
    "vs.plot_attributions_persistence_diagrams(inter)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
