{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12016045",
   "metadata": {},
   "source": [
    "# Tutorial: Deploying Regularizers with Giotto-deep\n",
    "\n",
    "**Author: Henry Kirveslahti**\n",
    "\n",
    "In this tutorial we discuss the technical details for implementing regularizers and their use in *giotto-deep*. For a less technical introduction to regularization, please refer to the notebook *Basic Tutorial: Regularization with Giotto-deep*.\n",
    "\n",
    "The notebook is organized as follows:\n",
    "\n",
    "1. Example of a custom regularizer\n",
    "2. Hyper-parameter tuning\n",
    "3. Ad hoc regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313734c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam, RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gdeep.trainer import Trainer\n",
    "from gdeep.trainer.regularizer import Regularizer\n",
    "from gdeep.trainer.regularizer import TihonovRegularizer\n",
    "from gdeep.search import GiottoSummaryWriter\n",
    "from gdeep.models import ModelExtractor\n",
    "from gdeep.utility import DEVICE\n",
    "from gdeep.search import HyperParameterOptimization\n",
    "from gdeep.models import FFNet\n",
    "writer = GiottoSummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3f02a3",
   "metadata": {},
   "source": [
    "## 1. Custom Regularizers\n",
    "*Giotto-deep* has already built-in support for $p$-norm regularization, but the framework allows for defining custom regularizers. Below we define the elastic net. It is similar to the existing $p$-norm regularizer, but the penalty term reads\n",
    "\n",
    "$$\n",
    "p_i = \\lambda_1 \\sum \\big( ||\\beta||_1 \\big) + \\lambda_2 \\sum \\big( ||\\beta||_2^2 \\big)\n",
    "$$\n",
    "\n",
    "Typically, a regularizer has just one penalty coefficient $\\lambda$. The Elastic net we have two of these, so we need to override the default behavior by specifying the init function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead0ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticNet:\n",
    "    def __init__(self, lamda1,lamda2):\n",
    "        self.lamda1=lamda1\n",
    "        self.lamda2=lamda2\n",
    "    def regularization_penalty(self, model):\n",
    "        \"\"\"\n",
    "        The penalty is a combination of the L1 and L2 norms:\n",
    "        \"\"\"\n",
    "        total = torch.tensor(0, dtype=float)\n",
    "        for parameter in model.parameters():\n",
    "            total = total + self.lambda1 * torch.norm(parameter, 1) \\\n",
    "                  + self.lambda2 * torch.norm(parameter, 2)**2\n",
    "        return total    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb110d",
   "metadata": {},
   "source": [
    "This is a simple regularizer much in spirit of the $p$-norm regularizers in that it does not require any preprocessing nor parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ba53c9",
   "metadata": {},
   "source": [
    "## 2. Hyper parameter tuning\n",
    "An important aspect of regularization is that of hyper parameter-tuning. To this end, we can use the HPO. Let us first do the example from last notebook: We saw how the value of $\\lambda$ about 0.2 boosted the regression coefficient $\\alpha_1$ while eliminating the other coefficient $\\alpha_2$ that had higher signal-to-noise ratio. Let us see which value of $\\lambda$ gives us the best performance when we predict on the validation set.\n",
    "\n",
    "To recap what we did, first we just run our models from last time on a smaller dataset (it won't take long):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b640c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "S=100\n",
    "z0=rng.standard_normal(S)\n",
    "z1=0.9*z0+0.1*rng.standard_normal(S)\n",
    "z2=0.85*z0+0.15*rng.standard_normal(S)\n",
    "y=z0+rng.standard_normal(S)\n",
    "X=np.stack([z1,z2],1)\n",
    "y=y.reshape(-1,1)\n",
    "y=y.astype(float)\n",
    "X=X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa5b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(X, y, test_size=0.1)\n",
    "tensor_x_t = torch.Tensor(train_x)\n",
    "tensor_x_t=tensor_x_t.float()\n",
    "tensor_y_t = torch.from_numpy(train_y)\n",
    "tensor_y_t=tensor_y_t.float()\n",
    "tensor_x_v = torch.Tensor(val_x)\n",
    "tensor_y_v = torch.from_numpy(val_y)\n",
    "train_dataset = TensorDataset(tensor_x_t,tensor_y_t)\n",
    "dl_tr = DataLoader(train_dataset,batch_size=10)\n",
    "val_dataset = TensorDataset(tensor_x_v,tensor_y_v)\n",
    "dl_val = DataLoader(val_dataset,batch_size=10)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,featdim='2'):\n",
    "        super(Net, self).__init__() \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(eval(featdim), 1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d89b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "network=Net('2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c053cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm(prediction, y):\n",
    "    return torch.norm(prediction - y, p=2).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a40a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "pipe = Trainer(network, (dl_tr, dl_val), loss_fn, writer,l2_norm)\n",
    "pipe.train(SGD, 20, False, {\"lr\": 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e0b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Trainer(network, (dl_tr, dl_val), loss_fn, writer,l2_norm,regularizer=TihonovRegularizer(0.2,p=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2.train(SGD, 20, False, {\"lr\": 0.01})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7c70b",
   "metadata": {},
   "source": [
    "### The optimization - LASSO\n",
    "Next we take 100 runs on the HPO to try to find the best value for $\\lambda$ for the LASSO in the range $[0.05,0.5]$ with step size $0.01$. We specify the regularization parameters by putting the regularizer, together with its parameters in a dictionary. For details on HPO, please see the HPO tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = HyperParameterOptimization(pipe, \"accuracy\", 100, best_not_last=True)\n",
    "search.regularize=True\n",
    "search.store_pickle = True\n",
    "reg=TihonovRegularizer\n",
    "optimizers_params = {\"lr\": [0.01]}\n",
    "dataloaders_params = {}\n",
    "models_hyperparams = {}\n",
    "regularization_params={'regularizer':[reg],'lamda':[0.05,0.5,0.01],'p':[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the HPO\n",
    "search.start(\n",
    "    [SGD],\n",
    "    30,\n",
    "    False,\n",
    "    optimizers_params,\n",
    "    dataloaders_params,\n",
    "    models_hyperparams,\n",
    "    regularization_params=regularization_params,\n",
    "    n_accumulated_grads=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97396e8",
   "metadata": {},
   "source": [
    "### Optimization - Custom regularizer\n",
    "Next we do the same thing for our custom regularizer that we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d99c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = HyperParameterOptimization(pipe, \"accuracy\", 20, best_not_last=True)\n",
    "search.regularize=True\n",
    "search.store_pickle = True\n",
    "reg=ElasticNet\n",
    "optimizers_params = {\"lr\": [0.01]}\n",
    "dataloaders_params = {}\n",
    "models_hyperparams = {}\n",
    "regularization_params={'reg':[reg], 'lambda1':[0.15,0.85,0.01],'lambda2':[0.0001,0.1,0.01]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e88f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.start(\n",
    "    [SGD],\n",
    "    30,\n",
    "    False,\n",
    "    optimizers_params,\n",
    "    dataloaders_params,\n",
    "    models_hyperparams,\n",
    "    regularization_params=regularization_params,\n",
    "    n_accumulated_grads=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa0920",
   "metadata": {},
   "source": [
    "## 3. Ad hoc regularizers\n",
    "\n",
    "The penalties in the regularizers that we have seen so far have been straightforward functions of the model parameters. Here we show an example of an ad hoc regularizer that directly penalizes the behavior of the model. Such regularizers may depend on external parameters, and the logic is completely wrapped in the regularizer object.\n",
    "\n",
    "First we generate some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "S=1000\n",
    "X=np.linspace(0,2*np.pi,S)\n",
    "y=3*np.sin(X)+0.5*rng.standard_normal(S)\n",
    "plt.plot(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae443441",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(X, y, test_size=0.1)\n",
    "tensor_x_t = torch.Tensor(train_x).reshape(-1, 1)\n",
    "tensor_x_t=tensor_x_t.float()\n",
    "tensor_y_t = torch.from_numpy(train_y).reshape(-1, 1)\n",
    "tensor_y_t=tensor_y_t.float()\n",
    "tensor_x_v = torch.Tensor(val_x)\n",
    "tensor_y_v = torch.from_numpy(val_y)\n",
    "train_dataset = TensorDataset(tensor_x_t,tensor_y_t)\n",
    "dl_tr = DataLoader(train_dataset,batch_size=10)\n",
    "val_dataset = TensorDataset(tensor_x_v,tensor_y_v)\n",
    "dl_val = DataLoader(val_dataset,batch_size=10)\n",
    "\n",
    "class model1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model1, self).__init__()\n",
    "        self.seqmodel = FFNet(arch=[1, 10,10,10, 1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seqmodel(x)\n",
    "\n",
    "\n",
    "model = model1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9803b7",
   "metadata": {},
   "source": [
    "We fit a basic unregularized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e25752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "pipe = Trainer(model, (dl_tr, dl_val), loss_fn, writer,l2_norm)\n",
    "pipe.train(SGD, 200, False, {\"lr\": 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321198df",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp=pipe.model(dl_tr.dataset.tensors[0])\n",
    "X_t=dl_tr.dataset.tensors[0].detach().numpy().reshape(-1)\n",
    "y_t=resp.detach().numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde5feb",
   "metadata": {},
   "source": [
    "Let us take a look of the graph of the model we defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6517eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=np.argsort(X_t)\n",
    "plt.plot(X_t[ind],y_t[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb25728",
   "metadata": {},
   "source": [
    "Next we define an ad hoc regularizer that penalizes the function from attaining values higher than 2, and we do this in a very barbarian way to demonstrate the regularization logic. In general, these kind of restrictions could also be effectively imposed by a suitable model architecture.\n",
    "\n",
    "Our regularizer evaluates the model on a grid, which is defined by preprocess step. The penalty consists of evaluating the model on this grid, and then picking out the points where the model exceeded 2. The penalty is the squared sum of the model values at these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a4cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapReg:\n",
    "    def __init__(self,lamda):\n",
    "        self.lamda=lamda\n",
    "        self.X=torch.linspace(0,2*torch.pi,1000)\n",
    "\n",
    "    def regularization_penalty(self, model):\n",
    "        \"\"\"\n",
    "        We penalized the squared values of the function at the points where it attains value higher than 2.\n",
    "        \"\"\"\n",
    "        res=model(self.X.reshape(-1,1)).reshape(-1)\n",
    "        inds1=torch.where(res>2)        \n",
    "        X1=self.X[inds1]\n",
    "        res1=model(X1.reshape(-1,1)).reshape(-1)\n",
    "        return torch.sum(res1**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf4c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg=CapReg(lamda=1/(2*S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa4bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Trainer(model, (dl_tr, dl_val), loss_fn, writer,l2_norm,regularizer=reg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da3115",
   "metadata": {},
   "source": [
    "The regression penalty that we defined is computed for every batch in our dataset. This way, their gradients are updated every single batch. A reasonable pick for the regression penalty coefficient $\\lambda$ should then be inversely proportional to the number of batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3999a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2.train(SGD, 10, False, {\"lr\": 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65c2db1",
   "metadata": {},
   "source": [
    "Let us now take a look at the graphs of the two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3587a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp=pipe2.model(dl_tr.dataset.tensors[0])\n",
    "X_t2=dl_tr.dataset.tensors[0].detach().numpy().reshape(-1)\n",
    "y_t2=resp.detach().numpy().reshape(-1)\n",
    "ind2=np.argsort(X_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caaba36",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(X_t[ind],y_t[ind], label = 'unregularized')\n",
    "plt.plot(X_t2[ind2],y_t2[ind2], label = 'regularized')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a480cb37",
   "metadata": {},
   "source": [
    "We see the regularizer does penalize the graph from taking values greater than 2. This type of regularization will also have side effects on the rest of graph, because the model is quite simple and there is no straightforward connection between the function value being higher than 2 and the network weights. This could be improved by tailoring the model architecture possibly in conjuction with a suitable regularizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
