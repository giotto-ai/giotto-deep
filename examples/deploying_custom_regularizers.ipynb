{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12016045",
   "metadata": {},
   "source": [
    "# Tutorial: Deploying Regularizers with Giotto-deep\n",
    "\n",
    "**Author: Henry Kirveslahti**\n",
    "\n",
    "In this tutorial we discuss the technical details for implementing regularizers and their use in *giotto-deep*. For a less technical introduction to regularization, please refer to the notebook *Basic Tutorial: Regularization with Giotto-deep*.\n",
    "\n",
    "The notebook is organized as follows:\n",
    "\n",
    "1. Example of a custom regularizer\n",
    "2. Hyper-parameter tuning\n",
    "3. Ad hoc regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313734c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPUs...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam, RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gdeep.trainer import Trainer\n",
    "from gdeep.trainer.regularizer import Regularizer\n",
    "from gdeep.trainer.regularizer import TihonovRegularizer\n",
    "from gdeep.search import GiottoSummaryWriter\n",
    "from gdeep.models import ModelExtractor\n",
    "from gdeep.utility import DEVICE\n",
    "from gdeep.search import HyperParameterOptimization\n",
    "from gdeep.models import FFNet\n",
    "writer = GiottoSummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3f02a3",
   "metadata": {},
   "source": [
    "## 1. Custom Regularizers\n",
    "*Giotto-deep* has already built-in support for $p$-norm regularization, but the framework allows for defining custom regularizers. Below we define the elastic net. It is similar to the existing $p$-norm regularizer, but the penalty term reads\n",
    "\n",
    "$$\n",
    "p_i = \\lambda_1 \\sum \\big( ||\\beta||_1 \\big) + \\lambda_2 \\sum \\big( ||\\beta||_2^2 \\big)\n",
    "$$\n",
    "\n",
    "Typically, a regularizer has just one penalty coefficient $\\lambda$. The Elastic net we have two of these, so we need to override the default behavior by specifying the init function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eead0ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticNet:\n",
    "    def __init__(self, lamda1,lamda2):\n",
    "        self.lamda1=lamda1\n",
    "        self.lamda2=lamda2\n",
    "    def regularization_penalty(self, model):\n",
    "        \"\"\"\n",
    "        The penalty is a combination of the L1 and L2 norms:\n",
    "        \"\"\"\n",
    "        total = torch.tensor(0, dtype=float)\n",
    "        for parameter in model.parameters():\n",
    "            total = total + self.lambda1 * torch.norm(parameter, 1) \\\n",
    "                  + self.lambda2 * torch.norm(parameter, 2)**2\n",
    "        return total    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb110d",
   "metadata": {},
   "source": [
    "This is a simple regularizer much in spirit of the $p$-norm regularizers in that it does not require any preprocessing nor parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ba53c9",
   "metadata": {},
   "source": [
    "## 2. Hyper parameter tuning\n",
    "An important aspect of regularization is that of hyper parameter-tuning. To this end, we can use the HPO. Let us first do the example from last notebook: We saw how the value of $\\lambda$ about 0.2 boosted the regression coefficient $\\alpha_1$ while eliminating the other coefficient $\\alpha_2$ that had higher signal-to-noise ratio. Let us see which value of $\\lambda$ gives us the best performance when we predict on the validation set.\n",
    "\n",
    "To recap what we did, first we just run our models from last time on a smaller dataset (it won't take long):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9b640c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "S=100\n",
    "z0=rng.standard_normal(S)\n",
    "z1=0.9*z0+0.1*rng.standard_normal(S)\n",
    "z2=0.85*z0+0.15*rng.standard_normal(S)\n",
    "y=z0+rng.standard_normal(S)\n",
    "X=np.stack([z1,z2],1)\n",
    "y=y.reshape(-1,1)\n",
    "y=y.astype(float)\n",
    "X=X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27fa5b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(X, y, test_size=0.1)\n",
    "tensor_x_t = torch.Tensor(train_x)\n",
    "tensor_x_t=tensor_x_t.float()\n",
    "tensor_y_t = torch.from_numpy(train_y)\n",
    "tensor_y_t=tensor_y_t.float()\n",
    "tensor_x_v = torch.Tensor(val_x)\n",
    "tensor_y_v = torch.from_numpy(val_y)\n",
    "train_dataset = TensorDataset(tensor_x_t,tensor_y_t)\n",
    "dl_tr = DataLoader(train_dataset,batch_size=10)\n",
    "val_dataset = TensorDataset(tensor_x_v,tensor_y_v)\n",
    "dl_val = DataLoader(val_dataset,batch_size=10)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,featdim='2'):\n",
    "        super(Net, self).__init__() \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(eval(featdim), 1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d89b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "network=Net('2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c053cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm(prediction, y):\n",
    "    return torch.norm(prediction - y, p=2).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31a40a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehdi/Documents/giotto-deep/venv/lib/python3.8/site-packages/torch/cuda/__init__.py:497: UserWarning:\n",
      "\n",
      "Can't initialize NVML\n",
      "\n",
      "/home/mehdi/Documents/giotto-deep/gdeep/trainer/trainer.py:665: UserWarning:\n",
      "\n",
      "Cannot store data in the PR curve\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.138893 \tEpoch training l2_norm: 3.20%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.90%,                 Avg loss: 0.843773 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.929893 \tEpoch training l2_norm: 2.97%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.88%,                 Avg loss: 0.832076 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925090 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.88%,                 Avg loss: 0.829517 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.924768 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.88%,                 Avg loss: 0.827400 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.924650 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.87%,                 Avg loss: 0.825371 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.924548 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.87%,                 Avg loss: 0.823413 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.924454 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.87%,                 Avg loss: 0.821523 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.924366 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.86%,                 Avg loss: 0.819697 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.924284 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.86%,                 Avg loss: 0.817933 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.924208 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.86%,                 Avg loss: 0.816230 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.924137 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.85%,                 Avg loss: 0.814584 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.924072 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.85%,                 Avg loss: 0.812995 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.924010 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.85%,                 Avg loss: 0.811459 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.923953 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.85%,                 Avg loss: 0.809976 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.923900 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.84%,                 Avg loss: 0.808543 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.923851 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.84%,                 Avg loss: 0.807159 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.923805 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.84%,                 Avg loss: 0.805821 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.923763 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.84%,                 Avg loss: 0.804528 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.923723 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.83%,                 Avg loss: 0.803279 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.923687 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.83%,                 Avg loss: 0.802072 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8020720689133871, tensor(2.8321, dtype=torch.float64))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "pipe = Trainer(network, (dl_tr, dl_val), loss_fn, writer,l2_norm)\n",
    "pipe.train(SGD, 20, False, {\"lr\": 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3e0b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Trainer(network, (dl_tr, dl_val), loss_fn, writer,l2_norm,regularizer=TihonovRegularizer(0.2,p=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2253a954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.706082 \tEpoch training l2_norm: 3.79%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 3.56%,                 Avg loss: 1.265369 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.459512 \tEpoch training l2_norm: 3.47%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 3.28%,                 Avg loss: 1.075488 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.307753 \tEpoch training l2_norm: 3.26%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 3.10%,                 Avg loss: 0.960881 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.213044 \tEpoch training l2_norm: 3.14%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 2.99%,                 Avg loss: 0.891143 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.154838 \tEpoch training l2_norm: 3.06%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 2.93%,                 Avg loss: 0.856383 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.136455 \tEpoch training l2_norm: 3.02%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 2.90%,                 Avg loss: 0.842142 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.129777 \tEpoch training l2_norm: 3.00%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 2.89%,                 Avg loss: 0.832541 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.125640 \tEpoch training l2_norm: 2.99%                                        ard1>)  \t[ 9 / 9 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 2.87%,                 Avg loss: 0.825967 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.123056 \tEpoch training l2_norm: 2.98%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 2.87%,                 Avg loss: 0.821396 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.121427 \tEpoch training l2_norm: 2.97%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 2.86%,                 Avg loss: 0.818173 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.120388 \tEpoch training l2_norm: 2.97%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 2.86%,                 Avg loss: 0.815871 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.119717 \tEpoch training l2_norm: 2.97%                                        ard1>)  \t[ 9 / 9 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 2.85%,                 Avg loss: 0.814210 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.119277 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 2.85%,                 Avg loss: 0.813000 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.118984 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 2.85%,                 Avg loss: 0.812114 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.118786 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 2.85%,                 Avg loss: 0.811463 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.118650 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     tensor(2.2491, grad_fn=<NormBackward1>)  \t[ 6 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 2.85%,                 Avg loss: 0.810983 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.118554 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 2.85%,                 Avg loss: 0.810632 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.118486 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 2.85%,                 Avg loss: 0.810376 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.118437 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 2.85%,                 Avg loss: 0.810191 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.118401 \tEpoch training l2_norm: 2.96%                                        ward1>)  \t[ 9 / 9 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 2.85%,                 Avg loss: 0.810061 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8100613526505803, tensor(2.8462, dtype=torch.float64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2.train(SGD, 20, False, {\"lr\": 0.01})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7c70b",
   "metadata": {},
   "source": [
    "### The optimization - LASSO\n",
    "Next we take 100 runs on the HPO to try to find the best value for $\\lambda$ for the LASSO in the range $[0.05,0.5]$ with step size $0.01$. We specify the regularization parameters by putting the regularizer, together with its parameters in a dictionary. For details on HPO, please see the HPO tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0d0065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = HyperParameterOptimization(pipe, \"accuracy\", 100, best_not_last=True)\n",
    "search.regularize=True\n",
    "search.store_pickle = True\n",
    "reg=TihonovRegularizer\n",
    "optimizers_params = {\"lr\": [0.01]}\n",
    "dataloaders_params = {}\n",
    "models_hyperparams = {}\n",
    "regularization_params={'regularizer':[reg],'lamda':[0.05,0.5,0.01],'p':[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c03e0983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:20:44,170] A new study created in memory with name: VQRL9Z1I91HCLGQ5TKFW\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.613511468283832  \tBatch training l2_norm:  tensor(0.5178, grad_fn=<NormBackward1>)  \t[ 32 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehdi/Documents/giotto-deep/gdeep/search/hpo.py:260: UserWarning:\n",
      "\n",
      "Model cannot be re-initialized. Using existing one.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.290696 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.857930 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297288 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.861096 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297713 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.861556 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297745 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.861790 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297746 \tEpoch training l2_norm: 0.79%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.861999 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297743 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.862201 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297741 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.862396 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297744 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.858093 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297251 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.861450 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297706 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.861915 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297741 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.862138 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297741 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.862337 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297743 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.858036 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297252 \tEpoch training l2_norm: 0.79%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.861395 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297707 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.9942, grad_fn=<NormBackward1>)  \t[ 66 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.861861 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297741 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.862087 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297742 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     ensor(0.7246, grad_fn=<NormBackward1>)  \t[ 11 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.862287 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297742 \tEpoch training l2_norm: 0.79%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.857988 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297253 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.861348 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297707 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.861816 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297742 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.862043 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297742 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.862245 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297741 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.857947 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297253 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.861308 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297708 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.4525, grad_fn=<NormBackward1>)  \t[ 71 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.861778 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297742 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.862006 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297743 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.862209 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297741 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.862403 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.297745 \tEpoch training l2_norm: 0.79%                                        ard1>)  \t[ 90 / 90 ]                      tensor(0.6223, grad_fn=<NormBackward1>)  \t[ 34 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.858100 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.3416914658887045  \tBatch training l2_norm:  tensor(0.8018, grad_fn=<NormBackward1>)  \t[ 70 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:20:57,499] Trial 0 finished with value: 0.8094418170852062 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.38, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 0 with value: 0.8094418170852062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.297251 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.861457 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.38, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8100, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.2369, grad_fn=<NormBackward1>)  \t[ 20 / 90 ]                     tensor(1.3441, grad_fn=<NormBackward1>)  \t[ 62 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7675, grad_fn=<NormBackward1>)  \t[ 88 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.2484, grad_fn=<NormBackward1>)  \t[ 16 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7743, grad_fn=<NormBackward1>)  \t[ 23 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.452607939640681  \tBatch training l2_norm:  tensor(0.6851, grad_fn=<NormBackward1>)  \t[ 78 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:21:10,870] Trial 1 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.342211 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.864071 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.347510 \tEpoch training l2_norm: 0.80%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.870833 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.348417 \tEpoch training l2_norm: 0.80%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.870714 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.348771 \tEpoch training l2_norm: 0.80%                                        >)  \t[ 90 / 90 ]                          \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869689 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.348736 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874613 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.349276 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869855 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.348779 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874488 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.349277 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869712 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.348783 \tEpoch training l2_norm: 0.80%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874347 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.349276 \tEpoch training l2_norm: 0.80%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.875222 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.349324 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     ensor(0.2125, grad_fn=<NormBackward1>)  \t[ 12 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.870127 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.348777 \tEpoch training l2_norm: 0.80%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874727 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.349280 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     ensor(0.6750, grad_fn=<NormBackward1>)  \t[ 11 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869940 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.348778 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874568 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.349278 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869789 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.348781 \tEpoch training l2_norm: 0.80%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874422 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.349276 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.9915, grad_fn=<NormBackward1>)  \t[ 28 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869647 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.348784 \tEpoch training l2_norm: 0.80%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874285 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.349277 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.875162 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.349323 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.870069 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.348779 \tEpoch training l2_norm: 0.80%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874670 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.349280 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.6655, grad_fn=<NormBackward1>)  \t[ 76 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869885 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.348779 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874516 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.349277 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869738 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.348782 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874372 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.349275 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.875247 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.349325 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.870151 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.348777 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.3617, grad_fn=<NormBackward1>)  \t[ 53 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874750 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.349281 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869962 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.399081569668409  \tBatch training l2_norm:   tensor(0.1813, grad_fn=<NormBackward1>)  \t[ 73 / 90 ]                     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:21:25,138] Trial 2 finished with value: 0.8174008700055477 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.44, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.348777 \tEpoch training l2_norm: 0.80%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874590 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.44, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8178, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.4274, grad_fn=<NormBackward1>)  \t[ 41 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     ensor(1.7527, grad_fn=<NormBackward1>)  \t[ 3 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.440433953540871  \tBatch training l2_norm:  tensor(1.3942, grad_fn=<NormBackward1>)  \t[ 83 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:21:38,873] Trial 3 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.325158 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.864099 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.331133 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.866595 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.331820 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.865751 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.331809 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.870233 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.332338 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.865733 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.331856 \tEpoch training l2_norm: 0.80%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869935 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.332339 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.870670 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.332384 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.865861 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.331858 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.0228, grad_fn=<NormBackward1>)  \t[ 22 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.870036 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.332338 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.865518 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.331861 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.0598, grad_fn=<NormBackward1>)  \t[ 14 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869725 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.332343 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.870466 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.332381 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.865664 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.331862 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869846 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.332341 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.870582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.332382 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.865775 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.331859 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     ensor(0.7134, grad_fn=<NormBackward1>)  \t[ 10 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869953 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.332339 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.870686 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.332384 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.865876 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.331857 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.870051 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.332339 \tEpoch training l2_norm: 0.80%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.865532 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.331861 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869738 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.332343 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.870480 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.332381 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.865677 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.331861 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869858 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.332341 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.1784, grad_fn=<NormBackward1>)  \t[ 63 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.870594 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.332383 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.865787 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.331859 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869965 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.332339 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.8222, grad_fn=<NormBackward1>)  \t[ 37 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.870697 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.3967713910928914  \tBatch training l2_norm:  tensor(1.4688, grad_fn=<NormBackward1>)  \t[ 71 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:21:52,415] Trial 4 finished with value: 0.8122853353609738 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.42, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.332384 \tEpoch training l2_norm: 0.80%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.865887 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.42, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8152, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.0419572630276284  \tBatch training l2_norm:  tensor(0.7340, grad_fn=<NormBackward1>)  \t[ 60 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:21:52,890] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.985272 \tEpoch training l2_norm: 0.77%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798350 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.255567576778069  \tBatch training l2_norm:  tensor(0.0538, grad_fn=<NormBackward1>)  \t[ 89 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:21:53,340] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.255624 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.80%,                 Avg loss: 0.851189 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.0024215096562374  \tBatch training l2_norm:  tensor(0.7448, grad_fn=<NormBackward1>)  \t[ 82 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:21:53,779] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.985272 \tEpoch training l2_norm: 0.77%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798350 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.1095572321398839  \tBatch training l2_norm:  tensor(0.5478, grad_fn=<NormBackward1>)  \t[ 74 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:21:54,232] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.076583 \tEpoch training l2_norm: 0.77%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.77%,                 Avg loss: 0.810952 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.1142054349184036  \tBatch training l2_norm:  tensor(0.8727, grad_fn=<NormBackward1>)  \t[ 61 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:21:54,702] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.056653 \tEpoch training l2_norm: 0.77%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.77%,                 Avg loss: 0.807793 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.1947492635385557  \tBatch training l2_norm:  tensor(0.6519, grad_fn=<NormBackward1>)  \t[ 88 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:21:55,176] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.182509 \tEpoch training l2_norm: 0.77%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.79%,                 Avg loss: 0.831992 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.2591, grad_fn=<NormBackward1>)  \t[ 74 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.9043, grad_fn=<NormBackward1>)  \t[ 68 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      tensor(0.7681, grad_fn=<NormBackward1>)  \t[ 88 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      tensor(0.1294, grad_fn=<NormBackward1>)  \t[ 63 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.9470, grad_fn=<NormBackward1>)  \t[ 68 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7743, grad_fn=<NormBackward1>)  \t[ 23 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.1363, grad_fn=<NormBackward1>)  \t[ 43 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7175, grad_fn=<NormBackward1>)  \t[ 76 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.5189, grad_fn=<NormBackward1>)  \t[ 61 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.504497039213515  \tBatch training l2_norm:  tensor(0.6268, grad_fn=<NormBackward1>)  \t[ 57 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:22:08,756] Trial 11 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.383639 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.872057 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.387892 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.879395 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.389047 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.877581 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.389376 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.881822 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.389885 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.881735 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.389948 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.887949 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390534 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     ensor(0.6988, grad_fn=<NormBackward1>)  \t[ 8 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.882495 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390007 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.888166 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390543 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.882651 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390003 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.888312 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390544 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.1737, grad_fn=<NormBackward1>)  \t[ 73 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.882792 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390003 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.881714 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.389965 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.887831 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390535 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.882371 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390011 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.888045 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390542 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.882534 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390007 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.0483, grad_fn=<NormBackward1>)  \t[ 29 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.888198 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390543 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      tensor(0.7766, grad_fn=<NormBackward1>)  \t[ 65 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.882681 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390002 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.881607 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.389968 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.887727 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390534 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.882270 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390015 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.887947 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390541 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.882439 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.888106 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390542 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.882592 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390005 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.888255 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390544 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.882737 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.390003 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.881661 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.389967 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.887779 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.472087470193704  \tBatch training l2_norm:  tensor(1.3081, grad_fn=<NormBackward1>)  \t[ 72 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:22:21,892] Trial 12 finished with value: 0.8214685691510735 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.49, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.390534 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.882321 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.49, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8247, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.2886188486891408  \tBatch training l2_norm:  tensor(1.3755, grad_fn=<NormBackward1>)  \t[ 62 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:22:22,350] Trial 13 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.210314 \tEpoch training l2_norm: 0.78%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.79%,                 Avg loss: 0.838807 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.5145003433321036  \tBatch training l2_norm:  tensor(0.0362, grad_fn=<NormBackward1>)  \t[ 51 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:22:22,806] Trial 14 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.350396 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.867008 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.2818735930426368  \tBatch training l2_norm:  tensor(0.1090, grad_fn=<NormBackward1>)  \t[ 87 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:22:23,247] Trial 15 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.264531 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.80%,                 Avg loss: 0.853819 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.232490929402411  \tBatch training l2_norm:  tensor(0.9641, grad_fn=<NormBackward1>)  \t[ 80 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:22:23,692] Trial 16 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.191829 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.79%,                 Avg loss: 0.834212 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.369832429470438  \tBatch training l2_norm:  tensor(0.9840, grad_fn=<NormBackward1>)  \t[ 66 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:22:24,166] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.316565 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.861461 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.1942213443192569  \tBatch training l2_norm:  tensor(0.3099, grad_fn=<NormBackward1>)  \t[ 55 / 90 ]                        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:22:24,627] Trial 18 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.125506 \tEpoch training l2_norm: 0.77%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.819747 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.0159, grad_fn=<NormBackward1>)  \t[ 67 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.0529, grad_fn=<NormBackward1>)  \t[ 29 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      tensor(0.0196, grad_fn=<NormBackward1>)  \t[ 30 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.4646521699614823  \tBatch training l2_norm:  tensor(0.9650, grad_fn=<NormBackward1>)  \t[ 64 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:22:38,182] Trial 19 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3425054901412554  \tBatch training l2_norm:  tensor(1.3692, grad_fn=<NormBackward1>)  \t[ 56 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:22:38,641] Trial 20 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.246665 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.80%,                 Avg loss: 0.848610 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     ensor(1.0844, grad_fn=<NormBackward1>)  \t[ 7 / 90 ]                     tensor(0.8710, grad_fn=<NormBackward1>)  \t[ 49 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.4853844080240495  \tBatch training l2_norm:  tensor(1.3438, grad_fn=<NormBackward1>)  \t[ 62 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:22:51,986] Trial 21 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.4515412953766909  \tBatch training l2_norm:  tensor(0.5313, grad_fn=<NormBackward1>)  \t[ 55 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:22:52,420] Trial 22 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.358816 \tEpoch training l2_norm: 0.78%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869642 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.5668460167184168  \tBatch training l2_norm:  tensor(0.8731, grad_fn=<NormBackward1>)  \t[ 49 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:22:52,853] Trial 23 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.367193 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.866404 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3273679522628135  \tBatch training l2_norm:  tensor(0.7147, grad_fn=<NormBackward1>)  \t[ 88 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:22:53,285] Trial 24 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.316565 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.861461 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.32935398350279  \tBatch training l2_norm:  tensor(1.4346, grad_fn=<NormBackward1>)  \t[ 83 / 90 ]                       \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:22:53,716] Trial 25 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.290696 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.857930 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.4101892763223403  \tBatch training l2_norm:  tensor(0.7263, grad_fn=<NormBackward1>)  \t[ 78 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:22:54,167] Trial 26 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.358816 \tEpoch training l2_norm: 0.78%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869642 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       tensor(1.1212, grad_fn=<NormBackward1>)  \t[ 69 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     ensor(0.7752, grad_fn=<NormBackward1>)  \t[ 10 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.4258620624625407  \tBatch training l2_norm:  tensor(1.3555, grad_fn=<NormBackward1>)  \t[ 86 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:07,124] Trial 27 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3790656252752376  \tBatch training l2_norm:  tensor(0.8013, grad_fn=<NormBackward1>)  \t[ 79 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:07,561] Trial 28 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.333703 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.866782 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3441311914373089  \tBatch training l2_norm:  tensor(0.3499, grad_fn=<NormBackward1>)  \t[ 74 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:07,993] Trial 29 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.299417 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.860599 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3341481273665148  \tBatch training l2_norm:  tensor(0.6483, grad_fn=<NormBackward1>)  \t[ 68 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:08,442] Trial 30 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.290696 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.857930 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.4588505163274963  \tBatch training l2_norm:  tensor(0.4513, grad_fn=<NormBackward1>)  \t[ 58 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:08,879] Trial 31 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.375651 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869088 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.5145003433321036  \tBatch training l2_norm:  tensor(0.0362, grad_fn=<NormBackward1>)  \t[ 51 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:09,312] Trial 32 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.350396 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.867008 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.4880132719874382  \tBatch training l2_norm:  tensor(0.6837, grad_fn=<NormBackward1>)  \t[ 60 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:22,270] Trial 33 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.4672485685573433  \tBatch training l2_norm:  tensor(0.4065, grad_fn=<NormBackward1>)  \t[ 53 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:22,711] Trial 34 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.342211 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.864071 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.415877415239811  \tBatch training l2_norm:  tensor(1.1185, grad_fn=<NormBackward1>)  \t[ 80 / 90 ]                      tensor(1.3842, grad_fn=<NormBackward1>)  \t[ 41 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:23,195] Trial 35 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.367193 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.866404 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3710289185538012  \tBatch training l2_norm:  tensor(0.7068, grad_fn=<NormBackward1>)  \t[ 68 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:23,656] Trial 36 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.325158 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.864099 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.375651 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:24,160] Trial 37 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869088 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7422, grad_fn=<NormBackward1>)  \t[ 40 / 90 ]                     tensor(0.6861, grad_fn=<NormBackward1>)  \t[ 78 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.2008, grad_fn=<NormBackward1>)  \t[ 74 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.4853844080240495  \tBatch training l2_norm:  tensor(1.3438, grad_fn=<NormBackward1>)  \t[ 62 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:38,321] Trial 38 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.1901360981794724  \tBatch training l2_norm:  tensor(1.4702, grad_fn=<NormBackward1>)  \t[ 83 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:38,809] Trial 39 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.154240 \tEpoch training l2_norm: 0.77%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.825639 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.2789838323226341  \tBatch training l2_norm:  tensor(0.6638, grad_fn=<NormBackward1>)  \t[ 65 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:39,310] Trial 40 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.228593 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.80%,                 Avg loss: 0.843606 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       tensor(1.0474, grad_fn=<NormBackward1>)  \t[ 66 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     ensor(0.6975, grad_fn=<NormBackward1>)  \t[ 8 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     ensor(2.1782, grad_fn=<NormBackward1>)  \t[ 2 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.0463, grad_fn=<NormBackward1>)  \t[ 66 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.4810071591701772  \tBatch training l2_norm:  tensor(1.3008, grad_fn=<NormBackward1>)  \t[ 72 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:52,689] Trial 41 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.432928522608497  \tBatch training l2_norm:  tensor(0.5143, grad_fn=<NormBackward1>)  \t[ 55 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:53,165] Trial 42 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.342211 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.864071 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.4277166015859963  \tBatch training l2_norm:  tensor(1.1031, grad_fn=<NormBackward1>)  \t[ 69 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:53,705] Trial 43 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.375651 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869088 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.0714008344684618  \tBatch training l2_norm:  tensor(1.1112, grad_fn=<NormBackward1>)  \t[ 59 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:54,153] Trial 44 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.005925 \tEpoch training l2_norm: 0.77%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.800792 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.5574274136095632  \tBatch training l2_norm:  tensor(0.8729, grad_fn=<NormBackward1>)  \t[ 49 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:54,601] Trial 45 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.358816 \tEpoch training l2_norm: 0.78%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869642 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.334984645602249  \tBatch training l2_norm:  tensor(0.1439, grad_fn=<NormBackward1>)  \t[ 84 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:55,041] Trial 46 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.307922 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.858868 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3906267842689117  \tBatch training l2_norm:  tensor(0.0760, grad_fn=<NormBackward1>)  \t[ 77 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:55,480] Trial 47 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.333703 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.866782 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3117294128452028  \tBatch training l2_norm:  tensor(0.7315, grad_fn=<NormBackward1>)  \t[ 70 / 90 ]                     tensor(1.9637, grad_fn=<NormBackward1>)  \t[ 28 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:55,919] Trial 48 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.273387 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.856501 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.4591552128394445  \tBatch training l2_norm:  tensor(0.6905, grad_fn=<NormBackward1>)  \t[ 60 / 90 ]                     tensor(0.6507, grad_fn=<NormBackward1>)  \t[ 18 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:56,367] Trial 49 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.375651 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869088 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.2085641993666596  \tBatch training l2_norm:  tensor(0.5131, grad_fn=<NormBackward1>)  \t[ 53 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:56,807] Trial 50 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.106091 \tEpoch training l2_norm: 0.77%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.77%,                 Avg loss: 0.816075 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.4118241330697423  \tBatch training l2_norm:  tensor(0.1641, grad_fn=<NormBackward1>)  \t[ 84 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:23:57,275] Trial 51 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.383639 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.872057 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      tensor(1.3438, grad_fn=<NormBackward1>)  \t[ 62 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.6423, grad_fn=<NormBackward1>)  \t[ 32 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.4271, grad_fn=<NormBackward1>)  \t[ 41 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.50364974249493  \tBatch training l2_norm:  tensor(0.6636, grad_fn=<NormBackward1>)  \t[ 55 / 90 ]                       \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:24:11,226] Trial 52 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.358816 \tEpoch training l2_norm: 0.78%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869642 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:24:11,662] Trial 53 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      nsor(2.1783, grad_fn=<NormBackward1>)  \t[ 2 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     ensor(2.1680, grad_fn=<NormBackward1>)  \t[ 2 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.3986031780640285  \tBatch training l2_norm:  tensor(1.1498, grad_fn=<NormBackward1>)  \t[ 90 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:24:24,655] Trial 54 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3785301083471717  \tBatch training l2_norm:  tensor(0.6448, grad_fn=<NormBackward1>)  \t[ 82 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:24:25,090] Trial 55 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.350396 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.867008 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.4565891297658284  \tBatch training l2_norm:  tensor(1.8451, grad_fn=<NormBackward1>)  \t[ 75 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:24:25,528] Trial 56 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.375651 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869088 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3710289185538012  \tBatch training l2_norm:  tensor(0.7068, grad_fn=<NormBackward1>)  \t[ 68 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:24:25,962] Trial 57 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.325158 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.864099 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.4500426387992398  \tBatch training l2_norm:  tensor(0.4578, grad_fn=<NormBackward1>)  \t[ 58 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:24:26,416] Trial 58 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.367193 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.866404 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.5050981529787475  \tBatch training l2_norm:  tensor(0.0464, grad_fn=<NormBackward1>)  \t[ 51 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:24:26,852] Trial 59 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.342211 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.864071 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.399730535759323  \tBatch training l2_norm:  tensor(0.1041, grad_fn=<NormBackward1>)  \t[ 87 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:24:27,290] Trial 60 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.383639 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.872057 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7000, grad_fn=<NormBackward1>)  \t[ 76 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.0390, grad_fn=<NormBackward1>)  \t[ 14 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.4298088059919636  \tBatch training l2_norm:  tensor(0.6229, grad_fn=<NormBackward1>)  \t[ 82 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:24:40,497] Trial 61 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.416131284994048  \tBatch training l2_norm:  tensor(0.2839, grad_fn=<NormBackward1>)  \t[ 74 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:24:40,937] Trial 62 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.367193 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.866404 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      tensor(0.2185, grad_fn=<NormBackward1>)  \t[ 12 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.9649, grad_fn=<NormBackward1>)  \t[ 64 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      tensor(1.4026, grad_fn=<NormBackward1>)  \t[ 19 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.4168993862236248  \tBatch training l2_norm:  tensor(0.3861, grad_fn=<NormBackward1>)  \t[ 85 / 90 ]                     tensor(0.1423, grad_fn=<NormBackward1>)  \t[ 43 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:24:54,329] Trial 63 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.358816 \tEpoch training l2_norm: 0.78%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:24:54,785] Trial 64 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869642 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.427239791322977  \tBatch training l2_norm:  tensor(0.7180, grad_fn=<NormBackward1>)  \t[ 78 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:24:55,253] Trial 65 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.375651 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869088 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.399730535759323  \tBatch training l2_norm:  tensor(0.1041, grad_fn=<NormBackward1>)  \t[ 87 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:24:55,714] Trial 66 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.383639 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.872057 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.398421360552311  \tBatch training l2_norm:  tensor(1.1031, grad_fn=<NormBackward1>)  \t[ 80 / 90 ]                      tensor(1.3662, grad_fn=<NormBackward1>)  \t[ 72 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:24:56,180] Trial 67 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.350396 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.867008 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3500773661276873  \tBatch training l2_norm:  tensor(0.3183, grad_fn=<NormBackward1>)  \t[ 85 / 90 ]                     tensor(0.8264, grad_fn=<NormBackward1>)  \t[ 25 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:24:56,648] Trial 68 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.333703 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.866782 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.046611 \tEpoch training l2_norm: 0.77%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:24:57,122] Trial 69 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.806290 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.1976786207939898  \tBatch training l2_norm:  tensor(0.1103, grad_fn=<NormBackward1>)  \t[ 84 / 90 ]                     tensor(0.7821, grad_fn=<NormBackward1>)  \t[ 61 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:24:57,583] Trial 70 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.173138 \tEpoch training l2_norm: 0.77%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.79%,                 Avg loss: 0.829823 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     ensor(0.7675, grad_fn=<NormBackward1>)  \t[ 10 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7742, grad_fn=<NormBackward1>)  \t[ 23 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.0928, grad_fn=<NormBackward1>)  \t[ 89 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7537, grad_fn=<NormBackward1>)  \t[ 50 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.9414, grad_fn=<NormBackward1>)  \t[ 31 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.4168993862236248  \tBatch training l2_norm:  tensor(0.3861, grad_fn=<NormBackward1>)  \t[ 85 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:25:11,533] Trial 71 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.383639 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.3924, grad_fn=<NormBackward1>)  \t[ 35 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:25:11,974] Trial 72 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.872057 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.4074049505842738  \tBatch training l2_norm:  tensor(1.4132, grad_fn=<NormBackward1>)  \t[ 83 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:25:12,421] Trial 73 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.367193 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.866404 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.0137, grad_fn=<NormBackward1>)  \t[ 22 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7672, grad_fn=<NormBackward1>)  \t[ 88 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.3621, grad_fn=<NormBackward1>)  \t[ 58 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.0028, grad_fn=<NormBackward1>)  \t[ 54 / 90 ]                     tensor(0.7678, grad_fn=<NormBackward1>)  \t[ 88 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     ensor(2.1680, grad_fn=<NormBackward1>)  \t[ 2 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.8136, grad_fn=<NormBackward1>)  \t[ 79 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.2483, grad_fn=<NormBackward1>)  \t[ 16 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:25:26,852] Trial 74 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.4342038201821314  \tBatch training l2_norm:  tensor(0.0745, grad_fn=<NormBackward1>)  \t[ 77 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:25:27,324] Trial 75 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.375651 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869088 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3749857839496655  \tBatch training l2_norm:  tensor(0.1041, grad_fn=<NormBackward1>)  \t[ 87 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:25:27,803] Trial 76 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.358816 \tEpoch training l2_norm: 0.78%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869642 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3822584477033508  \tBatch training l2_norm:  tensor(0.0784, grad_fn=<NormBackward1>)  \t[ 89 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:25:28,281] Trial 77 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.383639 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.872057 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.416131284994048  \tBatch training l2_norm:  tensor(0.2839, grad_fn=<NormBackward1>)  \t[ 74 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:25:28,759] Trial 78 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.367193 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.866404 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.358495414599605  \tBatch training l2_norm:  tensor(0.1050, grad_fn=<NormBackward1>)  \t[ 87 / 90 ]                      tensor(0.7460, grad_fn=<NormBackward1>)  \t[ 50 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:25:29,225] Trial 79 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.342211 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.864071 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.240131949616033  \tBatch training l2_norm:  tensor(1.5273, grad_fn=<NormBackward1>)  \t[ 86 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:25:29,725] Trial 80 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.210314 \tEpoch training l2_norm: 0.78%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.79%,                 Avg loss: 0.838807 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      ensor(0.9346, grad_fn=<NormBackward1>)  \t[ 6 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.1816, grad_fn=<NormBackward1>)  \t[ 80 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.0021, grad_fn=<NormBackward1>)  \t[ 54 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.8752, grad_fn=<NormBackward1>)  \t[ 13 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.3643, grad_fn=<NormBackward1>)  \t[ 56 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     ensor(0.9358, grad_fn=<NormBackward1>)  \t[ 6 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.2105, grad_fn=<NormBackward1>)  \t[ 27 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.413455279394128  \tBatch training l2_norm:  tensor(0.1043, grad_fn=<NormBackward1>)  \t[ 87 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:25:43,748] Trial 81 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.4235581798212869  \tBatch training l2_norm:  tensor(0.8548, grad_fn=<NormBackward1>)  \t[ 70 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:25:44,234] Trial 82 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.375651 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869088 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3940238864584402  \tBatch training l2_norm:  tensor(0.7421, grad_fn=<NormBackward1>)  \t[ 88 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:25:44,714] Trial 83 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.383639 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.872057 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3859197195665336  \tBatch training l2_norm:  tensor(0.1328, grad_fn=<NormBackward1>)  \t[ 81 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:25:45,199] Trial 84 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.350396 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.867008 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.9481, grad_fn=<NormBackward1>)  \t[ 46 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.6835, grad_fn=<NormBackward1>)  \t[ 60 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7037, grad_fn=<NormBackward1>)  \t[ 33 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.5801, grad_fn=<NormBackward1>)  \t[ 34 / 90 ]                     tensor(0.1302, grad_fn=<NormBackward1>)  \t[ 81 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.5083, grad_fn=<NormBackward1>)  \t[ 71 / 90 ]                     tensor(1.3009, grad_fn=<NormBackward1>)  \t[ 72 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.9711, grad_fn=<NormBackward1>)  \t[ 44 / 90 ]                     tensor(1.3441, grad_fn=<NormBackward1>)  \t[ 62 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.2225, grad_fn=<NormBackward1>)  \t[ 12 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      tensor(0.0205, grad_fn=<NormBackward1>)  \t[ 48 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.9318, grad_fn=<NormBackward1>)  \t[ 25 / 90 ]                     tensor(0.3037, grad_fn=<NormBackward1>)  \t[ 52 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.1422, grad_fn=<NormBackward1>)  \t[ 43 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.0525, grad_fn=<NormBackward1>)  \t[ 29 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7736, grad_fn=<NormBackward1>)  \t[ 23 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:25:59,250] Trial 85 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3951458878988443  \tBatch training l2_norm:  tensor(1.4081, grad_fn=<NormBackward1>)  \t[ 86 / 90 ]                     tensor(0.5448, grad_fn=<NormBackward1>)  \t[ 32 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:25:59,719] Trial 86 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.367193 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.866404 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3692828958684748  \tBatch training l2_norm:  tensor(0.7372, grad_fn=<NormBackward1>)  \t[ 88 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:26:00,244] Trial 87 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.358816 \tEpoch training l2_norm: 0.78%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869642 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3822584477033508  \tBatch training l2_norm:  tensor(0.0784, grad_fn=<NormBackward1>)  \t[ 89 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:26:00,726] Trial 88 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.383639 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.872057 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.4036738037369971  \tBatch training l2_norm:  tensor(1.4118, grad_fn=<NormBackward1>)  \t[ 86 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:26:01,225] Trial 89 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.375651 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869088 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.316565 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                       \tBatch training l2_norm:  tensor(0.9840, grad_fn=<NormBackward1>)  \t[ 66 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:26:01,737] Trial 90 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.861461 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.4850, grad_fn=<NormBackward1>)  \t[ 45 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.3076, grad_fn=<NormBackward1>)  \t[ 35 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.8032, grad_fn=<NormBackward1>)  \t[ 21 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     nsor(1.7436, grad_fn=<NormBackward1>)  \t[ 3 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.5460, grad_fn=<NormBackward1>)  \t[ 17 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7539, grad_fn=<NormBackward1>)  \t[ 50 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.9688, grad_fn=<NormBackward1>)  \t[ 26 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.4270, grad_fn=<NormBackward1>)  \t[ 41 / 90 ]                     tensor(0.1294, grad_fn=<NormBackward1>)  \t[ 63 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     ensor(0.6297, grad_fn=<NormBackward1>)  \t[ 11 / 90 ]                     tensor(0.0028, grad_fn=<NormBackward1>)  \t[ 54 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     nsor(2.1680, grad_fn=<NormBackward1>)  \t[ 2 / 90 ]                     tensor(0.6862, grad_fn=<NormBackward1>)  \t[ 78 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.407866899262775  \tBatch training l2_norm:  tensor(0.7687, grad_fn=<NormBackward1>)  \t[ 88 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:26:15,515] Trial 91 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.5932, grad_fn=<NormBackward1>)  \t[ 61 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.0170, grad_fn=<NormBackward1>)  \t[ 67 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7176, grad_fn=<NormBackward1>)  \t[ 76 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.3644, grad_fn=<NormBackward1>)  \t[ 58 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.6426, grad_fn=<NormBackward1>)  \t[ 32 / 90 ]                     tensor(1.1212, grad_fn=<NormBackward1>)  \t[ 69 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     nsor(1.7438, grad_fn=<NormBackward1>)  \t[ 3 / 90 ]                     tensor(0.1302, grad_fn=<NormBackward1>)  \t[ 81 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.0463, grad_fn=<NormBackward1>)  \t[ 66 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.0036, grad_fn=<NormBackward1>)  \t[ 47 / 90 ]                     tensor(1.0165, grad_fn=<NormBackward1>)  \t[ 67 / 90 ]                     tensor(1.5080, grad_fn=<NormBackward1>)  \t[ 71 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.1294, grad_fn=<NormBackward1>)  \t[ 63 / 90 ]                     tensor(0.3861, grad_fn=<NormBackward1>)  \t[ 85 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.3335, grad_fn=<NormBackward1>)  \t[ 59 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.452607939640681  \tBatch training l2_norm:  tensor(0.6851, grad_fn=<NormBackward1>)  \t[ 78 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:26:29,708] Trial 92 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.399730535759323  \tBatch training l2_norm:  tensor(0.1041, grad_fn=<NormBackward1>)  \t[ 87 / 90 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:26:30,168] Trial 93 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.383639 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.872057 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3658324111713451  \tBatch training l2_norm:  tensor(0.0781, grad_fn=<NormBackward1>)  \t[ 89 / 90 ]                     tensor(0.6436, grad_fn=<NormBackward1>)  \t[ 34 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:26:30,641] Trial 94 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.367193 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.866404 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.3781865820998238  \tBatch training l2_norm:  tensor(1.4179, grad_fn=<NormBackward1>)  \t[ 83 / 90 ]                     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:26:31,098] Trial 95 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.350396 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.867008 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.424913628681286  \tBatch training l2_norm:  tensor(0.2756, grad_fn=<NormBackward1>)  \t[ 74 / 90 ]                      tensor(0.8328, grad_fn=<NormBackward1>)  \t[ 13 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:26:31,555] Trial 96 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.375651 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.869088 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.391823 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.0387, grad_fn=<NormBackward1>)  \t[ 22 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.874871 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.878183 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.396749 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.397621 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      tensor(0.6831, grad_fn=<NormBackward1>)  \t[ 60 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.886346 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398053 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398010 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398006 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398606 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      tensor(0.3076, grad_fn=<NormBackward1>)  \t[ 35 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.5078, grad_fn=<NormBackward1>)  \t[ 71 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884445 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398024 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890809 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     ensor(1.0897, grad_fn=<NormBackward1>)  \t[ 7 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885174 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398070 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891076 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885387 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891278 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885582 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884498 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398023 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890860 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398604 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885223 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398068 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398611 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885433 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398062 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884354 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398028 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(2.0041, grad_fn=<NormBackward1>)  \t[ 28 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890720 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885088 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398073 \tEpoch training l2_norm: 0.81%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890992 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398610 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398066 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7036, grad_fn=<NormBackward1>)  \t[ 33 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.891199 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398612 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885505 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398063 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.884424 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 1.398025 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.83%,                 Avg loss: 0.890788 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  1.4509540654718875  \tBatch training l2_norm:  tensor(0.8136, grad_fn=<NormBackward1>)  \t[ 79 / 90 ]                     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:26:45,572] Trial 97 finished with value: 0.8229174558994767 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}. Best is trial 1 with value: 0.8229174558994767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.398603 \tEpoch training l2_norm: 0.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.885154 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'lamda': 0.5, 'p': 1}\n",
      "Best Validation accuracy: tensor(0.8262, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.383639 \tEpoch training l2_norm: 0.79%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:26:46,032] Trial 98 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " l2_norm: 0.82%,                 Avg loss: 0.872057 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  1.4033048542929285  \tBatch training l2_norm:  tensor(0.1331, grad_fn=<NormBackward1>)  \t[ 81 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:26:46,515] Trial 99 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.367193 \tEpoch training l2_norm: 0.78%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.81%,                 Avg loss: 0.866404 \n",
      "\n",
      "Study statistics: \n",
      "Number of pruned trials:  76\n",
      "Number of complete trials:  24\n",
      "******************** BEST TRIAL: ********************\n",
      "Metric Value for best trial:  0.8229174558994767\n",
      "Parameters Values for best trial:  {'optimizer': 'SGD', 'lr': 0.01, 'lamda': 0.5, 'p': 1, 'regularizer': 'TihonovRegularizer'}\n",
      "DateTime start of the best trial:  2023-11-30 16:20:57.500038\n"
     ]
    }
   ],
   "source": [
    "# starting the HPO\n",
    "search.start(\n",
    "    [SGD],\n",
    "    30,\n",
    "    False,\n",
    "    optimizers_params,\n",
    "    dataloaders_params,\n",
    "    models_hyperparams,\n",
    "    regularization_params=regularization_params,\n",
    "    n_accumulated_grads=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97396e8",
   "metadata": {},
   "source": [
    "### Optimization - Custom regularizer\n",
    "Next we do the same thing for our custom regularizer that we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d99c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = HyperParameterOptimization(pipe, \"accuracy\", 20, best_not_last=True)\n",
    "search.regularize=True\n",
    "search.store_pickle = True\n",
    "reg=ElasticNet\n",
    "optimizers_params = {\"lr\": [0.01]}\n",
    "dataloaders_params = {}\n",
    "models_hyperparams = {}\n",
    "regularization_params={'reg':[reg], 'lambda1':[0.15,0.85,0.01],'lambda2':[0.0001,0.1,0.01]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2e88f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:26:47,447] A new study created in memory with name: NVLYHM6KRTOCXV6N432M\n",
      "/home/mehdi/Documents/giotto-deep/venv/lib/python3.8/site-packages/optuna/distributions.py:685: UserWarning:\n",
      "\n",
      "The distribution is specified by [0.0001, 0.1] and step=0.01, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.0901].\n",
      "\n",
      "/home/mehdi/Documents/giotto-deep/gdeep/search/hpo.py:260: UserWarning:\n",
      "\n",
      "Model cannot be re-initialized. Using existing one.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Batch training loss:  1.2909491114504634  \tBatch training l2_norm:  tensor(0.7543, grad_fn=<NormBackward1>)  \t[ 25 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehdi/Documents/giotto-deep/gdeep/trainer/trainer.py:665: UserWarning:\n",
      "\n",
      "Cannot store data in the PR curve\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  0.9383707710911277  \tBatch training l2_norm:  tensor(1.0420, grad_fn=<NormBackward1>)  \t[ 69 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:27:00,359] Trial 0 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.19, 'lambda2': 0.0201, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.19, 'lambda2': 0.0201}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.0702, grad_fn=<NormBackward1>)  \t[ 22 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.4034, grad_fn=<NormBackward1>)  \t[ 39 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  0.9393534617620285  \tBatch training l2_norm:  tensor(0.1265, grad_fn=<NormBackward1>)  \t[ 87 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:27:13,080] Trial 1 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.79, 'lambda2': 0.07010000000000001, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.79, 'lambda2': 0.07010000000000001}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.4865, grad_fn=<NormBackward1>)  \t[ 83 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7675, grad_fn=<NormBackward1>)  \t[ 80 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  0.9523200406325766  \tBatch training l2_norm:  tensor(0.5270, grad_fn=<NormBackward1>)  \t[ 65 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:27:25,771] Trial 2 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.8, 'lambda2': 0.0201, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.8, 'lambda2': 0.0201}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.1358, grad_fn=<NormBackward1>)  \t[ 81 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     ensor(0.4398, grad_fn=<NormBackward1>)  \t[ 10 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  0.9213395500509853  \tBatch training l2_norm:  tensor(0.0360, grad_fn=<NormBackward1>)  \t[ 89 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:27:38,469] Trial 3 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.67, 'lambda2': 0.0301, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.67, 'lambda2': 0.0301}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     ensor(0.2106, grad_fn=<NormBackward1>)  \t[ 5 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(2.0030, grad_fn=<NormBackward1>)  \t[ 75 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  0.9482868923232397  \tBatch training l2_norm:  tensor(0.8283, grad_fn=<NormBackward1>)  \t[ 66 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:27:51,160] Trial 4 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.38, 'lambda2': 0.0601, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.38, 'lambda2': 0.0601}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.5253, grad_fn=<NormBackward1>)  \t[ 65 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  0.9262823300648426  \tBatch training l2_norm:  tensor(0.0586, grad_fn=<NormBackward1>)  \t[ 85 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:28:03,843] Trial 5 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.22999999999999998, 'lambda2': 0.07010000000000001, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.22999999999999998, 'lambda2': 0.07010000000000001}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.1463, grad_fn=<NormBackward1>)  \t[ 16 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.5637, grad_fn=<NormBackward1>)  \t[ 35 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  0.9605804222640776  \tBatch training l2_norm:  tensor(1.5875, grad_fn=<NormBackward1>)  \t[ 72 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:28:16,486] Trial 6 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.54, 'lambda2': 0.050100000000000006, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.54, 'lambda2': 0.050100000000000006}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  0.9523200406325766  \tBatch training l2_norm:  tensor(0.5270, grad_fn=<NormBackward1>)  \t[ 65 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:28:29,330] Trial 7 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.27, 'lambda2': 0.050100000000000006, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.27, 'lambda2': 0.050100000000000006}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.1106, grad_fn=<NormBackward1>)  \t[ 54 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.6021, grad_fn=<NormBackward1>)  \t[ 53 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  0.9386162090789146  \tBatch training l2_norm:  tensor(1.3325, grad_fn=<NormBackward1>)  \t[ 71 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:28:42,053] Trial 8 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.56, 'lambda2': 0.0801, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.56, 'lambda2': 0.0801}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  0.9362031334577992  \tBatch training l2_norm:  tensor(0.0705, grad_fn=<NormBackward1>)  \t[ 68 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:28:54,875] Trial 9 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.19, 'lambda2': 0.0001, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.19, 'lambda2': 0.0001}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.3016, grad_fn=<NormBackward1>)  \t[ 41 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7577, grad_fn=<NormBackward1>)  \t[ 34 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.3260, grad_fn=<NormBackward1>)  \t[ 71 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.8878, grad_fn=<NormBackward1>)  \t[ 29 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.3442, grad_fn=<NormBackward1>)  \t[ 70 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.2604, grad_fn=<NormBackward1>)  \t[ 20 / 90 ]                     tensor(0.0244, grad_fn=<NormBackward1>)  \t[ 36 / 90 ]                     tensor(0.0883, grad_fn=<NormBackward1>)  \t[ 77 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      tensor(0.8393, grad_fn=<NormBackward1>)  \t[ 27 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  0.9485254987551135  \tBatch training l2_norm:  tensor(1.5272, grad_fn=<NormBackward1>)  \t[ 83 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:29:07,945] Trial 10 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.39, 'lambda2': 0.0001, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.39, 'lambda2': 0.0001}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.8165, grad_fn=<NormBackward1>)  \t[ 13 / 90 ]                     tensor(0.7303, grad_fn=<NormBackward1>)  \t[ 60 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7562, grad_fn=<NormBackward1>)  \t[ 64 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.4343, grad_fn=<NormBackward1>)  \t[ 63 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7570, grad_fn=<NormBackward1>)  \t[ 34 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  0.9372686704702486  \tBatch training l2_norm:  tensor(0.0543, grad_fn=<NormBackward1>)  \t[ 84 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:29:21,628] Trial 11 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.77, 'lambda2': 0.0901, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.77, 'lambda2': 0.0901}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.4414, grad_fn=<NormBackward1>)  \t[ 19 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.1400, grad_fn=<NormBackward1>)  \t[ 81 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.1317, grad_fn=<NormBackward1>)  \t[ 55 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.4338, grad_fn=<NormBackward1>)  \t[ 63 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.0611, grad_fn=<NormBackward1>)  \t[ 85 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.1120, grad_fn=<NormBackward1>)  \t[ 54 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.5231, grad_fn=<NormBackward1>)  \t[ 65 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.4841, grad_fn=<NormBackward1>)  \t[ 23 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.6003, grad_fn=<NormBackward1>)  \t[ 53 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.4009, grad_fn=<NormBackward1>)  \t[ 39 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     nsor(1.8576, grad_fn=<NormBackward1>)  \t[ 2 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.6900000000000001, 'lambda2': 0.0301}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:29:34,965] Trial 12 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.6900000000000001, 'lambda2': 0.0301, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     ensor(0.2697, grad_fn=<NormBackward1>)  \t[ 4 / 90 ]                     tensor(0.8329, grad_fn=<NormBackward1>)  \t[ 78 / 90 ]                     tensor(1.6350, grad_fn=<NormBackward1>)  \t[ 86 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.2229, grad_fn=<NormBackward1>)  \t[ 73 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.2305, grad_fn=<NormBackward1>)  \t[ 73 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.2326, grad_fn=<NormBackward1>)  \t[ 73 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.3104, grad_fn=<NormBackward1>)  \t[ 71 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.8732, grad_fn=<NormBackward1>)  \t[ 49 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        rd1>)  \t[ 89 / 90 ]                       tensor(0.7027, grad_fn=<NormBackward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.5257, grad_fn=<NormBackward1>)  \t[ 88 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  0.9523907636131655  \tBatch training l2_norm:  tensor(0.7483, grad_fn=<NormBackward1>)  \t[ 79 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:29:47,879] Trial 13 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.38, 'lambda2': 0.0201, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.38, 'lambda2': 0.0201}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     ensor(1.2662, grad_fn=<NormBackward1>)  \t[ 10 / 90 ]                     tensor(0.4765, grad_fn=<NormBackward1>)  \t[ 15 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.8368, grad_fn=<NormBackward1>)  \t[ 66 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7509, grad_fn=<NormBackward1>)  \t[ 40 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.8721, grad_fn=<NormBackward1>)  \t[ 57 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.9524, grad_fn=<NormBackward1>)  \t[ 31 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:30:00,806] Trial 14 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.65, 'lambda2': 0.07010000000000001, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.65, 'lambda2': 0.07010000000000001}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     ensor(0.2697, grad_fn=<NormBackward1>)  \t[ 4 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.6031, grad_fn=<NormBackward1>)  \t[ 53 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7588, grad_fn=<NormBackward1>)  \t[ 34 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7578, grad_fn=<NormBackward1>)  \t[ 37 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.3428, grad_fn=<NormBackward1>)  \t[ 70 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     ensor(0.4420, grad_fn=<NormBackward1>)  \t[ 10 / 90 ]                     tensor(0.1573, grad_fn=<NormBackward1>)  \t[ 16 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        rd1>)  \t[ 89 / 90 ]                       nsor(0.7078, grad_fn=<NormBackward1>)  \t[ 4 / 90 ]                     tensor(0.1204, grad_fn=<NormBackward1>)  \t[ 87 / 90 ]                     tensor(0.7027, grad_fn=<NormBackward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7585, grad_fn=<NormBackward1>)  \t[ 82 / 90 ]                     tensor(0.0606, grad_fn=<NormBackward1>)  \t[ 85 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.1186, grad_fn=<NormBackward1>)  \t[ 55 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.85, 'lambda2': 0.040100000000000004}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:30:13,845] Trial 15 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.85, 'lambda2': 0.040100000000000004, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.1390, grad_fn=<NormBackward1>)  \t[ 43 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.1419, grad_fn=<NormBackward1>)  \t[ 16 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.0991, grad_fn=<NormBackward1>)  \t[ 52 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.1322, grad_fn=<NormBackward1>)  \t[ 81 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.0683, grad_fn=<NormBackward1>)  \t[ 47 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.0220, grad_fn=<NormBackward1>)  \t[ 36 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     ensor(0.9896, grad_fn=<NormBackward1>)  \t[ 6 / 90 ]                     tensor(0.7637, grad_fn=<NormBackward1>)  \t[ 82 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  0.9574217689693726  \tBatch training l2_norm:  tensor(0.9217, grad_fn=<NormBackward1>)  \t[ 78 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:30:26,958] Trial 16 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.47, 'lambda2': 0.0101, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.47, 'lambda2': 0.0101}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.1316, grad_fn=<NormBackward1>)  \t[ 12 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(2.0021, grad_fn=<NormBackward1>)  \t[ 75 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7271, grad_fn=<NormBackward1>)  \t[ 50 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.9524, grad_fn=<NormBackward1>)  \t[ 31 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  0.9588239394580377  \tBatch training l2_norm:  tensor(0.0877, grad_fn=<NormBackward1>)  \t[ 77 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:30:40,608] Trial 17 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.15, 'lambda2': 0.0901, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.15, 'lambda2': 0.0901}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.1166, grad_fn=<NormBackward1>)  \t[ 14 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     ensor(3.3369, grad_fn=<NormBackward1>)  \t[ 9 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.4072, grad_fn=<NormBackward1>)  \t[ 62 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.3, 'lambda2': 0.0601}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:30:54,168] Trial 18 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.3, 'lambda2': 0.0601, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 1.133319 \tEpoch training l2_norm: 0.85%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.78%,                 Avg loss: 0.839323 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.925486 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.823773 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.918219 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.821130 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917620 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.819256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917497 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.817499 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917417 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.815804 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917344 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.76%,                 Avg loss: 0.814168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917277 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.812586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917215 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.4094, grad_fn=<NormBackward1>)  \t[ 62 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.811058 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917157 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.809581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917102 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.808153 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917052 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.806774 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.917005 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.805440 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916961 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.804151 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916920 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.802905 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916882 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.801701 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916847 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.800537 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916814 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.799411 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916784 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.798323 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916755 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.5165, grad_fn=<NormBackward1>)  \t[ 18 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.797271 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916729 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.796254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916705 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.795270 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916682 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.794319 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916661 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.793400 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916642 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.75%,                 Avg loss: 0.792511 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916623 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.791651 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916607 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790819 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916591 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     tensor(0.7269, grad_fn=<NormBackward1>)  \t[ 50 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.790015 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.916577 \tEpoch training l2_norm: 0.76%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.789238 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch training loss:  0.9474429883404809  \tBatch training l2_norm:  tensor(0.7460, grad_fn=<NormBackward1>)  \t[ 80 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-30 16:31:07,455] Trial 19 finished with value: 0.7433606476054183 and parameters: {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.48, 'lambda2': 0.040100000000000004, 'reg': 'ElasticNet'}. Best is trial 0 with value: 0.7433606476054183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.916563 \tEpoch training l2_norm: 0.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " l2_norm: 0.74%,                 Avg loss: 0.788486 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Net \n",
      "Model Hyperparameters: {}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.01}\n",
      "Dataloader parameters: {}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: {'reg': <class '__main__.ElasticNet'>, 'lambda1': 0.48, 'lambda2': 0.040100000000000004}\n",
      "Best Validation accuracy: tensor(2.9048, dtype=torch.float64)\n",
      "Study statistics: \n",
      "Number of pruned trials:  0\n",
      "Number of complete trials:  20\n",
      "******************** BEST TRIAL: ********************\n",
      "Metric Value for best trial:  0.7433606476054183\n",
      "Parameters Values for best trial:  {'optimizer': 'SGD', 'lr': 0.01, 'lambda1': 0.19, 'lambda2': 0.0201, 'reg': 'ElasticNet'}\n",
      "DateTime start of the best trial:  2023-11-30 16:26:47.448030\n"
     ]
    }
   ],
   "source": [
    "search.start(\n",
    "    [SGD],\n",
    "    30,\n",
    "    False,\n",
    "    optimizers_params,\n",
    "    dataloaders_params,\n",
    "    models_hyperparams,\n",
    "    regularization_params=regularization_params,\n",
    "    n_accumulated_grads=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa0920",
   "metadata": {},
   "source": [
    "## 3. Ad hoc regularizers\n",
    "\n",
    "The penalties in the regularizers that we have seen so far have been straightforward functions of the model parameters. Here we show an example of an ad hoc regularizer that directly penalizes the behavior of the model. Such regularizers may depend on external parameters, and the logic is completely wrapped in the regularizer object.\n",
    "\n",
    "First we generate some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88a5c63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6c6ffc5a30>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "S=1000\n",
    "X=np.linspace(0,2*np.pi,S)\n",
    "y=3*np.sin(X)+0.5*rng.standard_normal(S)\n",
    "plt.plot(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae443441",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(X, y, test_size=0.1)\n",
    "tensor_x_t = torch.Tensor(train_x).reshape(-1, 1)\n",
    "tensor_x_t=tensor_x_t.float()\n",
    "tensor_y_t = torch.from_numpy(train_y).reshape(-1, 1)\n",
    "tensor_y_t=tensor_y_t.float()\n",
    "tensor_x_v = torch.Tensor(val_x).reshape(-1, 1)\n",
    "tensor_y_v = torch.from_numpy(val_y).reshape(-1, 1)\n",
    "train_dataset = TensorDataset(tensor_x_t,tensor_y_t)\n",
    "dl_tr = DataLoader(train_dataset,batch_size=10)\n",
    "val_dataset = TensorDataset(tensor_x_v,tensor_y_v)\n",
    "dl_val = DataLoader(val_dataset,batch_size=10)\n",
    "\n",
    "class model1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model1, self).__init__()\n",
    "        self.seqmodel = FFNet(arch=[1, 10,10,10, 1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seqmodel(x)\n",
    "\n",
    "\n",
    "model = model1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9803b7",
   "metadata": {},
   "source": [
    "We fit a basic unregularized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e25752e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 3.141686 \tEpoch training l2_norm: 5.23%                                        ward1>)  \t[ 90 / 90 ]                      ensor(2.7563, grad_fn=<NormBackward1>)  \t[ 38 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 5.98%,                 Avg loss: 3.700457 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Batch training loss:  1.9925980667273204  \tBatch training l2_norm:  tensor(5.9180, grad_fn=<NormBackward1>)  \t[ 24 / 90 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehdi/Documents/giotto-deep/gdeep/trainer/trainer.py:665: UserWarning:\n",
      "\n",
      "Cannot store data in the PR curve\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.743931 \tEpoch training l2_norm: 3.98%                                        rd1>)  \t[ 90 / 90 ]                       \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 4.52%,                 Avg loss: 2.169215 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 1.199008 \tEpoch training l2_norm: 3.33%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.8068, grad_fn=<NormBackward1>)  \t[ 51 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 3.23%,                 Avg loss: 1.088290 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 1.013476 \tEpoch training l2_norm: 3.07%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 3.29%,                 Avg loss: 1.106831 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.874769 \tEpoch training l2_norm: 2.85%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 3.07%,                 Avg loss: 0.969470 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.888001 \tEpoch training l2_norm: 2.89%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.78%,                 Avg loss: 0.801410 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.891198 \tEpoch training l2_norm: 2.87%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.92%,                 Avg loss: 0.887762 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.809946 \tEpoch training l2_norm: 2.76%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.98%,                 Avg loss: 0.925794 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.785424 \tEpoch training l2_norm: 2.69%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.82%,                 Avg loss: 0.816467 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.834194 \tEpoch training l2_norm: 2.81%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.46%,                 Avg loss: 0.629142 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.799383 \tEpoch training l2_norm: 2.73%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.82%,                 Avg loss: 0.826367 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.827849 \tEpoch training l2_norm: 2.79%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.82%,                 Avg loss: 0.819329 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.793469 \tEpoch training l2_norm: 2.73%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.67%,                 Avg loss: 0.735993 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.817634 \tEpoch training l2_norm: 2.77%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.56%,                 Avg loss: 0.686998 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.722600 \tEpoch training l2_norm: 2.60%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.71%,                 Avg loss: 0.768648 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.754578 \tEpoch training l2_norm: 2.66%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.58%,                 Avg loss: 0.689969 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.719042 \tEpoch training l2_norm: 2.61%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.57%,                 Avg loss: 0.690157 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.703890 \tEpoch training l2_norm: 2.56%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 3.11%,                 Avg loss: 0.997577 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.750115 \tEpoch training l2_norm: 2.65%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.42%,                 Avg loss: 0.611195 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.649170 \tEpoch training l2_norm: 2.47%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 3.04%,                 Avg loss: 0.950202 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.740259 \tEpoch training l2_norm: 2.63%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.59%,                 Avg loss: 0.697117 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.739207 \tEpoch training l2_norm: 2.65%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.41%,                 Avg loss: 0.603999 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.743079 \tEpoch training l2_norm: 2.63%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.90%,                 Avg loss: 0.861251 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.687642 \tEpoch training l2_norm: 2.55%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.58%,                 Avg loss: 0.690303 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.697710 \tEpoch training l2_norm: 2.55%                                        ward1>)  \t[ 90 / 90 ]                     ensor(2.4040, grad_fn=<NormBackward1>)  \t[ 5 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.38%,                 Avg loss: 0.592841 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.727170 \tEpoch training l2_norm: 2.60%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.40%,                 Avg loss: 0.598515 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.715549 \tEpoch training l2_norm: 2.59%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.66%,                 Avg loss: 0.727522 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.709594 \tEpoch training l2_norm: 2.57%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.64%,                 Avg loss: 0.720221 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.691528 \tEpoch training l2_norm: 2.56%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.38%,                 Avg loss: 0.592983 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Epoch training loss: 0.687039 \tEpoch training l2_norm: 2.54%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.42%,                 Avg loss: 0.612001 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Epoch training loss: 0.657322 \tEpoch training l2_norm: 2.49%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.82%,                 Avg loss: 0.815243 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Epoch training loss: 0.693740 \tEpoch training l2_norm: 2.56%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.35%,                 Avg loss: 0.579149 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Epoch training loss: 0.688733 \tEpoch training l2_norm: 2.55%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.55%,                 Avg loss: 0.674241 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Epoch training loss: 0.690351 \tEpoch training l2_norm: 2.55%                                        ward1>)  \t[ 89 / 90 ]                     tensor(3.6569, grad_fn=<NormBackward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.36%,                 Avg loss: 0.584330 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Epoch training loss: 0.675399 \tEpoch training l2_norm: 2.52%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.64%,                 Avg loss: 0.715470 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Epoch training loss: 0.670070 \tEpoch training l2_norm: 2.51%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.25%,                 Avg loss: 0.532706 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Epoch training loss: 0.644049 \tEpoch training l2_norm: 2.46%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.55%,                 Avg loss: 0.675201 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Epoch training loss: 0.738784 \tEpoch training l2_norm: 2.65%                                        ward1>)  \t[ 90 / 90 ]                     tensor(2.4559, grad_fn=<NormBackward1>)  \t[ 83 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.42%,                 Avg loss: 0.610225 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Epoch training loss: 0.649243 \tEpoch training l2_norm: 2.47%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.64%,                 Avg loss: 0.719868 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Epoch training loss: 0.679914 \tEpoch training l2_norm: 2.53%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.45%,                 Avg loss: 0.624553 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Epoch training loss: 0.666952 \tEpoch training l2_norm: 2.50%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.28%,                 Avg loss: 0.545388 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Epoch training loss: 0.661822 \tEpoch training l2_norm: 2.49%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.18%,                 Avg loss: 0.503834 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Epoch training loss: 0.683380 \tEpoch training l2_norm: 2.54%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.26%,                 Avg loss: 0.538652 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Epoch training loss: 0.697433 \tEpoch training l2_norm: 2.57%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.24%,                 Avg loss: 0.527499 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Epoch training loss: 0.672305 \tEpoch training l2_norm: 2.50%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.37%,                 Avg loss: 0.588871 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Epoch training loss: 0.652030 \tEpoch training l2_norm: 2.48%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.30%,                 Avg loss: 0.555916 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Epoch training loss: 0.658616 \tEpoch training l2_norm: 2.49%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.21%,                 Avg loss: 0.514500 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Epoch training loss: 0.646094 \tEpoch training l2_norm: 2.46%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.30%,                 Avg loss: 0.555303 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Epoch training loss: 0.661843 \tEpoch training l2_norm: 2.50%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.21%,                 Avg loss: 0.514369 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Epoch training loss: 0.659827 \tEpoch training l2_norm: 2.49%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.19%,                 Avg loss: 0.505270 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Epoch training loss: 0.668468 \tEpoch training l2_norm: 2.50%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.24%,                 Avg loss: 0.528016 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Epoch training loss: 0.645196 \tEpoch training l2_norm: 2.46%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.27%,                 Avg loss: 0.540389 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Epoch training loss: 0.655519 \tEpoch training l2_norm: 2.49%                                        ward1>)  \t[ 90 / 90 ]                     tensor(1.9510, grad_fn=<NormBackward1>)  \t[ 81 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.36%,                 Avg loss: 0.584776 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Epoch training loss: 0.667595 \tEpoch training l2_norm: 2.50%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.24%,                 Avg loss: 0.530327 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Epoch training loss: 0.669571 \tEpoch training l2_norm: 2.50%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.21%,                 Avg loss: 0.514831 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Epoch training loss: 0.665386 \tEpoch training l2_norm: 2.50%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.31%,                 Avg loss: 0.562016 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Epoch training loss: 0.677257 \tEpoch training l2_norm: 2.51%                                        ward1>)  \t[ 90 / 90 ]                     tensor(2.5828, grad_fn=<NormBackward1>)  \t[ 49 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.36%,                 Avg loss: 0.583308 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Epoch training loss: 0.671827 \tEpoch training l2_norm: 2.51%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.28%,                 Avg loss: 0.545966 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Epoch training loss: 0.657587 \tEpoch training l2_norm: 2.48%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.30%,                 Avg loss: 0.555964 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Epoch training loss: 0.646809 \tEpoch training l2_norm: 2.47%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.25%,                 Avg loss: 0.533309 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Epoch training loss: 0.657893 \tEpoch training l2_norm: 2.48%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.20%,                 Avg loss: 0.512377 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Epoch training loss: 0.655438 \tEpoch training l2_norm: 2.48%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.28%,                 Avg loss: 0.547517 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Epoch training loss: 0.638296 \tEpoch training l2_norm: 2.45%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.25%,                 Avg loss: 0.530286 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Epoch training loss: 0.630707 \tEpoch training l2_norm: 2.43%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.47%,                 Avg loss: 0.633479 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Epoch training loss: 0.644285 \tEpoch training l2_norm: 2.46%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.27%,                 Avg loss: 0.539797 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Epoch training loss: 0.661078 \tEpoch training l2_norm: 2.50%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.28%,                 Avg loss: 0.545437 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Epoch training loss: 0.654800 \tEpoch training l2_norm: 2.47%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.27%,                 Avg loss: 0.542991 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Epoch training loss: 0.675176 \tEpoch training l2_norm: 2.51%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.32%,                 Avg loss: 0.563106 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Epoch training loss: 0.664891 \tEpoch training l2_norm: 2.51%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.23%,                 Avg loss: 0.525588 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Epoch training loss: 0.669521 \tEpoch training l2_norm: 2.50%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.27%,                 Avg loss: 0.539246 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Epoch training loss: 0.659866 \tEpoch training l2_norm: 2.49%                                        ward1>)  \t[ 90 / 90 ]                     tensor(2.6667, grad_fn=<NormBackward1>)  \t[ 61 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.55%,                 Avg loss: 0.673869 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Epoch training loss: 0.645419 \tEpoch training l2_norm: 2.46%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.15%,                 Avg loss: 0.490392 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Epoch training loss: 0.647024 \tEpoch training l2_norm: 2.46%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.17%,                 Avg loss: 0.498354 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Epoch training loss: 0.655558 \tEpoch training l2_norm: 2.47%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.32%,                 Avg loss: 0.563593 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Epoch training loss: 0.645335 \tEpoch training l2_norm: 2.45%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.42%,                 Avg loss: 0.616244 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Epoch training loss: 0.661614 \tEpoch training l2_norm: 2.50%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.38%,                 Avg loss: 0.596164 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Epoch training loss: 0.664365 \tEpoch training l2_norm: 2.49%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.38%,                 Avg loss: 0.591041 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Epoch training loss: 0.635077 \tEpoch training l2_norm: 2.44%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.32%,                 Avg loss: 0.563628 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Epoch training loss: 0.644220 \tEpoch training l2_norm: 2.46%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.47%,                 Avg loss: 0.636048 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Epoch training loss: 0.642219 \tEpoch training l2_norm: 2.46%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.22%,                 Avg loss: 0.518078 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Epoch training loss: 0.622067 \tEpoch training l2_norm: 2.42%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.24%,                 Avg loss: 0.526863 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Epoch training loss: 0.639295 \tEpoch training l2_norm: 2.45%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.24%,                 Avg loss: 0.528395 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Epoch training loss: 0.645797 \tEpoch training l2_norm: 2.46%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.25%,                 Avg loss: 0.530488 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Epoch training loss: 0.642933 \tEpoch training l2_norm: 2.46%                                        ward1>)  \t[ 90 / 90 ]                     tensor(2.4589, grad_fn=<NormBackward1>)  \t[ 54 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.31%,                 Avg loss: 0.561931 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Epoch training loss: 0.638617 \tEpoch training l2_norm: 2.44%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.29%,                 Avg loss: 0.551322 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Epoch training loss: 0.641394 \tEpoch training l2_norm: 2.45%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.31%,                 Avg loss: 0.560974 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Epoch training loss: 0.633577 \tEpoch training l2_norm: 2.44%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.22%,                 Avg loss: 0.519378 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Epoch training loss: 0.656620 \tEpoch training l2_norm: 2.48%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.23%,                 Avg loss: 0.523307 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Epoch training loss: 0.639798 \tEpoch training l2_norm: 2.46%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.26%,                 Avg loss: 0.537946 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Epoch training loss: 0.632833 \tEpoch training l2_norm: 2.43%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.30%,                 Avg loss: 0.556786 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Epoch training loss: 0.652687 \tEpoch training l2_norm: 2.48%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.47%,                 Avg loss: 0.633371 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Epoch training loss: 0.639079 \tEpoch training l2_norm: 2.45%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.26%,                 Avg loss: 0.535348 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Epoch training loss: 0.661580 \tEpoch training l2_norm: 2.49%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.24%,                 Avg loss: 0.526390 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Epoch training loss: 0.643955 \tEpoch training l2_norm: 2.47%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.24%,                 Avg loss: 0.526386 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Epoch training loss: 0.649552 \tEpoch training l2_norm: 2.47%                                        ward1>)  \t[ 90 / 90 ]                     tensor(2.7033, grad_fn=<NormBackward1>)  \t[ 41 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.33%,                 Avg loss: 0.568159 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Epoch training loss: 0.638682 \tEpoch training l2_norm: 2.44%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.22%,                 Avg loss: 0.519387 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Epoch training loss: 0.650205 \tEpoch training l2_norm: 2.47%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.27%,                 Avg loss: 0.538583 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Epoch training loss: 0.635025 \tEpoch training l2_norm: 2.43%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.31%,                 Avg loss: 0.561790 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Epoch training loss: 0.646210 \tEpoch training l2_norm: 2.47%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.33%,                 Avg loss: 0.568143 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Epoch training loss: 0.631454 \tEpoch training l2_norm: 2.43%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.34%,                 Avg loss: 0.574605 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Epoch training loss: 0.639550 \tEpoch training l2_norm: 2.44%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.22%,                 Avg loss: 0.518120 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Epoch training loss: 0.647569 \tEpoch training l2_norm: 2.46%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.23%,                 Avg loss: 0.521739 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Epoch training loss: 0.635799 \tEpoch training l2_norm: 2.45%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.34%,                 Avg loss: 0.572414 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Epoch training loss: 0.645166 \tEpoch training l2_norm: 2.46%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.29%,                 Avg loss: 0.549311 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Epoch training loss: 0.660312 \tEpoch training l2_norm: 2.48%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.24%,                 Avg loss: 0.531339 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Epoch training loss: 0.661256 \tEpoch training l2_norm: 2.49%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.27%,                 Avg loss: 0.539520 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Epoch training loss: 0.631522 \tEpoch training l2_norm: 2.43%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.28%,                 Avg loss: 0.544995 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Epoch training loss: 0.630064 \tEpoch training l2_norm: 2.43%                                        ward1>)  \t[ 90 / 90 ]                     tensor(3.2951, grad_fn=<NormBackward1>)  \t[ 24 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.30%,                 Avg loss: 0.557876 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Epoch training loss: 0.638321 \tEpoch training l2_norm: 2.44%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.20%,                 Avg loss: 0.509998 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Epoch training loss: 0.630223 \tEpoch training l2_norm: 2.43%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.33%,                 Avg loss: 0.569032 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Epoch training loss: 0.643508 \tEpoch training l2_norm: 2.45%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.41%,                 Avg loss: 0.605144 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Epoch training loss: 0.629805 \tEpoch training l2_norm: 2.43%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.24%,                 Avg loss: 0.528570 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Epoch training loss: 0.649677 \tEpoch training l2_norm: 2.46%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.22%,                 Avg loss: 0.518975 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Epoch training loss: 0.673734 \tEpoch training l2_norm: 2.50%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.48%,                 Avg loss: 0.639311 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Epoch training loss: 0.634404 \tEpoch training l2_norm: 2.44%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.27%,                 Avg loss: 0.543396 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Epoch training loss: 0.648740 \tEpoch training l2_norm: 2.46%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.33%,                 Avg loss: 0.570862 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Epoch training loss: 0.668477 \tEpoch training l2_norm: 2.51%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.53%,                 Avg loss: 0.662731 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Epoch training loss: 0.642018 \tEpoch training l2_norm: 2.45%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.31%,                 Avg loss: 0.561591 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Epoch training loss: 0.626507 \tEpoch training l2_norm: 2.42%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.24%,                 Avg loss: 0.529853 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Epoch training loss: 0.642188 \tEpoch training l2_norm: 2.45%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.22%,                 Avg loss: 0.521169 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Epoch training loss: 0.631897 \tEpoch training l2_norm: 2.43%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.37%,                 Avg loss: 0.589332 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Epoch training loss: 0.638813 \tEpoch training l2_norm: 2.44%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.66%,                 Avg loss: 0.728751 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Epoch training loss: 0.645245 \tEpoch training l2_norm: 2.45%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.34%,                 Avg loss: 0.573508 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Epoch training loss: 0.625935 \tEpoch training l2_norm: 2.42%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.28%,                 Avg loss: 0.545094 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Epoch training loss: 0.638103 \tEpoch training l2_norm: 2.44%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.50%,                 Avg loss: 0.653234 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Epoch training loss: 0.617790 \tEpoch training l2_norm: 2.40%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.30%,                 Avg loss: 0.554776 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Epoch training loss: 0.634239 \tEpoch training l2_norm: 2.43%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.38%,                 Avg loss: 0.592484 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Epoch training loss: 0.664169 \tEpoch training l2_norm: 2.50%                                        ward1>)  \t[ 90 / 90 ]                     tensor(3.1264, grad_fn=<NormBackward1>)  \t[ 31 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.40%,                 Avg loss: 0.599951 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Epoch training loss: 0.628431 \tEpoch training l2_norm: 2.42%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.42%,                 Avg loss: 0.612897 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Epoch training loss: 0.630241 \tEpoch training l2_norm: 2.42%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.46%,                 Avg loss: 0.632137 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Epoch training loss: 0.642768 \tEpoch training l2_norm: 2.46%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.38%,                 Avg loss: 0.592424 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Epoch training loss: 0.647931 \tEpoch training l2_norm: 2.46%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.31%,                 Avg loss: 0.557716 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Epoch training loss: 0.651622 \tEpoch training l2_norm: 2.48%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 3.17%,                 Avg loss: 1.037227 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Epoch training loss: 0.641257 \tEpoch training l2_norm: 2.45%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.42%,                 Avg loss: 0.609969 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Epoch training loss: 0.645007 \tEpoch training l2_norm: 2.45%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.34%,                 Avg loss: 0.575305 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Epoch training loss: 0.639465 \tEpoch training l2_norm: 2.44%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.33%,                 Avg loss: 0.571419 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Epoch training loss: 0.630690 \tEpoch training l2_norm: 2.42%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.33%,                 Avg loss: 0.572006 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Epoch training loss: 0.622629 \tEpoch training l2_norm: 2.41%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.32%,                 Avg loss: 0.568102 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Epoch training loss: 0.634056 \tEpoch training l2_norm: 2.43%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.32%,                 Avg loss: 0.567043 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Epoch training loss: 0.651679 \tEpoch training l2_norm: 2.47%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.40%,                 Avg loss: 0.602147 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Epoch training loss: 0.636586 \tEpoch training l2_norm: 2.44%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.35%,                 Avg loss: 0.583365 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Epoch training loss: 0.621562 \tEpoch training l2_norm: 2.40%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.35%,                 Avg loss: 0.580698 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Epoch training loss: 0.640581 \tEpoch training l2_norm: 2.44%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.30%,                 Avg loss: 0.555350 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Epoch training loss: 0.625339 \tEpoch training l2_norm: 2.42%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.32%,                 Avg loss: 0.568021 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Epoch training loss: 0.634224 \tEpoch training l2_norm: 2.43%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.36%,                 Avg loss: 0.587023 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Epoch training loss: 0.615975 \tEpoch training l2_norm: 2.39%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.32%,                 Avg loss: 0.566881 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Epoch training loss: 0.651610 \tEpoch training l2_norm: 2.47%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.41%,                 Avg loss: 0.607834 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Epoch training loss: 0.614034 \tEpoch training l2_norm: 2.39%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.37%,                 Avg loss: 0.591295 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Epoch training loss: 0.616596 \tEpoch training l2_norm: 2.39%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.38%,                 Avg loss: 0.593561 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Epoch training loss: 0.643427 \tEpoch training l2_norm: 2.44%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.42%,                 Avg loss: 0.609345 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Epoch training loss: 0.631133 \tEpoch training l2_norm: 2.42%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.32%,                 Avg loss: 0.566631 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Epoch training loss: 0.659894 \tEpoch training l2_norm: 2.49%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.56%,                 Avg loss: 0.678508 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Epoch training loss: 0.631746 \tEpoch training l2_norm: 2.43%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.32%,                 Avg loss: 0.568073 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Epoch training loss: 0.641317 \tEpoch training l2_norm: 2.44%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.30%,                 Avg loss: 0.557232 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Epoch training loss: 0.617156 \tEpoch training l2_norm: 2.39%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.34%,                 Avg loss: 0.572899 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Epoch training loss: 0.617226 \tEpoch training l2_norm: 2.40%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.37%,                 Avg loss: 0.591424 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Epoch training loss: 0.654376 \tEpoch training l2_norm: 2.48%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.24%,                 Avg loss: 0.526742 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Epoch training loss: 0.604496 \tEpoch training l2_norm: 2.37%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.34%,                 Avg loss: 0.579503 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Epoch training loss: 0.660171 \tEpoch training l2_norm: 2.50%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.32%,                 Avg loss: 0.564697 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Epoch training loss: 0.601790 \tEpoch training l2_norm: 2.36%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.35%,                 Avg loss: 0.580498 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Epoch training loss: 0.632540 \tEpoch training l2_norm: 2.43%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.31%,                 Avg loss: 0.561430 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Epoch training loss: 0.625548 \tEpoch training l2_norm: 2.41%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.31%,                 Avg loss: 0.562865 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Epoch training loss: 0.616326 \tEpoch training l2_norm: 2.39%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.46%,                 Avg loss: 0.632484 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Epoch training loss: 0.624840 \tEpoch training l2_norm: 2.41%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.30%,                 Avg loss: 0.555998 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Epoch training loss: 0.624001 \tEpoch training l2_norm: 2.41%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.40%,                 Avg loss: 0.605241 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Epoch training loss: 0.629341 \tEpoch training l2_norm: 2.42%                                        ward1>)  \t[ 90 / 90 ]                     tensor(2.2666, grad_fn=<NormBackward1>)  \t[ 67 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.45%,                 Avg loss: 0.631126 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Epoch training loss: 0.615657 \tEpoch training l2_norm: 2.40%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.21%,                 Avg loss: 0.517414 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Epoch training loss: 0.646218 \tEpoch training l2_norm: 2.47%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.30%,                 Avg loss: 0.556237 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Epoch training loss: 0.638078 \tEpoch training l2_norm: 2.44%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.26%,                 Avg loss: 0.539166 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Epoch training loss: 0.612532 \tEpoch training l2_norm: 2.40%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.37%,                 Avg loss: 0.586873 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Epoch training loss: 0.624298 \tEpoch training l2_norm: 2.42%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.30%,                 Avg loss: 0.556765 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "Epoch training loss: 0.612508 \tEpoch training l2_norm: 2.39%                                        ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.23%,                 Avg loss: 0.526820 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Epoch training loss: 0.621969 \tEpoch training l2_norm: 2.41%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.27%,                 Avg loss: 0.545005 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Epoch training loss: 0.625386 \tEpoch training l2_norm: 2.42%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.26%,                 Avg loss: 0.538270 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Epoch training loss: 0.628470 \tEpoch training l2_norm: 2.42%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.33%,                 Avg loss: 0.574416 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Epoch training loss: 0.614191 \tEpoch training l2_norm: 2.39%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.22%,                 Avg loss: 0.518740 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Epoch training loss: 0.624718 \tEpoch training l2_norm: 2.42%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.39%,                 Avg loss: 0.600949 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Epoch training loss: 0.632250 \tEpoch training l2_norm: 2.43%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.24%,                 Avg loss: 0.530359 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Epoch training loss: 0.614947 \tEpoch training l2_norm: 2.40%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.22%,                 Avg loss: 0.518401 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Epoch training loss: 0.630370 \tEpoch training l2_norm: 2.43%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.25%,                 Avg loss: 0.533151 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Epoch training loss: 0.625917 \tEpoch training l2_norm: 2.43%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.30%,                 Avg loss: 0.556099 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Epoch training loss: 0.621048 \tEpoch training l2_norm: 2.40%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.31%,                 Avg loss: 0.559958 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Epoch training loss: 0.626792 \tEpoch training l2_norm: 2.42%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.25%,                 Avg loss: 0.533361 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Epoch training loss: 0.627015 \tEpoch training l2_norm: 2.42%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.30%,                 Avg loss: 0.557392 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Epoch training loss: 0.626443 \tEpoch training l2_norm: 2.42%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.36%,                 Avg loss: 0.589515 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Epoch training loss: 0.613385 \tEpoch training l2_norm: 2.40%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.27%,                 Avg loss: 0.541550 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Epoch training loss: 0.602616 \tEpoch training l2_norm: 2.37%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.29%,                 Avg loss: 0.556388 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Epoch training loss: 0.611873 \tEpoch training l2_norm: 2.39%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.27%,                 Avg loss: 0.542569 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Epoch training loss: 0.622377 \tEpoch training l2_norm: 2.41%                                        ward1>)  \t[ 90 / 90 ]                     tensor(2.3944, grad_fn=<NormBackward1>)  \t[ 13 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.34%,                 Avg loss: 0.576851 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Epoch training loss: 0.610030 \tEpoch training l2_norm: 2.38%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.18%,                 Avg loss: 0.502142 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Epoch training loss: 0.625792 \tEpoch training l2_norm: 2.42%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.20%,                 Avg loss: 0.510801 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Epoch training loss: 0.610997 \tEpoch training l2_norm: 2.39%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.35%,                 Avg loss: 0.584000 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Epoch training loss: 0.633462 \tEpoch training l2_norm: 2.43%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.39%,                 Avg loss: 0.597597 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Epoch training loss: 0.621702 \tEpoch training l2_norm: 2.41%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.38%,                 Avg loss: 0.592924 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Epoch training loss: 0.633180 \tEpoch training l2_norm: 2.44%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.27%,                 Avg loss: 0.541414 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Epoch training loss: 0.600874 \tEpoch training l2_norm: 2.36%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.20%,                 Avg loss: 0.511262 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Epoch training loss: 0.632233 \tEpoch training l2_norm: 2.43%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.26%,                 Avg loss: 0.538477 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Epoch training loss: 0.624723 \tEpoch training l2_norm: 2.41%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.26%,                 Avg loss: 0.536450 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Epoch training loss: 0.630327 \tEpoch training l2_norm: 2.42%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.43%,                 Avg loss: 0.620272 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Epoch training loss: 0.623931 \tEpoch training l2_norm: 2.41%                                        ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 2.26%,                 Avg loss: 0.540012 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5400117442995163, tensor(2.2638, dtype=torch.float64))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "pipe = Trainer(model, (dl_tr, dl_val), loss_fn, writer,l2_norm)\n",
    "pipe.train(SGD, 200, False, {\"lr\": 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "321198df",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp=pipe.model(dl_tr.dataset.tensors[0])\n",
    "X_t=dl_tr.dataset.tensors[0].detach().numpy().reshape(-1)\n",
    "y_t=resp.detach().numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde5feb",
   "metadata": {},
   "source": [
    "Let us take a look of the graph of the model we defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6517eecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6c6ff626d0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind=np.argsort(X_t)\n",
    "plt.plot(X_t[ind],y_t[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb25728",
   "metadata": {},
   "source": [
    "Next we define an ad hoc regularizer that penalizes the function from attaining values higher than 2, and we do this in a very barbarian way to demonstrate the regularization logic. In general, these kind of restrictions could also be effectively imposed by a suitable model architecture.\n",
    "\n",
    "Our regularizer evaluates the model on a grid, which is defined by preprocess step. The penalty consists of evaluating the model on this grid, and then picking out the points where the model exceeded 2. The penalty is the squared sum of the model values at these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9a4cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapReg:\n",
    "    def __init__(self,lamda):\n",
    "        self.lamda=lamda\n",
    "        self.X=torch.linspace(0,2*torch.pi,1000)\n",
    "\n",
    "    def regularization_penalty(self, model):\n",
    "        \"\"\"\n",
    "        We penalized the squared values of the function at the points where it attains value higher than 2.\n",
    "        \"\"\"\n",
    "        res=model(self.X.reshape(-1,1)).reshape(-1)\n",
    "        inds1=torch.where(res>2)        \n",
    "        X1=self.X[inds1]\n",
    "        res1=model(X1.reshape(-1,1)).reshape(-1)\n",
    "        return torch.sum(res1**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44cf4c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg=CapReg(lamda=1/(2*S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6aa4bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Trainer(model, (dl_tr, dl_val), loss_fn, writer,l2_norm,regularizer=reg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da3115",
   "metadata": {},
   "source": [
    "The regression penalty that we defined is computed for every batch in our dataset. This way, their gradients are updated every single batch. A reasonable pick for the regression penalty coefficient $\\lambda$ should then be inversely proportional to the number of batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3999a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 126.537467 \tEpoch training l2_norm: 5.33%                                      ward1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 4.03%,                 Avg loss: 1.664963 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 294.389519 \tEpoch training l2_norm: 4.32%                                      ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 3.72%,                 Avg loss: 1.435014 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 537.780211 \tEpoch training l2_norm: 4.13%                                      ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 4.30%,                 Avg loss: 1.929356 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 454.557764 \tEpoch training l2_norm: 3.90%                                      ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 3.86%,                 Avg loss: 1.519824 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 472.303556 \tEpoch training l2_norm: 3.72%                                      ward1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 3.60%,                 Avg loss: 1.348129 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 513.919797 \tEpoch training l2_norm: 3.55%                                      ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 3.42%,                 Avg loss: 1.209986 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 551.482306 \tEpoch training l2_norm: 3.65%                                      ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 3.25%,                 Avg loss: 1.095451 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 488.047499 \tEpoch training l2_norm: 3.63%                                      ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 3.84%,                 Avg loss: 1.524658 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 562.818941 \tEpoch training l2_norm: 3.65%                                      ard1>)  \t[ 90 / 90 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 3.45%,                 Avg loss: 1.207476 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 627.845327 \tEpoch training l2_norm: 3.54%                                      ard1>)  \t[ 90 / 90 ]                      \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.10000000\n",
      "Validation results: \n",
      " l2_norm: 3.00%,                 Avg loss: 0.935804 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9358044771156159, tensor(2.9968, dtype=torch.float64))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2.train(SGD, 10, False, {\"lr\": 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65c2db1",
   "metadata": {},
   "source": [
    "Let us now take a look at the graphs of the two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d3587a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp=pipe2.model(dl_tr.dataset.tensors[0])\n",
    "X_t2=dl_tr.dataset.tensors[0].detach().numpy().reshape(-1)\n",
    "y_t2=resp.detach().numpy().reshape(-1)\n",
    "ind2=np.argsort(X_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3caaba36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGdCAYAAAAvwBgXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAABTEElEQVR4nO3dd3hUdf728ffMpFdICJAKoXcIVUQBscEqiq7KKrrgsq4FdkXUVXafteyq2LAriAX4WRbLrqwdEQEFpfdOaAmEhJ5eZ+b54yQDEUgmJJMzk9yv65orJ5lzznwYMXPzrRan0+lERERExARWswsQERGRxktBREREREyjICIiIiKmURARERER0yiIiIiIiGkURERERMQ0CiIiIiJiGgURERERMY2f2QVUxeFwkJGRQXh4OBaLxexyRERExA1Op5Pc3Fzi4uKwWqtu8/DqIJKRkUFiYqLZZYiIiMh5SE9PJyEhocpzvDqIhIeHA8YfJCIiwuRqRERExB05OTkkJia6Pser4tVBpKI7JiIiQkFERETEx7gzrEKDVUVERMQ0CiIiIiJiGgURERERMY1XjxERERHPcTqdlJWVYbfbzS5FfJC/vz82m63W91EQERFphEpKSjh06BAFBQVmlyI+ymKxkJCQQFhYWK3uoyAiItLIOBwO9u7di81mIy4ujoCAAC0aKTXidDo5cuQIBw4coH379rVqGVEQERFpZEpKSnA4HCQmJhISEmJ2OeKjYmJi2LdvH6WlpbUKIhqsKiLSSFW39LZIVeqqFU1/C0VERMQ0CiIiIiL1ZNy4cYwaNarW95k9ezZNmjSp9X2qUle1VkdBRERExMeMHj2anTt3ml1GndBgVRERabScTid2ux0/P9/5OCwtLSU4OJjg4GCzS6kTHn3np0+fzvTp09m3bx8AXbt25ZFHHmHEiBGefFmppWN5xby7bC8FJVrkqCb8rBZGpcTTNS7S7FJEGqzWrVszadIkJk2a5PpZr169GDVqFI899hgWi4W33nqLr776ivnz5xMfH8+0adO45pprAFi8eDGXXHIJX3/9Nf/v//0/Nm3axHfffcfgwYN55plnmDlzJpmZmXTo0IF//OMf3HDDDa7X+fzzz7n//vtJT09n4MCBjBs3jnHjxnHixAmaNGnCY489xrx581i/fr3rmpdeeomXXnrJ9Tn4a99++y1PPPEEmzdvxmazMXDgQF5++WXatm0LwL59+0hOTmbu3Lm88cYbrFixghkzZgAwadIkTp486Xpf9u/ff8b9nU4nYOxif//99/Pdd99htVq5+OKLefnll2ndujUAdrudBx98kHfffRebzcb48eNd13qaR4NIQkICTz/9NO3bt8fpdDJnzhyuvfZa1q1bR9euXT350lILL36/k/eXp5ldhk/6cEUaS/56Cc3CAs0uRaRGnE4nhaXm/OMj2N9Wp+uYPP744zz77LM899xzvPrqq4wZM4b9+/cTFRXlOufhhx/m+eefp02bNjRt2pSpU6fy/vvvM2PGDNq3b8+PP/7IrbfeSkxMDEOGDGHv3r3ccMMN3Hvvvfzxj39k3bp1PPDAA7WuNT8/n8mTJ9OjRw/y8vJ45JFHuO6661i/fn2lWU0PP/ww06ZNIyUlhaCgIObPn1/pPqtWrXKtkGu327nhhhvw9/cHjBaUK6+8koEDB/LTTz/h5+fHE088wfDhw9m4cSMBAQFMmzaN2bNn8+6779K5c2emTZvGZ599xrBhw2r9Z6yOR4PIyJEjK33/5JNPMn36dJYvX64g4qWcTidfbTwEwO/6JRIdFmByRb7B6YQ3Fu8mv8TOurSTXN6lhdklidRIYamdLo/Mr/5ED9j6zysJCai7j6Nx48Zx8803A/DUU0/xyiuvsHLlSoYPH+4655///CeXX345AMXFxTz11FN8//33DBw4EIA2bdqwdOlS3nzzTYYMGcKbb75Jx44dee655wDo2LEjmzdv5sknn6xVrb/97W8rff/uu+8SExPD1q1b6datm+vnkyZN4vrrrz/nfWJiYlzH9957L4cOHWLVqlUAfPTRRzgcDt5++21X4Js1axZNmjRh8eLFXHHFFbz00ktMmTLF9RozZsw4I+x4Sr11itntdj755BPy8/Nd/6F/rbi4mOLiYtf3OTk59VWelNt/rIATBaUE+Fn516hu+Ns0ntldaccL+HLjIfYcyQMURETM0qNHD9dxaGgoERERHD58uNI5ffv2dR2npqZSUFDgCiYVSkpKSElJAWDHjh3069ev0vP9+/evda27du3ikUceYcWKFRw9ehSHwwFAWlpapSByer1VmTlzJu+88w4///yzK5xs2LCB1NRUwsPDK51bVFTE7t27yc7O5tChQwwYMMD1nJ+fH3379q2X7hmPB5FNmzYxcOBAioqKCAsL47PPPqNLly5nPXfq1Kk8/vjjni5JqrDpYDYAnVuGK4TUUJsYY7+FPUfyTa5EpOaC/W1s/eeVpr22u6xW6xkfjqWlpZW+r+iSqGCxWFwf8BVCQ0Ndx3l5eQB89dVXxMfHVzovMND9blZ3avu1kSNH0qpVK9566y3i4uJwOBx069aNkpKSc9Z7LosWLeLPf/4z//73vyuFsby8PPr06cMHH3xwxjWnt6SYxeNBpGPHjqxfv57s7Gw+/fRTxo4dy5IlS84aRqZMmcLkyZNd3+fk5JCYmOjpEuU0mzOMINItXgMua6ptjPGLYs/RPJMrEak5i8VSp90jnhITE8OhQ4dc3+fk5LB3795a3bNLly4EBgaSlpbGkCFDznpOx44d+frrryv9rKLr4/TaMjMzcTqdri6Q0weu/tqxY8fYsWMHb731FhdffDEAS5cuPa8/Q2pqKjfccAN/+9vfzujC6d27Nx999BHNmzcnIiLirNfHxsayYsUKBg8eDEBZWRlr1qyhd+/e51VPTXj8n7wBAQG0a9eOPn36MHXqVHr27MnLL7981nMDAwOJiIio9JD6tbm8RaS7gkiNtWmmFhERTxs2bBjvvfceP/30E5s2bWLs2LG13oo+PDycBx54gPvuu485c+awe/du1q5dy6uvvsqcOXMAuPPOO9m+fTsPPfQQO3fu5OOPP2b27NnAqaXOhw4dypEjR3j22WfZvXs3r7/+Ot988805X7dp06ZER0czc+ZMUlNT+eGHHyr9Y9xdhYWFjBw5kpSUFP70pz+RmZnpegCMGTOGZs2ace211/LTTz+xd+9eFi9ezF/+8hcOHDgAGONKnn76aebNm8f27du55557XDNyPK3e294dDkelcSDiPZxOJ5sPGuNy1CJSc8nlLSLH8kvILqi6OVZEzs+UKVMYMmQIV199NVdddRWjRo1yTXWtjX/961/84x//YOrUqXTu3Jnhw4fz1VdfkZycDEBycjKffvop//3vf+nRowfTp0/n73//O3Cq+6Zz58688cYbvP766/Ts2ZOVK1dWObPGarUyd+5c1qxZQ7du3bjvvvtcg2FrIisri+3bt7Nw4ULi4uKIjY11PQBCQkL48ccfSUpK4vrrr6dz586MHz+eoqIi1z/477//fm677TbGjh3LwIEDCQ8P57rrrqtxLefD4vTgSJQpU6YwYsQIkpKSyM3N5cMPP+SZZ55h/vz5ZwwKOpucnBwiIyPJzs5W60g9SDtWwODnFhFgs7L58SsJ8NMYkZoa8NT3ZOUU8997LqR3UlOzyxE5q6KiIvbu3UtycjJBQUFml+OznnzySWbMmEF6errZpZiiqr9HNfn89miH4OHDh/n973/PoUOHiIyMpEePHm6HEKl/FQNVO7YMVwg5T22ahZGVU8yeI/kKIiINzBtvvEG/fv2Ijo5m2bJlPPfcc0ycONHssnyeR4PIO++848nbSx3TQNXaaxMTyi97jpVP4RWRhmTXrl088cQTHD9+nKSkJO6//36mTJlidlk+z/uHSEu90UDV2tMUXpGG68UXX+TFF180u4wGR+3vAhgDVSu6ZrrFazzO+WqjKbwiIjWiICIAHDhRyMmCUvxtFjq2DK/+AjmrtuVTePcdK8DuqJ8No0REfJmCiACwpXx8SIcW4QT61W5OfmMW3zSYAD8rJWUODp4oNLscERGvpyAiwKkZMxofUjs2q4XW0SEA7Fb3jIhItRREBIBNWsiszmiFVRER9ymISPmKqpq6W1dcA1Y1hVdEpFoKIkJGdhHH80vws1ropIGqtaYpvCINz759+7BYLFVuYueu1q1b89JLL9X6PudSl7XWB60jIq7WkPYtwgmqwXbccnaawisiVVm1ahWhoaFml+E11CIipy1kpvVD6kLFFN6snGLyistMrkakYSspKTG7BLdV1BoTE0NISIjJ1XgPBRE5bSEzjQ+pC5Eh/kSHBgCwV90zInVq6NChTJw4kUmTJtGsWTOuvPJKNm/ezIgRIwgLC6NFixbcdtttHD161HVNbm4uY8aMITQ0lNjYWF588UWGDh3KpEmTXOdYLBbmzZtX6bWaNGnC7Nmzz1qH3W5n/PjxJCcnExwcTMeOHXn55ZcrnTNu3DhGjRrFk08+SVxcHB07dgQqd83Mnj0bi8VyxuOxxx5z3eftt9+mc+fOBAUF0alTJ954441Kr7Ny5UpSUlIICgqib9++rFu3rmZvqsnUNdPIaaCqZ7SJCeVYfgl7jubRPUHvq/gApxNKC8x5bf8QsFjcPn3OnDncfffdLFu2jJMnTzJs2DD++Mc/8uKLL1JYWMhDDz3ETTfdxA8//ADA5MmTWbZsGZ9//jktWrTgkUceYe3atfTq1eu8S3Y4HCQkJPDJJ58QHR3Nzz//zJ/+9CdiY2O56aabXOctXLiQiIgIFixYcNb7jB49muHDh7u+X7x4MbfddhuDBg0C4IMPPuCRRx7htddeIyUlhXXr1nHHHXcQGhrK2LFjycvL4+qrr+byyy/n/fffZ+/evdx7773n/ecyg4JII5eVU8zRvBJsVgtdYtU1U1faNAtj1b4T7FaLiPiK0gJ4Ks6c1/5bBgS4P2aiffv2PPvsswA88cQTpKSk8NRTT7mef/fdd0lMTGTnzp3ExsYyZ84cPvzwQy699FIAZs2aRVxc7f6s/v7+PP74467vk5OT+eWXX/j4448rBZHQ0FDefvttAgICznqf4OBggoODAdi9ezcTJkzgqaeecu1S/+ijjzJt2jSuv/561+ts3bqVN998k7Fjx/Lhhx/icDh45513CAoKomvXrhw4cIC77767Vn+++qQg0shVdMu0bx6mgap1SFN4RTynT58+ruMNGzawaNEiwsLCzjhv9+7dFBYWUlpaSv/+/V0/j4yMdHWT1Mbrr7/Ou+++S1paGoWFhZSUlJzRytK9e/dzhpDTZWdnc/XVV3PVVVfx4IMPApCfn8/u3bsZP348d9xxh+vcsrIyIiONltZt27bRo0cPgoKCXM8PHDiw1n+2+qQg0shpfIhnaAqv+Bz/EKNlwqzXroHTZ5zk5eUxcuRInnnmmTPOi42NJTU11a17WiwWnM7K+0OVlpae8/y5c+fywAMPMG3aNAYOHEh4eDjPPfccK1asOGet52K32xk9ejQRERHMnDnT9fO8POMfMm+99RYDBgyodI3N1nD+4agg0si5xofEqVumLlW0iOw9mo/D4cRqdb//W8QUFkuNuke8Re/evfnPf/5D69at8fM78yOtTZs2+Pv7s2rVKpKSkgCj9WHnzp0MHjzYdV5MTAyHDh1yfb9r1y4KCs49ZmbZsmVceOGF3HPPPa6f7d69+7z+DPfddx+bNm1i9erVlVo2WrRoQVxcHHv27GHMmDFnvbZz58689957FBUVua5dvnz5edVhFs2aaeRce8xoQGWdSooKwc9qobDUTmZOkdnliDRYEyZM4Pjx49x8882sWrWK3bt3M3/+fG6//Xbsdjvh4eGMHTuWBx98kEWLFrFlyxbGjx+P1WrFctoA2WHDhvHaa6+xbt06Vq9ezV133YW/v/85X7d9+/asXr2a+fPns3PnTv7xj3+watWqGtc/a9Ys3njjDWbMmIHFYiEzM5PMzExXa8jjjz/O1KlTeeWVV9i5cyebNm1i1qxZvPDCCwDccsstWCwW7rjjDrZu3crXX3/N888/X+M6zKQg0ogdziniSG4xVgt0iVUQqUv+NitJUUZzs7pnRDwnLi6OZcuWYbfbueKKK+jevTuTJk2iSZMmWK3GR9wLL7zAwIEDufrqq7nssssYNGiQazpshWnTppGYmMjFF1/MLbfcwgMPPFDlWh933nkn119/PaNHj2bAgAEcO3asUuuIu5YsWYLdbueaa64hNjbW9agIE3/84x95++23mTVrFt27d2fIkCHMnj2b5ORkAMLCwvjiiy/YtGkTKSkp/P3vfz9rN5U3szh/3SnmRXJycoiMjCQ7O5uICHUd1LWF27IYP2c1HVqE8d19Q8wup8H545xVfL/tMP+8tiu/H9ja7HJEXIqKiti7dy/JycmVPowbi/z8fOLj45k2bRrjx483uxyfVdXfo5p8fmuMSCOmgaqe1SYmDLYdVouIiMnWrVvH9u3b6d+/P9nZ2fzzn/8E4NprrzW5MgEFkUbt1EBVBRFPaNPMGPi3W1N4RUz3/PPPs2PHDgICAujTpw8//fQTzZo1M7ssQUGkUdt8MAfQQFVP0RReEe+QkpLCmjVrzC5DzkGDVRupI7nFZOYUYbGgFVU9pGIKb0Z2IUWldpOrERHxTgoijVRFt0zbmDBCA9Uw5gnRoQFEBPnhdBrriYiIyJkURBqpTVrIzOMsFou6Z8SrefGkSfEBdfX3R0GkkdKMmfqhPWfEG1Us1FXVyqEi1SkpKQFqv9y82uQbqS0VK6oqiHhU24oWEXXNiBex2Ww0adKEw4cPAxASElJplVGR6jgcDo4cOUJISMhZl9avCQWRRuhYXjEZ2cZA1a4KIh5VMYVXLSLibVq2bAngCiMiNWW1WklKSqp1iFUQaYQqumWSm4USpoGqHnX6GBGn06l/dYrXsFgsxMbG0rx58yp3mRU5l4CAANcy+rWhT6FGSAuZ1Z9W0SFYLJBbXMaRvGKahze+5bTFu9lstga1pbz4Hg1WbYRcC5mpW8bjgvxtJDQNBjRzRkTkbBREGiHNmKlfbZppCq+IyLkoiDQyJ/JLOHiyEICu8VpDpD5oCq+IyLkpiDQypw9UjQjyN7maxqGNpvCKiJyTgkgjsyH9JABdtaJqvWmrKbwiIuekINJIOJ1OFm7L4oMVaQBc2FbbX9eXts2NFpG04wUUlJSZXI2IiHfR9N0GrKCkjG2HctmSkc28dQdZm3YSgLjIIEb2jDW3uEakeXggLSICycopZuOBbC5oE212SSIiXkNBpAEpLrPznzUHWbn3GJszcthzJA/HaXsSBflbGTuwNXcNaUu4xofUG4vFQt/WUXy18RB//XQjb4zprRlLIiLlFEQaCIfDyYQP1vL9tsrLNTcPD6RrXAQ9E5twS/8kmkdoQS0zPHhFRzYeOEna8QKufnUpA9tE87v+iVzZtSVB/lpMSkQaL4vTi/eBzsnJITIykuzsbCIiNLiyKq8vSuW5+Tvws1r40+A29GsdRde4CAUPL3I0r5hHP9/CN5sOuVqqIoP9uS4lnpv6JtJFA4hFpIGoyee3gkgDsCz1KLe9swKHE575bXdG90syuySpwsGThXyyOp1PVh9wrekC0CMhkpv6JnJNrzhNrRYRn6Yg0lCc2A+7voOSPCjJB3sJ2MvAUQr2UnCUUlhUzJLtGZSVlZHYNISeCaeNPThjg7VffV/p+aqeM+N5qnneg69fT392h9NJxslCdmTlsf9YAY7y/xVtVivJzULp2DKclhFBZ26UV9X9A0KheWdofTGERCEiYoaafH5rjIi32jIPPrsTyoqqPC0YGA5gA3KArR6vTOqIFUgof/DrYSLHyx/nfXN/uOIJuOCuWtxERMTzFES8Ud4R+OIvRghp2R1a9gT/YPALBKsf2PzB6s/CncdZvj8Hm38Afxrcjqiw08aDVGro+lWj1xmNYDV4vjbXevzeHqrrjOc9d2+n00lWThHbM3NJPZJHmd1BsL+N3/aOJyzQr/p7F56Eg6vhyHb49iEoOglDHjpLK4qIiHdQEPFG3/0/KMqGlj3gjkVgO/M/0xcbMvjznnUAvDWmL1FdWtR3leIBFqBl+aN3USk3z1zOlowcthQn8OzInu7dxOmEH5+DRU/C4qnG36UrngSr1i8UEe+j30zeZu+PsHEuYIGRL501hKQezuWh/2wE4J6hbblcIaRBigjy51+jugHw37UHKw1srZLFAkP+CiOeNb5f/gZ88eeztMyIiJhPQcSblBXDl5ON437jIb7PGafkF5dx1/trKSixM7BNNJMv71DPRUp96p3UlAvbRlPmcDJzye6aXTzgThg1Ayw2WPc+rHvPM0WKiNSCgog3+fkVOLYLQpvDsH+c8bTT6eTh/24i9XAeLSICeeXmFPxs+k/Y0E28pB0Ac1elcyS3uGYX97oZLnvMOJ7/d8g+ULfFiYjUkj7FvMXxvfDj88bxlU9BcJMzTpnz8z6+2JCBn9XC67f0JiY8sH5rFFMMbBtNr8QmFJc5eGfp3vO4wQRI6AfFOfDFveqiERGvoiDiDZxO+PoBY5ZM8hDofsMZp6zZf4InvtoGwN9+05m+rbVGRGNhsVhcrSLvL99PdkFpzW5gtcG1b4AtEFK/h/UfeKBKEZHzoyDiDbb+z/iAsAXAVS+cMdXyaF4xEz5YS5nDyVU9Yrl9UGtz6hTTXNq5OZ1ahpNXXMbsn/fV/AYxHWDY343jb/8G2QfrtD4RkfOlIGK2ohz49mHj+KL7oFm7Sk/bHU7unbuOzJwi2saE8sxve5y50qY0eBaLhQnlrSKzft5LfnFZzW8ycCLE94XibHXRiIjXUBAx2+KpkHsImibDRZPPePrFBTtZlnqMkAAbM27tU76olTRGv+keS3KzUE4WlPLBiv01v4HVBqMqumgWwPoP675IEZEaUhAx06ENsGKGcXzVNPCvvFPuwm1ZvLYoFYCp13enfYvw+q5QvIjNauHuIW0BeOunvRSV2mt+k5iOcMkU4/jbKZCTUYcViojUnEeDyNSpU+nXrx/h4eE0b96cUaNGsWPHDk++pO9w2OGLSeB0QLffQrtLKz2ddqyA+z5aD8C4C1tzba/4+q9RvM6olHjiIoM4klvMJ2vOcyruwD9DXG+ji+Z/E40NFEVETOLRILJkyRImTJjA8uXLWbBgAaWlpVxxxRXk5+d78mV9w5pZkLEWAiOM6bqnKSq1c/cHa8gpKiMlqQl/+01nk4oUbxPgZ+XO8laRGYt3U2p31PwmNj8YNd3ootm9ED79g8KIiJjGo0Hk22+/Zdy4cXTt2pWePXsye/Zs0tLSWLNmjSdf1vvlHYbv/2kcD/sHhLes9PRjn29hS0YOUaEBvDGmNwF+6kGTU0b3S6RZWAAHTxbyv/Xn2bXSvBPc9H/GTK1tn8Mn46CspE7rFBFxR71+wmVnZwMQFXX2NTCKi4vJycmp9GiQ5v/daBaP7WUs5X6aj1elM3dVOhYLvPK7FGIjg82pUbxWkL+N8Re1AeCNxanYHec5+6XjcPjdh0bLyPYv4ePfG9sMiIjUo3oLIg6Hg0mTJjFo0CC6det21nOmTp1KZGSk65GYmFhf5dWfJc/Bpo8BC1z9ojGTodzmg9n843+bAbj/8g5c1L6ZSUWKt7v1giQigvzYcySf+Vsyz/9G7S+Hm/8NfkGw8xv46v66K1JExA31FkQmTJjA5s2bmTt37jnPmTJlCtnZ2a5Henp6fZVXP9bMgUVPGMeXPQrxvV1PZReUcs8HaykuczCsU3PuGdruHDcRgfAgf8YNSgbg9UWpOGuzJki7S+F3HwAWY2O8QxvrpkgRETfUSxCZOHEiX375JYsWLSIhIeGc5wUGBhIREVHp0WDkZMB3/884HvKwsXhZOYfDyf2frCfteAEJTYN58aZeWK1atEyqdvuFrQkJsLElI4fFO47U7mbtLoOu1xnHS1+ofXEiIm7yaBBxOp1MnDiRzz77jB9++IHk5GRPvpx3+/pBY9Ox+D4w5K+Vnprx426+33aYAD8rM27tQ2SIv0lFii9pGhrAmAFJALxW21YRgIvLu2W2zIOju2p3LxERN3k0iEyYMIH333+fDz/8kPDwcDIzM8nMzKSwsNCTL+t9tn5uDAa0+sHIVyqNC/k59SjPzzfWVvnnNV3pFh9pVpXig+64uA0BflbW7D/B8j3Ha3ezlt2gwwjACUtfqovyRESq5dEgMn36dLKzsxk6dCixsbGux0cffeTJl/UuhSeN1hCAQfcav+zLZWYX8ed/r8PhhBv7JDC6XwMcnCse1TwiiJv6Gt2dbyxOrf0NBz9gfN04F06m1f5+IiLV8HjXzNke48aN8+TLepfvH4O8TIhqC4NPdcmUlDm454M1HMsvoXNsBP8a1U2b2cl5uXNwW2xWCz/tOsqG9JO1u1lCX0geAo4yWPZKndQnIlIVrZTlSft/NlZQBbjmlUp7yUz9Zhtr004SHuTHjFt7E+RvO8dNRKqWGBXCtb3iAFx7E9VKxViRtf8HuVm1v5+ISBUURDyltAg+/4tx3Pv30Poi11Nfbsxg1rJ9ALxwUy9aRYeaUKA0JPcMbYfFAgu2ZrE9s5YLASYPhoR+YC+G5a/XTYEiIuegIOIpP02DY7sgtDlc/k/Xj1MP5/LQp8Y6DXcPbcvlXVqYVaE0IO2ahzGim7FVwBuLdtfuZhYLXFw+VmTVO1BQy0GwIiJVUBDxhKytsPRF4/g3z0FwUwDK7A7u+WAt+SV2BraJ5v7LO5hYpDQ0FYvgfbkxg31Ha7mxZIcroUU3KMmDlTProDoRkbNTEKlrDjt88RdwlELH30CXa11PfbXpEDuz8ogKDeCVm1Pws+ntl7rTLT6SSzrG4HDCjCV10Soy2ThePh2Kc2tfoIjIWeiTsK6tegcOrIKAcPjN88Yv9HLfbDL2BBkzIImY8ECzKpQGbMIlRqvIf9YeIONkLdfr6TIKottB0UlYPavWtYmInI2CSF06mQ4LHzeOL3sUIuNdT5WUOViaetR4qrPGhYhn9G0dxYDkKErtTmb+uKd2N7PaTm1F8MtrxgBsEZE6piBSV5xO+PI+o0898QLoO77S02v2nyCvuIzo0AC6a/VU8aCJw4xWkbmr0jiaV1y7m/UYDZGJkJcF69+vg+pERCpTEKkrGz+G1AVgC4BrXgVr5bd28Y7DAAzpEKMN7cSjLmrXjJ4JkRSVOnh36d7a3czmDxeWT0Nf+jLYS2tfoIjIaRREasthh2Uvw+d/Nr4f8leIOXM2TMXuqEM6xtRnddIIWSwW7ikfK/LeL/vJLqxleOh9mzENPTsNNn1SBxWKiJyiIFIbR3bAO1fAgkeMxZ/aXwmDJp1xWsbJQnZk5WK1wOD2CiLieZd3bkGHFmHkFpfxfz/vq93N/INh4ATj+KcXjPAtIlJHFETOh73MWCdkxsVwcDUERsA1r8EtHxlN2b9S0RrSM7EJTUMD6rtaaYSsVotrBs27y/aSX1xWuxv2Gw9BkcYifdu+qIMKRUQMCiI1dXgbvHO5sZmdvRjaXQ73LDear8+xad0P243xIcM6Nq/HQqWxu6p7LK2iQzhRUMq/V9ZyJ93AcBhwl3H80/PG4GwRkTqgIOIuexn8+Dy8ORgy1kJgJFz7Boz5pNI03V8rKrWzrHza7iWdFESk/vjZrNw1pC0AM3/cQ3FZLbtUBtwF/iGQuQnSV9ZBhSIiCiLuydoCb18KP/wL7CXGWJAJyyFlzDlbQSqs3HucwlI7zcMD6RoXUU8Fixiu7x1Py4ggDucW8+maA7W7WUgUdL3OOP7l1doXJyKCgkjV7KWw5Fl4cwgcWm/0kV/3pjEWJCLOrVtUdMtc0rE5lmpCi0hdC/Sz8afBbQBj2fcyu6N2N7zwz4DFGCdycE3tCxSRRk9B5FwyN8Fbw2DRk6f2jZmwEnr+rtpWkApOp5NF5euHqFtGzHJz/ySiQwNIP17I5xsyanez5p2N/wcAFv6z6nNFRNygIPJrZSWw+GmYORQyNxo7517/NvzuQwhvWaNb7T2az/5jBfjbLFzUvpln6hWpRnCAjT9clAzAG4t3s+1QTqVHTlEN1xkZOgWs/rBnsfEQEakFP7ML8CqHNsC8CZC1yfi+09Vw1QsQfn57w1R0y/RPjiIsUG+1mOe2ga2YsWQ3qYfzGPHyT5Wes1kt9Elqyg19E7ixT0L1XYhNW0HfP8DKN41WkeQhbrcSioj8mlpEwGgF+eFJoysmaxMER8EN78Lo9887hACnumU0bVdMFhHkz5QRnWkZEUSzsEDXo2mIP3aHk5X7jvPXTzeycNth9244+AHwDzXGiWz/0rPFi0iDpn+mZ6wzWkEObzG+73It/GYahNVuBdS84jJW7j0OwDCNDxEvcMuAJG4ZkHTGz9OOFfDywl38Z+0Bpn6zjaEdY/CzVfNvlLDmcMHdxpoiC/9ljKGy2jxUuYg0ZI23RaSs2GhWfutSI4SENIMbZ8NN/1frEAKwdNdRSu1OWkWHkNwstPb1inhIUnQIj4zsQtMQf3Yfyeej1enuXTjoL8YYqqM7YMNczxYpIg1W4wwiGeuMKbk/TQOnHbpeDxNWnFojoQ4s3qFpu+I7IoP9+fOw9gC8uGAXee4sCR8UCRfdZxwvnmqEexGRGmqcQSQ3E45sg9AYowXkxlkQWnezWk6ftqtuGfEVt17QilbRIRzNK2bmj3vcu6j/nyA8FrLTYfUszxYoIg1S4wwiHUcYs2HuWWGMCaljWzJyyMopJtjfRv/kqDq/v4gnBPhZ+euVnQB468c9ZOUUVX+RfzAM+atx/ONzUJzrwQpFpCFqnEEEjN1EQ6M9cuuKbplB7ZoR5K8BfOI7ftO9JSlJTSgstfPigp3uXZRyG0S1gYKjsOodzxYoIg1O4w0iHuTabVfdMuJjLBYLf/9NZwA+Xp3Oziw3Wjhs/nDx/cbxijeN6fAiIm5SEKljx/NLWJd+EoChHWs/+0akvvVtHcXwri1xOGHq19vcu6j7jRDaHHIzYOs8j9YnIg2Lgkgd+3HnEZxO6NQynLgmwWaXI3JeHhrRCT+rhUU7jvBz6tHqL/ALNAauAvz8Kjidni1QRBoMBZE6pm4ZaQiSm4Uypnzxsye/3obD4Uaw6Dce/IKNPZr2/VT9+SIiKIjUqTK7gyU7jwDabVd8318ubU94oB9bMnL434aD1V8QEgW9bjGOf37Ns8WJSIOhIFKH1qefJLuwlMhgf1ISm5hdjkitRIcFctfQtgA8P38nRaX26i8aOAGwwK75cGSHZwsUkQZBQaQOVSxiNqSDG3t1iPiA8RclExsZxMGThcz+eV/1F0S3NfadAfjldY/WJiINgz4t69CKPcYmdxe1r7tVWkXMFORv4/4rOgLw+qJUTuS7MTX3wonG1w1zIe+IB6sTkYZAQaSOFJXa2XggG4D+rbWaqjQc16XE06llOLlFZXyyxo0N8ZIGQlxvsBfDqrc9X6CI+DQFkTqy+WA2JXYHzcICaBUdYnY5InXGZrUw9sLWAMxdmY6zuqm5FsupVpFVb0NpoWcLFBGfpiBSR1btOwFA31ZR2m1XGpyRPeMIDbCx52g+K/Yer/6CztdCZKKx7PvGjzxfoIj4LAWROrJmv/HLuW/rpiZXIlL3wgL9uKZXHABzV6ZVf4HNDwbcZRz/8jo4HB6sTkR8mYJIHXA4nKzeX94iovEh0kD9rp+xwNnXmzM5WeDGoNXev4fACDi6E1IXeLg6EfFVCiJ1YM/RPE4WlBLkb6VrXITZ5Yh4RI+ESLrERlBS5uC/a91Y4CwowggjYCz7LiJyFgoidaBifEivxCb4a/0QaaAsFgs3908EYO6qtOoHrYLRPWOxGUu+H9rg4QpFxBfpU7MOrD5toKpIQ3ZtSjxB/lZ2ZuWxNu1k9Rc0SYSu1xnHWvZdRM5CQaSWUg/n8t3WTAD6aKCqNHARQf5c3cMYtPpvdwatwqmpvFv+C9ludOmISKOiIFILDoeTiR+uI7eojJSkJlzUTiuqSsNX0T3z5cYMsgtKq78gLgVaXQSOMlj5poerExFfoyBSC19szGB7Zi7hQX68/fu+Gh8ijULvpKZ0ahlOUamDT9cecO+iilaR1bOhONdjtYmI79EnZy289dMeAO4c3IbosECTqxGpHxaLhdsGtgLgg+X73Ru02v5KiG4Hxdmw7n0PVygivkRB5Dxtychm88Ec/G0WbhnQyuxyROrVqF7xhAX6sedoPj/vPlb9BVYrXHCPcbz8DbCXebZAEfEZCiLn6ZPVRpP0FV1aEhUaYHI1IvUrNNCP63vHA/DeL/vdu6jnzRAcBSfTYPsXHqxORHyJgsh5KC6zM2+9Mfr/xr4JJlcjYo5bLzBaAhdsy+JQthsb2wWEQL8/Gsc/vwbudOmISIOnIHIeFmzN4mRBKbGRQVzcPsbsckRM0aFFOAOSo7A7nPx7Zbp7F/W/A2yBcHA1pK/wbIEi4hMURM7Dx+XdMjf0ScBm1U670nhVDFqduzKNUrsbG9uFNYceNxnHWvZdRFAQqbGDJwv5adcRwAgiIo3ZFV1a0iwskMO5xSzYmuXeRQPLp/Ju/wqO7/FccSLiExREaug/aw7gdMIFbaJoFR1qdjkipgrws7oWOHN70GrzTtDucsAJy6d7rjgR8QkeDSI//vgjI0eOJC4uDovFwrx58zz5ch7ncDj5ZI3RFz66X6LJ1Yh4h5v7J2G1wC97jpF62M3FyioWOFv3PhQc91xxIuL1PBpE8vPz6dmzJ6+//ronX6beLN9zjPTjhYQH+jG8a6zZ5Yh4hbgmwVzWuQUA7y93c/+Z5CHQohuUFsCaWR6sTkS8nUeDyIgRI3jiiSe47rrrPPky9ebj1UZryMhecQQH2EyuRsR7VEzl/c+aAxSUuLFYmcVyaqzIiplQVuLB6kTEm3nVGJHi4mJycnIqPbxFdmEp32w2dtkd3VfdMiKnu6hdM1pHh5BbXMb/1me4d1G330J4LORlwuZPPVugiHgtrwoiU6dOJTIy0vVITPSeD/wvNmRQXOagY4tweiREml2OiFexWi2uVpH3fnFz/xm/AOj/J+NYC5yJNFpeFUSmTJlCdna265Ge7uYiSfWgolvmxr4JWCxaO0Tk127ok0Cgn5Wth3JYm3bSvYv63g7+oXB4C+xZ5NH6RMQ7eVUQCQwMJCIiotLDG2w7lMPGA9n42yxclxJvdjkiXqlJSADX9IwD4P3lbk7lDW4KKbcax780jEHtIlIzXhVEvFXFBneXdW5BdFigydWIeK+K7pmvNh7ieL6bA1AvuBssVkj9Hg5v82B1IuKNPBpE8vLyWL9+PevXrwdg7969rF+/nrQ0N6f4eYHiMjufrTOCyE0apCpSpZ6JTeiREEmJ3eHqzqxWVDJ0uto4/uU1zxUnIl7Jo0Fk9erVpKSkkJKSAsDkyZNJSUnhkUce8eTL1qmF2w5zoqCUlhFBDO6gDe5EqlPRKvLBiv3YHW4OQL3wz8bXjR9DrptLxYtIg+DRIDJ06FCcTucZj9mzZ3vyZetUxb/qftsnXhvcibhhZI84IoP9ST9eyI87j7h3UWJ/SOgH9hJY9ZZnCxQRr6IxIlU4lH3qF+mNfdQtI+KO4ACba0PI99wdtAqnFjhb9Q6UFHigMhHxRgoiVfjPmgM4nDAgOYrWzbTBnYi7xgxIAmDRjsOkH3czVHQeCU1aQeFx2PChB6sTEW+iIHIODoeTj1drkKrI+WgTE8bF7ZvhdMKHK90cnG61wQX3GMe/vAEOh+cKFBGvoSByDiv2HifteAFhgX6M6N7S7HJEfE7FoNWPVqVTXGZ376KUWyEoEo7vhp3ferA6EfEWCiLn8EnFBnc9YwkJ8DO5GhHfc2mn5sRGBnE8v4RvNmW6d1FgGPS53TjWVF6RRkFB5Cxyikr5evMhQN0yIufLz2bl5v7GWJEaDVodcCdY/WD/Mji41kPViYi3UBA5iy82ZFBU6qB98zB6JTYxuxwRn/W7fon4WS2s2X+CrRlu7qYdEQfdbjCO1Soi0uApiJzF6YNUtcGdyPlrHhHEld2MMVbvr6jJVN4Jxtct8+Ck92x+KSJ1T0HkV3Zk5rIh/SR+VgvX9dYGdyK1dVv5oNV56w6SU1Tq3kWxPSB5MDjtsGKGB6sTEbMpiPxKxUqql3ZuTjNtcCdSawOSo2jfPIyCEjufrT3o/oUDy5d9XzMHirI9U5yImE5B5DQlZQ4+W2f8otQgVZG6YbFYXFN531u+H6fTzf1n2l0GzTpCSS6s/T8PVigiZlIQOc0P27M4nl9C8/BAhmiDO5E6c13veEICbKQezmPF3uPuXWS1nhorsuJNLXAm0kApiJzmo1UVG9wl4GfTWyNSVyKC/BmVYoy5qtFU3h6jITASstMh7WcPVSciZtKnbbnM7CKWuDa4SzC5GpGG59YBRvfM/M2ZHM4pcu8i/yDoOMI43v2DhyoTETMpiJT7z1pjg7v+raNoExNmdjkiDU6XuAj6tGpKmcPJ3FU1mJLb6kLja9oKzxQmIqZSEAGcTqdrSfcb+6o1RMRTKqbyfrgijTK7m2M+EgcYXw+uAbub039FxGcoiAAr9x5n37ECQgNs/KZ7rNnliDRYI7q3JCo0gMycIhZuP+zeRc06QFATKCuEzI0erU9E6p+CCKdWUh3ZM47QQG1wJ+IpgX42Rvczpsa/7+6gVasVEvsbx+krPVSZiJil0QeR3KJSvt5kbHB3o9YOEfG4W/onYbHAT7uOsudInnsXVXTPpC33XGEiYopGH0S+3HiIwlI7bWNC6Z3UxOxyRBq8xKgQLunYHIAPVqS5d1HSBcbX9BXg7oJoIuITGn0QqVjSfXQ/bXAnUl8qBq1+sjqdwhJ79RfE9QarH+QeMtYUEZEGo1EHkV1ZuaxLO4nNauG6FM2WEakvgzvEkBgVTE5RGV9szKj+goAQaNnDONY0XpEGpVEHkYrWkGGdmhMTrg3uROqLzWphTPkCZ24PWq0YJ5KuICLSkDTaIFJqd/Df8p1AR2uQqki9u7FPAgE2KxsPZLMh/WT1FyRVBBENWBVpSBptEFm47TDH8kuICQ9kaEdtcCdS36LDArmqh7Fuj1v7zySWD1jN2gLFuR6sTETqU6MNIhUrqV7fO14b3ImY5NbyQatfbMjgZEFJ1SdHxEJkEjgdcGB1PVQnIvWhUX4CZ+UUsWiHsarjjX3ULSNilt5JTegSG0FxmYNP1xyo/oIkjRMRaWgaZRBZsDULhxP6tmpKu+ba4E7ELBaLxdUq8v7y/Tgc1awRogGrIg1Oo1zPfMyAJLrFR7q/6ZaIeMy1veKY+vU29h0rYGnqUQZ3qGLMliuIrAKHHay2+ilSRDymUbaIWCwWeiU2oW/rKLNLEWn0QgP9+G0fYx2fagettugKAWFQkguHt9VDdSLiaY0yiIiId7n1giQAFm7L4khu8blPtNogoa9xrGm8Ig2CgoiImK5d83C6xUfgcMKSnUeqPrliGq9WWBVpEBRERMQrDCvfCG/R9sNVn5jY3/iqAasiDYKCiIh4haGdjCDy464jlFY1kDyhH1iscHI/5GbWU3Ui4ikKIiLiFXomNCEqNIDcojLW7D9x7hODIqB5V+NYrSIiPk9BRES8gs1qYUj51N2KBQfPqaJ7RuNERHyegoiIeI1LOrk5TiSpfMCqZs6I+DwFERHxGoPbN8NqgZ1ZeRw4UXDuEytaRA5tgNLC+ilORDxCQUREvEaTkAD6tGoKwOIdVUzjbdIKwlqCowwOrq2n6kTEExRERMSrDHVnGq/Fomm8Ig2EgoiIeJVh5eNElu0+SlGp/dwnusaJKIiI+DIFERHxKp1ahhMbGURRqYNlqUfPfWLiaUHEWc2uvSLitRRERMSrWCwWruzaEoAvNx4694ktu4NfEBSegKO76qk6EalrCiIi4nWu6RUHwHdbMs/dPeMXAPF9jGNN4xXxWQoiIuJ1UhKbEN8kmPwSe9WDVjVgVcTnKYiIiNexWCyM7Gm0inyxMePcJ2onXhGfpyAiIl5pZM9YABZuO0xuUenZT6poETm2C/KP1VNlIlKXFERExCt1iY2gTUwoxWUOvt+WdfaTQqKgWQfj+MDK+itOROqMgoiIeCWLxcLQDsaaIlszcs59YuIA42uaBqyK+CIFERHxWs0jAgE4ll9y7pMqgki6WkREfJGCiIh4rajQAACOVxVEKlZYzVgLZVWcJyJeSUFERLxWVIgbQSS6HQRHQVkRZG6sp8pEpK4oiIiI14oKcyOIWCwaJyLiwxRERMRrRbvTNQOQVDFOREFExNfUSxB5/fXXad26NUFBQQwYMICVKzWoTESq17Q8iBSU2Kveiff0AavaAE/Ep3g8iHz00UdMnjyZRx99lLVr19KzZ0+uvPJKDh+uYtlmEREgPNAPf5sFqGbmTFwKWP0hLwtO7Kuf4kSkTng8iLzwwgvccccd3H777XTp0oUZM2YQEhLCu+++6+mXFhEfZ7FYTs2cyasiiPgHQ1wv41jTeEV8ikeDSElJCWvWrOGyyy479YJWK5dddhm//PLLGecXFxeTk5NT6SEijVvTipkzBdWME0nUOBERX+TRIHL06FHsdjstWrSo9PMWLVqQmZl5xvlTp04lMjLS9UhMTPRkeSLiA6JdM2eKqz5RC5uJ+CSvmjUzZcoUsrOzXY/09HSzSxIRk0WFlq+uWlXXDJwKIllboCjbw1WJSF3xaBBp1qwZNpuNrKzKG1ZlZWXRsmXLM84PDAwkIiKi0kNEGreoEH8ATlTXNRPeApq2BpxwYJXH6xKRuuHRIBIQEECfPn1YuHCh62cOh4OFCxcycOBAT760iDQQFS0i1a4lApBYvty7umdEfIbHu2YmT57MW2+9xZw5c9i2bRt33303+fn53H777Z5+aRFpAKJCjRYR94JIf+OrVlgV8Rl+nn6B0aNHc+TIER555BEyMzPp1asX33777RkDWEVEzqZGLSIVG+AdXAP2MrB5/FeciNRSvfxfOnHiRCZOnFgfLyUiDUzTmrSIxHSCwAgozoHDWyC2p4erE5Ha8qpZMyIivxZdkxYRqw0S+hnHaSs8WJWI1BUFERHxahUtIicLS7E73NhHpqJ7Jl1BRMQXKIiIiFerWFnV6YST1U3hhVMDVhVERHyCgoiIeDV/m5WIIGM4W7VriQDE9wWLFbLTIfugh6sTkdpSEBERrxcd5ubqqgCBYdCim3GsVhERr6cgIiJer6m7q6tWSNLCZiK+QkFERLyea78Zd2bOgHbiFfEhCiIi4vUqVlc9UdMgcmgjlOR7qCoRqQsKIiLi9WrcItIkESLiwWmHg2s9WJmI1JaCiIh4vRq3iMBp03jVPSPizRRERMTr1bhFBLQTr4iPUBAREa/nahFxd9YMVF7YzOHwQFUiUhcURETE67l24HVnHZEKLbuDfwgUZcPRHR6qTERqS0FERLxeVPky78dr0iJi84f4PsaxFjYT8VoKIiLi9aLCjCBSVOqgoKTM/QsrpvFqJ14Rr6UgIiJeLzTARoDN+HV1vCYDVrUTr4jXUxAREa9nsViICjVaRU7kl7p/YUJf4+vx3ZB3xAOViUhtKYiIiE9oWh5EjuUXu39RcFOI6Wwcq1VExCspiIiIT4iuaBGpyYBVgKSKfWcURES8kYKIiPgEV4tITabwwmkb4CmIiHgjBRER8Qnn3SJSEUQy1kFZDbp1RKReKIiIiE+oGKxao1kzAFFtIKQZ2EsgY33dFyYitaIgIiI+oen5BhGLRdN4RbyYgoiI+ITo8w0ioHEiIl5MQUREfELTkDoIImnLwemsw6pEpLYURETEJ0SH1SKIxPUCWwAUHIXje+q2MBGpFQUREfEJFS0iJwtLsTtq2KrhFwhxKcaxumdEvIqCiIj4hKYh/oDRs1Kj1VUraJyIiFdSEBERn+Bns9KueRgA/16RXvMbVMyc0U68Il5FQUREfMa9l7YHYOaPuzmaV8NWkYT+xtcj26DwRB1XJiLnS0FERHzGVd1j6ZEQSX6JnVcX7qrZxWExENXWOD6wuu6LE5HzoiAiIj7DarXw8IhOAHywIo19R/NrdoPTp/GKiFdQEBERn3Jh22YM6RBDmcPJ89/tqNnF2olXxOsoiIiIz3loeCcsFvhy4yE2pJ90/8LE8gGrB9eAvdQjtYlIzSiIiIjP6RIXwXW94gF4+pvtON1dLbVZBwiKhNICyNrswQpFxF0KIiLikyZf0YEAm5Vf9hxjyc4j7l1ktZ42TkTdMyLeQEFERHxSQtMQfj+wFWC0ijjcXW01sXwab7oGrIp4AwUREfFZEy5pR3iQH9szc5m3/qB7F1WME0lf6bnCRMRtCiIi4rOahgZwz9B2AEz7bidFpfbqL4rvAxYb5ByEk+exQquI1CkFERHxabcPak3LiCAOnizk/eX7q78gIARiexjHmsYrYjoFERHxaUH+NiZf3gGA1xalkl3oxrRcV/eMgoiI2RRERMTnXd87nvbNwzhZUMqMJburv6BiwKpWWBUxnYKIiPg8P5uVh4YbS7+/u3Qvh7ILq76gYiferM1QnOfh6kSkKgoiItIgXNq5Of1aN6W4zMFLC6rZEC8iDiKTwOmAg9oAT8RMCiIi0iBYLBYeHtEZgE/WpLMrK7fqC1zriWgar4iZFEREpMHo06opw7u2xOGEZ77dXvXJFd0zGiciYioFERFpUB4c3hGb1cL32w6zcu/xc59Y0SJyYBU43Fh/REQ8QkFERBqUtjFhjO6XCMDUb7ade0O85l0hIAyKc+BINa0nIuIxCiIi0uBMurQ9wf421qWdZP6WrLOfZPMzVlkFdc+ImEhBREQanOYRQfzx4mQAnp2/nTK74+wnJmnfGRGzuyYVRESkQfrT4DZEhQaw50g+H68+cPaTEgcYX7UTrzQmpUWw+weY/3d4/QL47v+ZWo6fqa8uIuIh4UH+/HlYOx7/Yisvfr+TUSlxhAT86ldeQl/AAif2QW4WhLcwo1QRz3I64VgqpC6E1O9h31IoO33Rv3OMo6onCiIi0mCNGdCKd5ftJf14Ie/8tJc/X9q+8glBkdCiq7HCavoK6HKNOYWK1LWiHNi7pDx8LITstMrPh8dCu0uh7aXQZqgpJVbwWBB58skn+eqrr1i/fj0BAQGcPHnSUy8lInJWAX5WHriiI/fOXc+bP+7hlgFJRIcFVj4psb+CiPg+hwMyN5wKHgdWgqPs1PO2AGh1oRE82l0GzTuDxWJevafxWBApKSnhxhtvZODAgbzzzjueehkRkSqN7BHHWz/tYfPBHF79IZXHrula+YTEC2D1u9qJV3xP3hFjrEfq98bXgqOVn49udyp4tB4EAaHm1FkNjwWRxx9/HIDZs2d76iVERKpltVp4eHhnbn1nBR+s2M8fBiWTFB1y6oSk8gGrGeuhtBD8g02pU6Ra9lJjhlfq97B7IRzaUPn5gDBIHmJ0ubS7FJq2NqXMmtIYERFp8C5q34yL2zfjp11Hef67Hbxyc8qpJ5u0grAWkJdlhJFWA02rU+QMJ/ad6m7Z+yOU/GoPpZY9jBaPdpdCQn/wCzClzNrwqiBSXFxMcXGx6/ucnBwTqxGRhuSh4Z34addSPt+QwZ8Gt6FbfKTxhMUCCf1g+5fGTrwKImKmknzYt+xUq8ex1MrPhzSDtsOM8NH2Eghrbk6ddahGQeThhx/mmWeeqfKcbdu20alTp/MqZurUqa4uHRGRutQtPpJRveKYtz6Dp7/Zzvt/HHDqyfg+RhA5sNq8AqVxcjrh8NZTU2vTfgF7yannLTZjvZuK7paWPcHasJYAq1EQuf/++xk3blyV57Rp0+a8i5kyZQqTJ092fZ+Tk0NiYuJ5309E5HT3X9GRrzdlsjT1KD/uPMLgDjHGEwl9ja8H15hXnDQeBcdhz2IjfOxeCLmHKj8fmXQqeCQPNqaZN2A1CiIxMTHExMR4qhYCAwMJDAys/kQRkfOQGBXCrRcYa4s8/c12LmrXDKvVAnEpgAWy07WwmdQ9hx0Orj3V3XJwDThP23bALxhaX1QePi4zZrt4ydTa+uCxMSJpaWkcP36ctLQ07HY769evB6Bdu3aEhYV56mVFRKo0cVg7PlmdztZDOXy+IYNRKfEQGG6sq3B4qzFOpNNVZpcpvq44D3Z+a3T57V4ERScrPx/T+VSrR9KF4B9kSpnewGNB5JFHHmHOnDmu71NSjFHqixYtYujQoZ56WRGRKkWFBnDX0LY8N38Hz3+3gxHdWxLoZzPGiRzeaowTURCR81ERPrbOg10LoKzo1HNBkcYKpu0uMwabRiaYVaXX8VgQmT17ttYQERGv9IdBycz5eR8HThTy/vI0xl+UbIwTWfee0SIi4q6qwkdUG+gyCjoMN4KuzasmqnoNvSsi0ugEB9i47/IOTPnvJl77YRc39k0gIr5iwOo6o0/fajO3SPFexXmwaz5s+ezc4aPrddCye6Ma63G+FEREpFG6sU8Cb/+0h91H8nlzyW4evLwz+IcaC0Yd3WmMGRGp4Aof88rDx2m717rCxyhjgTGFjxpREBGRRsnPZuWh4Z3403treGfpXn4/sDUt4lJg/1JjnIiCiFQVPpomG60eCh+1piAiIo3W5V1a0KdVU9bsP8FL3+9kanxvI4gcXAO9bzO7PDGDwke9UxARkUbLYrEwZUQnbpjxCx+tSucv13QlFjRgtbEpyYedp4/5+HX4GFU+5kPhwxMURESkUevbOorLu7RgwdYsXtwWybMAWVuhpAACQqq7XHxVRfjYOg92fnf28NFlFMT2VPjwMAUREWn0HhrekYXbsvh4p4MnoloQUJAFh9ZDqwvNLk3qUpXho7XR6qHwUe8URESk0WvXPJyb+iYyd1U66+xtGUCWMWBVQcT3leTDru+MbpezhY+KqbYKH6ZREBERASZd1oF56w+yKC+JAf4/a5yIL3OFj3nG19KCU88pfHgdBREREaBlZBB/GJTM2h/bAeA8sBp9RPkQt8LHKIjtpfDhZRRERETK3TW0LcNXdMDusGDLOQi5mRDe0uyy5FyqCh9NWp2aaqvw4dUUREREykUE+TP+0h7s/C6BzpZ0ivetJLD7NWaXJacrKTg15uOs4WNUebdLL4UPH6EgIiJymlsvSGL+oo50tqezeeVC+iiImK8ifGydZ8x6OVv46DIK4lIUPnyQgoiIyGkC/WzEd70YNn6P5eAanE4nFn241T+Fj0ZDQURE5Fc697sENj5KB/su9h7OoU2LSLNLahxKCiB1QflU21+Hj6RT63wofDQoCiIiIr8SEt+NIksQYRTx3bqVtBl+udklNVyu8DGvPHzkn3quSdKpqbYKHw2WgoiIyK9ZbRyP7ErcyTUc2/EzKIjULbfCxyiI663w0QgoiIiInEVwcn9Yt4bwYxsoKCkjJEC/LmulqvARmVQ+22WUwkcjpP+zRETOokn7gbBuOj1IZdW+EwzpEGN2Sb6ntNDYzdY15kPhQ86kICIichaWhH4AdLSk8c32vQoi7qoIH1vnwY5vzxI+ri0f86HwIQYFERGRs4mIIyeiPRE5u2ixdRZc09/siryXO+Gjy3UQr/AhZ1IQERE5l8F/hS/v4JrCzzie9TeiWiSaXZH3KC2E1O9PdbuU5J16LjKxfJ0PhQ+pnoKIiMg5RPS+gZ3fPEMHeyppXz9C1O2zzC7JXNWFjy7XQtfrFT6kRhRERETOxWplafuH6LD9DpL2/xfS74bERtZF4wof82DntwofUucUREREqpDcaygfbx7CTX5L4OsH4I5FYLWZXZZnuRU+roP4PgofUmsKIiIiVeifHMUwx+8Y7lxFxKENsHYO9P2D2WXVvdKi07pdFD6k/iiIiIhUITTQj1ZJybyQfgOP+f8fLPynsfJnSJTZpdVeRfjYOg92fFM5fEQklK/zofAhnqUgIiJSjUHtmvHKvsu5I3Qp8YV74Id/wdUvml3W+akUPr6FktxTz1WEjy6jIKGvwofUCwUREZFqXNQ+mhe/t/FI6Vje4VFYPQt6/97YiM0XlBbB7oVGt4vCh3gZBRERkWr0SGhCWKAfCwvbc7LrKJrsngdfPwh/+A6sVrPLOzt3w0d8H+/9M0ijoCAiIlINf5uVAclRLNx+mC9a3M1t6d/DgVWw4d+QMsbs8k5xhY955WM+fhU+Th9wqvAhXkJBRETEDYPaNWPh9sN8l27htiEPwYJ/wPePQqerILiJeYVVGT7ijVaPrqMgvq/Ch3glBRERETdc1L4ZAKv2HadozB0ErXsPju6ExU/DiKfrt5jSItj9Q3m3i8KH+DYFERERN7RvHkZMeCBHcotZezCfC0c8C++NgpUzofdt0KKr517c4YCT+yFzI2z/CrZ/rfAhDYaCiIiIGywWCxe1a8Zn6w6yLPUoF155iTHmYuv/jIGr476q3YwTpxNK8qHgKBzZCUe2weHtxtcjO6C0oPL5EfGnjflQ+BDfpSAiIuKmQa4gcowHrwSueBJ2fgf7l8GaWdD5GijKNh7FOeXHOWd+7zo+/dwccNrP/eK2QGjWAZIvLp9q20/hQxoEBRERETcNahcNwMYDJ8kuLCWySSIMvh9+eAK+vM941JbVH5q1h5hO0Lzzqa9Nk8GmX9nS8OhvtYiIm2Ijg2kTE8qeI/ks33OMK7u2hIF/NlpFDqw0TgoIh6AICIqEwIhfHUca37uOI888zz9Ei4pJo6IgIiJSAxe1a8aeI/ksSz1qBBH/IPjjAigrMXblbeg784rUMXUwiojUwKB2xjTepalHKz/hF6AQInIeFERERGrggjbRWC2w50g+GScLzS5HxOcpiIiI1EBksD89EpoAsOzXrSIiUmMKIiIiNVQxe+bn3cdMrkTE9ymIiIjU0OnjRJxOp8nViPg2BRERkRrqndSUIH8rR3KL2XU4z+xyRHyagoiISA0F+dvo1zoKgKW7NE5EpDYUREREzsNF5d0zGrAqUjsKIiIi56FinMjyPccotTtMrkbEdymIiIichy6xETQN8Se/xM7GAyfNLkfEZymIiIicB6vVwoVty2fP7NI0XpHzpSAiInKeBmmciEitKYiIiJynigGra9NOkF9cZnI1Ir5JQURE5DwlRYeQGBVMmcPJyr3HzS5HxCcpiIiI1MJF59qNV0Tc4rEgsm/fPsaPH09ycjLBwcG0bduWRx99lJKSEk+9pIhIvdM4EZHa8fPUjbdv347D4eDNN9+kXbt2bN68mTvuuIP8/Hyef/55T72siEi9GtjG2ABve2YuR3KLiQkPNLkiEd/isSAyfPhwhg8f7vq+TZs27Nixg+nTpyuIiEiDER0WSJfYCLYeyuHn3Ue5tle82SWJ+JR6HSOSnZ1NVFTUOZ8vLi4mJyen0kNExNtd1F7dMyLnq96CSGpqKq+++ip33nnnOc+ZOnUqkZGRrkdiYmJ9lScict4qxoks3XUUp9NpcjUivqXGQeThhx/GYrFU+di+fXulaw4ePMjw4cO58cYbueOOO8557ylTppCdne16pKen1/xPJCJSz/q1bkqAzUpGdhH7jhWYXY6IT6nxGJH777+fcePGVXlOmzZtXMcZGRlccsklXHjhhcycObPK6wIDAwkM1EAvEfEtIQF+9G7VhOV7jrM09SjJzULNLknEZ9Q4iMTExBATE+PWuQcPHuSSSy6hT58+zJo1C6tVy5aISMM0qG0zlu85zs+pR7ntglZmlyPiMzyWDA4ePMjQoUNJSkri+eef58iRI2RmZpKZmemplxQRMc2g8gGrP+8+ht2hcSIi7vLY9N0FCxaQmppKamoqCQkJlZ7TYC4RaWh6xEcSHuhHdmEpWzKy6ZHQxOySRHyCx1pExo0bh9PpPOtDRKSh8bNZuaCtsbiZlnsXcZ8GbYiI1JGLtNy7SI0piIiI1JGK9URW7TtBUand5GpEfIPHxoiIiDQ2bWNCiY0M4lB2Ede8ttQVTES8WduYMG41caaXgoiISB2xWCzc2CeBV35IZWdWHjuz8swuSaRagzvEKIiIiDQUdw9tR3CAH3nFpWaXIuKW1tHmLsCnICIiUoeCA2zcPbSt2WWI+AwNVhURERHTKIiIiIiIaRRERERExDQKIiIiImIaBRERERExjYKIiIiImEZBREREREyjICIiIiKmURARERER0yiIiIiIiGkURERERMQ0CiIiIiJiGgURERERMY1X777rdDoByMnJMbkSERERcVfF53bF53hVvDqI5ObmApCYmGhyJSIiIlJTubm5REZGVnmOxelOXDGJw+EgIyOD8PBwLBZLnd47JyeHxMRE0tPTiYiIqNN7NwR6f85N703V9P5UTe9P1fT+nJsvvTdOp5Pc3Fzi4uKwWqseBeLVLSJWq5WEhASPvkZERITX/wc1k96fc9N7UzW9P1XT+1M1vT/n5ivvTXUtIRU0WFVERERMoyAiIiIipmm0QSQwMJBHH32UwMBAs0vxSnp/zk3vTdX0/lRN70/V9P6cW0N9b7x6sKqIiIg0bI22RURERETMpyAiIiIiplEQEREREdMoiIiIiIhpGmUQef3112ndujVBQUEMGDCAlStXml2S1/jxxx8ZOXIkcXFxWCwW5s2bZ3ZJXmPq1Kn069eP8PBwmjdvzqhRo9ixY4fZZXmN6dOn06NHD9diSwMHDuSbb74xuyyv9PTTT2OxWJg0aZLZpXiFxx57DIvFUunRqVMns8vyKgcPHuTWW28lOjqa4OBgunfvzurVq80uq040uiDy0UcfMXnyZB599FHWrl1Lz549ufLKKzl8+LDZpXmF/Px8evbsyeuvv252KV5nyZIlTJgwgeXLl7NgwQJKS0u54ooryM/PN7s0r5CQkMDTTz/NmjVrWL16NcOGDePaa69ly5YtZpfmVVatWsWbb75Jjx49zC7Fq3Tt2pVDhw65HkuXLjW7JK9x4sQJBg0ahL+/P9988w1bt25l2rRpNG3a1OzS6oazkenfv79zwoQJru/tdrszLi7OOXXqVBOr8k6A87PPPjO7DK91+PBhJ+BcsmSJ2aV4raZNmzrffvtts8vwGrm5uc727ds7FyxY4BwyZIjz3nvvNbskr/Doo486e/bsaXYZXuuhhx5yXnTRRWaX4TGNqkWkpKSENWvWcNlll7l+ZrVaueyyy/jll19MrEx8UXZ2NgBRUVEmV+J97HY7c+fOJT8/n4EDB5pdjteYMGECV111VaXfQWLYtWsXcXFxtGnThjFjxpCWlmZ2SV7j888/p2/fvtx44400b96clJQU3nrrLbPLqjONKogcPXoUu91OixYtKv28RYsWZGZmmlSV+CKHw8GkSZMYNGgQ3bp1M7scr7Fp0ybCwsIIDAzkrrvu4rPPPqNLly5ml+UV5s6dy9q1a5k6darZpXidAQMGMHv2bL799lumT5/O3r17ufjii8nNzTW7NK+wZ88epk+fTvv27Zk/fz533303f/nLX5gzZ47ZpdUJr959V8RbTZgwgc2bN6sf+1c6duzI+vXryc7O5tNPP2Xs2LEsWbKk0YeR9PR07r33XhYsWEBQUJDZ5XidESNGuI579OjBgAEDaNWqFR9//DHjx483sTLv4HA46Nu3L0899RQAKSkpbN68mRkzZjB27FiTq6u9RtUi0qxZM2w2G1lZWZV+npWVRcuWLU2qSnzNxIkT+fLLL1m0aBEJCQlml+NVAgICaNeuHX369GHq1Kn07NmTl19+2eyyTLdmzRoOHz5M79698fPzw8/PjyVLlvDKK6/g5+eH3W43u0Sv0qRJEzp06EBqaqrZpXiF2NjYM8J8586dG0z3VaMKIgEBAfTp04eFCxe6fuZwOFi4cKH6saVaTqeTiRMn8tlnn/HDDz+QnJxsdklez+FwUFxcbHYZprv00kvZtGkT69evdz369u3LmDFjWL9+PTabzewSvUpeXh67d+8mNjbW7FK8wqBBg85YKmDnzp20atXKpIrqVqPrmpk8eTJjx46lb9++9O/fn5deeon8/Hxuv/12s0vzCnl5eZX+FbJ3717Wr19PVFQUSUlJJlZmvgkTJvDhhx/yv//9j/DwcNe4osjISIKDg02uznxTpkxhxIgRJCUlkZuby4cffsjixYuZP3++2aWZLjw8/IyxRKGhoURHR2uMEfDAAw8wcuRIWrVqRUZGBo8++ig2m42bb77Z7NK8wn333ceFF17IU089xU033cTKlSuZOXMmM2fONLu0umH2tB0zvPrqq86kpCRnQECAs3///s7ly5ebXZLXWLRokRM44zF27FizSzPd2d4XwDlr1iyzS/MKf/jDH5ytWrVyBgQEOGNiYpyXXnqp87vvvjO7LK+l6bunjB492hkbG+sMCAhwxsfHO0ePHu1MTU01uyyv8sUXXzi7devmDAwMdHbq1Mk5c+ZMs0uqMxan0+k0KQOJiIhII9eoxoiIiIiId1EQEREREdMoiIiIiIhpFERERETENAoiIiIiYhoFERERETGNgoiIiIiYRkFERERETKMgIiIiIqZREBERERHTKIiIiIiIaRRERERExDT/H3c5IcTlN+ysAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_t[ind],y_t[ind], label = 'unregularized')\n",
    "plt.plot(X_t2[ind2],y_t2[ind2], label = 'regularized')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a480cb37",
   "metadata": {},
   "source": [
    "We see the regularizer does penalize the graph from taking values greater than 2. This type of regularization will also have side effects on the rest of graph, because the model is quite simple and there is no straightforward connection between the function value being higher than 2 and the network weights. This could be improved by tailoring the model architecture possibly in conjuction with a suitable regularizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
