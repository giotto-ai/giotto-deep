{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: Question answering\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial provides you with the basic functioning of *giotto-deep* API.\n",
    "\n",
    "## Scope\n",
    "\n",
    "The example described in this tutorial is the one of question answering. A trained model would be able to **find** the answer inside a given *context*. Hence, we are not building models that can generate new sentences to answer an abstract question: rather, our models read a text (a.k.a. *context*) and try to answer a given question based on the information found in the context.\n",
    "\n",
    "## Content\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. Creation of a dataset\n",
    " 2. Creation of a model\n",
    " 3. Define metrics and losses\n",
    " 4. Train the model\n",
    " 5. Try to answer a question\n",
    " 6. Extract some features of the network to study the attention maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPUs...\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import copy\n",
    "\n",
    "from torch.nn import Transformer\n",
    "from torch.optim import Adam, SparseAdam, SGD\n",
    "import numpy as np\n",
    "from gtda.diagrams import BettiCurve\n",
    "from gtda.plotting import plot_betti_surfaces\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from gdeep.models import FFNet\n",
    "from gdeep.visualization import persistence_diagrams_of_activations\n",
    "from gdeep.data.datasets import DatasetBuilder\n",
    "from gdeep.trainer import Trainer\n",
    "from gdeep.models import ModelExtractor\n",
    "from gdeep.utility import DEVICE\n",
    "from gdeep.data import PreprocessingPipeline\n",
    "from gdeep.data import TransformingDataset\n",
    "from gdeep.data.preprocessors import Normalization, TokenizerQA\n",
    "from gdeep.data.datasets import DataLoaderBuilder\n",
    "from gdeep.visualization import Visualiser\n",
    "from gdeep.search import GiottoSummaryWriter\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to visualize and analyze the results of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/examples` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualization results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = GiottoSummaryWriter()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset\n",
    "\n",
    "In giotto-deep one can write a few lines to get the most famous datasets: in the next cell you will see how simple it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd = DatasetBuilder(name=\"SQuAD2\", convert_to_map_dataset=True)\n",
    "ds_tr_str, ds_val_str, ds_ts_str = bd.build()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An item of the dataset contains a context and a question whose answer can be found within that context. The correct answer as well as the starting token are also provided: check the output of the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before preprocessing: \n",
      " ('Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'When did Beyonce start becoming popular?', ['in the late 1990s'], [269])\n"
     ]
    }
   ],
   "source": [
    "print(\"Before preprocessing: \\n\", ds_tr_str[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required preprocessing\n",
    "\n",
    "Neural networks cannot direcly deal with strings. We first have to preprocess the dataset in three main ways:\n",
    " 1. Tokenise the strings into its words\n",
    " 2. Build a vocabulary out of these words\n",
    " 3. Embed each word into a vector, so that each sentence becomes a list of vectors\n",
    "\n",
    "The first two steps are performed by the `TokenizerQA`. The embedding will be added directly as layers to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After the preprocessing: \n",
      " ([tensor([  704, 26475, 38619,    13, 38620, 38621,    12,    13,   935,   354,\n",
      "          266,     2,  3117,    12,     9,    30,   119,  3243,     2, 11315,\n",
      "            2,   473,  2344,     5,  5209,     3,   935,     5,  1414,     6,\n",
      "          918,     2,  1515,     2,   221,  1185,     6,   265,  3456,     5,\n",
      "         8258,  3257,    10,     8,   923,     2,     5,  1895,     7,  3593,\n",
      "            6,     1,   211,  1306,    10,   854,  3243,     4,  4750, 38622,\n",
      "         4470,    14,    19,   923,     3,  1985,    17,   105,   714,     2,\n",
      "        14784, 11381,     2,     1,   143,   100,    44,     4,     1,    70,\n",
      "           14,    19,  4624,  5250,   213,     4,    61,    71,     3,    39,\n",
      "         9009,   602,     1,   719,     4,   704,    14,    19,  3317,   583,\n",
      "            2, 11248,     6,  1486,    13,   948,    12,     2,    25,   212,\n",
      "          105,    10,     8,  3143,  1626,   944,     2,  3003,   316,  4736,\n",
      "         1410,     5,  1437,     1,  2746,  1325,   569,  6130,  3118,  7619,\n",
      "            6,  1486,     5,  8017,  3187,     3,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0], device='cuda:0'), tensor([  53,   50, 5363,  734, 1064,  323,   15,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "       device='cuda:0')], tensor([50, 54]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = TokenizerQA()\n",
    "\n",
    "# in case you need to combine multiple preprocessing:\n",
    "# ppp = PreprocessingPipeline(((PreprocessTextData(), IdentityTransform(), TextDataset),\n",
    "#                             (Normalisation(), IdentityTransform(), BasicDataset)))\n",
    "\n",
    "\n",
    "tokenizer.fit_to_dataset(ds_tr_str)\n",
    "transformed_textds = tokenizer.attach_transform_to_dataset(ds_tr_str)\n",
    "\n",
    "transformed_textts = tokenizer.attach_transform_to_dataset(\n",
    "    ds_val_str\n",
    ")  # this has been fitted on the train set!\n",
    "\n",
    "print(\"After the preprocessing: \\n\", transformed_textds[0])\n",
    "\n",
    "# the only part of the training/test set we are interested in\n",
    "train_indices = list(range(64 * 2))\n",
    "test_indices = list(range(64 * 1))\n",
    "\n",
    "dl_tr2, dl_ts2, _ = DataLoaderBuilder((transformed_textds, transformed_textts)).build(\n",
    "    (\n",
    "        {\"batch_size\": 16, \"sampler\": SubsetRandomSampler(train_indices)},\n",
    "        {\"batch_size\": 16, \"sampler\": SubsetRandomSampler(test_indices)},\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train your model\n",
    "\n",
    "The model for QA shall accept as input the context and the question and return the probabilities for the initial and final token of the answer in the input context. The output then, shall be a pair of logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# my simple transformer model\n",
    "class QATransformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim):\n",
    "        super(QATransformer, self).__init__()\n",
    "        self.transformer = Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=16,\n",
    "            num_encoder_layers=12,\n",
    "            num_decoder_layers=12,\n",
    "            dim_feedforward=512,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        self.embedding_src = nn.Embedding(src_vocab_size, embed_dim, sparse=True)\n",
    "        self.embedding_tgt = nn.Embedding(tgt_vocab_size, embed_dim, sparse=True)\n",
    "        self.generator = nn.Linear(embed_dim, 2)\n",
    "\n",
    "    def forward(self, ctx, qst):\n",
    "        # print(src.shape, tgt.shape)\n",
    "        ctx_emb = self.embedding_src(ctx).permute(1, 0, 2)\n",
    "        qst_emb = self.embedding_tgt(qst).permute(1, 0, 2)\n",
    "        # print(src_emb.shape, tgt_emb.shape)\n",
    "        self.outs = self.transformer(qst_emb, ctx_emb).permute(1, 0, 2)\n",
    "        # print(outs.shape)\n",
    "        logits = self.generator(self.outs)\n",
    "        return logits\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        \"\"\"this is needed to make sure that the \n",
    "        non-leaf nodes do not\n",
    "        interfere with copy.deepcopy()\n",
    "        \"\"\"\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        memo[id(self)] = result\n",
    "        for k, v in self.__dict__.items():\n",
    "            setattr(result, k, copy.deepcopy(v, memo))\n",
    "        return result\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"this method is used only at the inference step\"\"\"\n",
    "        return self.transformer.encoder(self.embedding_src(src), src_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        \"\"\"this method is used only at the inference step\"\"\"\n",
    "        return self.transformer.decoder(self.embedding_tgt(tgt), memory, tgt_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QATransformer(\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (3): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (4): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (5): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (6): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (7): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (8): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (9): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (10): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (11): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (3): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (4): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (5): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (6): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (7): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (8): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (9): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (10): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (11): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (embedding_src): Embedding(102156, 2048, sparse=True)\n",
      "  (embedding_tgt): Embedding(102156, 2048, sparse=True)\n",
      "  (generator): Linear(in_features=2048, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(tokenizer.vocabulary)\n",
    "tgt_vocab_size = len(tokenizer.vocabulary)\n",
    "emb_size = 2048\n",
    "\n",
    "model = QATransformer(src_vocab_size, tgt_vocab_size, emb_size)\n",
    "print(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "This loss function is a adapted version of the Cross Entropy for the transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(output_of_network, label_of_dataloader):\n",
    "    # print(output_of_network.shape, label_of_dataloader.shape)\n",
    "    tgt_out = label_of_dataloader\n",
    "    logits = output_of_network\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    return cel(logits, tgt_out)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!\n",
    "\n",
    "We are finally there! We have defined the model, transformed the dataset so that it is manageable by standard layers and we have also adapted the loss function. We are ready to start the training: in giotto-deep, it is a matter of a few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from _init_profiler: pid = 2608136, profiling = False\n",
      "from train: PID=2608136, self.device=cuda\n",
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 23.70 GiB total capacity; 304.25 MiB already allocated; 13.00 MiB free; 318.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m pipe \u001b[39m=\u001b[39m Trainer(model, (dl_tr2, dl_ts2), loss_fn, writer)\n\u001b[1;32m      4\u001b[0m \u001b[39m# train the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m pipe\u001b[39m.\u001b[39;49mtrain(SGD, \u001b[39m3\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m, {\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0.01\u001b[39;49m}, {\u001b[39m\"\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m16\u001b[39;49m})\n",
      "File \u001b[0;32m~/Repositories/giotto-deep/gdeep/trainer/trainer.py:1116\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, optimizer, n_epochs, cross_validation, optimizers_param, dataloaders_param, lr_scheduler, scheduler_params, optuna_params, profiling, parallel_tpu, keep_training, store_grad_layer_hist, n_accumulated_grads, writer_tag, parallel)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfrom train: PID=\u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39mgetpid()\u001b[39m}\u001b[39;00m\u001b[39m, self.device=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1115\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parallel_tpu:\n\u001b[0;32m-> 1116\u001b[0m     valloss, valacc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_training_loops(\n\u001b[1;32m   1117\u001b[0m         n_epochs,\n\u001b[1;32m   1118\u001b[0m         dl_tr,\n\u001b[1;32m   1119\u001b[0m         dl_val,\n\u001b[1;32m   1120\u001b[0m         lr_scheduler,\n\u001b[1;32m   1121\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscheduler,\n\u001b[1;32m   1122\u001b[0m         check_optuna,\n\u001b[1;32m   1123\u001b[0m         search_metric,\n\u001b[1;32m   1124\u001b[0m         trial,\n\u001b[1;32m   1125\u001b[0m         \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1126\u001b[0m         writer_tag,\n\u001b[1;32m   1127\u001b[0m     )\n\u001b[1;32m   1128\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     valloss, valacc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_tpu_training_loops(\n\u001b[1;32m   1130\u001b[0m         n_epochs,\n\u001b[1;32m   1131\u001b[0m         dl_tr,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1140\u001b[0m     )\n",
      "File \u001b[0;32m~/Repositories/giotto-deep/gdeep/trainer/trainer.py:1202\u001b[0m, in \u001b[0;36mTrainer._training_loops\u001b[0;34m(self, n_epochs, dl_tr, dl_val, lr_scheduler, scheduler, check_optuna, search_metric, trial, cross_validation, writer_tag)\u001b[0m\n\u001b[1;32m   1200\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_epoch \u001b[39m=\u001b[39m t\n\u001b[1;32m   1201\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_epoch \u001b[39m=\u001b[39m t\n\u001b[0;32m-> 1202\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_loop(dl_tr, writer_tag)\n\u001b[1;32m   1203\u001b[0m me \u001b[39m=\u001b[39m ModelExtractor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn)\n\u001b[1;32m   1204\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstore_grad_layer_hist:\n",
      "File \u001b[0;32m~/Repositories/giotto-deep/gdeep/trainer/trainer.py:514\u001b[0m, in \u001b[0;36mTrainer._train_loop\u001b[0;34m(self, dl_tr, writer_tag)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"private method to run a single training\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[39mloop\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39m# Once setup, FSDP handles the model's device\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    515\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m    516\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Repositories/giotto-deep/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    987\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 989\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/Repositories/giotto-deep/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Repositories/giotto-deep/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 641 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Repositories/giotto-deep/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Repositories/giotto-deep/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 664\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    665\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    666\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/Repositories/giotto-deep/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    985\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 987\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 23.70 GiB total capacity; 304.25 MiB already allocated; 13.00 MiB free; 318.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# prepare a pipeline class with the model, dataloaders loss_fn and tensorboard writer\n",
    "pipe = Trainer(model, (dl_tr2, dl_ts2), loss_fn, writer)\n",
    "\n",
    "# train the model\n",
    "pipe.train(SGD, 3, False, {\"lr\": 0.01}, {\"batch_size\": 16})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering questions!\n",
    "\n",
    "Here we have a question and its associated context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = next(iter(ds_val_str))\n",
    "bb[:2]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Get the vocabulary and numericize the question and context to then input both to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocabulary and tokenizer\n",
    "voc = tokenizer.vocabulary\n",
    "context = tokenizer.tokenizer(bb[0])\n",
    "question = tokenizer.tokenizer(bb[1])\n",
    "\n",
    "# get the indexes in the vocabulary of the tokens\n",
    "context_idx = torch.tensor(list(map(voc.__getitem__, context)))\n",
    "question_idx = torch.tensor(list(map(voc.__getitem__, question)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = next(iter(dl_tr2))\n",
    "pad_fn = lambda length_to_pad, item: torch.cat(\n",
    "    [item, tokenizer.pad_item * torch.ones(length_to_pad - item.shape[0])]\n",
    ").to(torch.long)\n",
    "\n",
    "# these tensors are ready to be fitted into the model\n",
    "length_to_pad = aa[0][0].shape[-1]  # context length\n",
    "context_ready_for_model = pad_fn(length_to_pad, context_idx)\n",
    "length_to_pad = aa[0][1].shape[-1]  # question length\n",
    "question_ready_for_model = pad_fn(length_to_pad, question_idx)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the two tensors of context and question together and input them to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = [context_ready_for_model.view(1, -1).to(DEVICE), \n",
    "              question_ready_for_model.view(1,-1).to(DEVICE)]\n",
    "\n",
    "out = pipe.model(*input_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output corresponds to the digits for the start and end tokens of the answer. It is now time to extract them with `torch.argmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_idx = torch.argmax(out, dim=1)\n",
    "\n",
    "# simple code to convert the model's answer into words\n",
    "try:\n",
    "    if answer_idx[0][1] > answer_idx[0][0]:\n",
    "        print(\n",
    "            \"The model proposes: '\",\n",
    "            \" \".join(context[answer_idx[0][0] : answer_idx[0][1]]),\n",
    "            \"...'\",\n",
    "        )\n",
    "    else:\n",
    "        print(\"The model proposes: '\", context[answer_idx[0][0]], \"...'\")\n",
    "except IndexError:\n",
    "    print(\"The model was not able to find the answer.\")\n",
    "print(\"The actual answer was: '\" + bb[2][0] + \"'\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract inner data from your models\n",
    "\n",
    "In this section we are extracting, for the same input as above, the attention maps.\n",
    "\n",
    "Such matrices are creating a relationsip between the question and the context, highlighting the words that most captured the transformer attention. Such maps are really useful to interpret the model results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the model extractor\n",
    "ex = ModelExtractor(pipe.model, loss_fn)\n",
    "\n",
    "# getting the names of the layers\n",
    "layer_names = ex.get_layers_param().keys()\n",
    "\n",
    "print(\"Let's extract the activations of the first attention layer: \", next(iter(layer_names)))\n",
    "self_attention = ex.get_activations(input_list)[-5:-3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the tensor! First, load th visualizer\n",
    "vs = Visualiser(pipe)\n",
    "vs.plot_self_attention(self_attention, context, question, figsize=(20, 20));\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "If you have trained the model with very few epochs and only a small subset of data, you would have probably obtained almost random results: Can you improve the model and interpret these attention maps?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise your model interactively\n",
    "\n",
    "One final note about visualising and inspecting transformer models: it is possible, in giotto-deep, to plot an interactive model graph in tensdorboard, so that you can eviscerate the inner working of the transformer visually and demistify these powerful models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.visualization import Visualiser\n",
    "\n",
    "vs = Visualiser(pipe)\n",
    "\n",
    "vs.plot_interactive_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
