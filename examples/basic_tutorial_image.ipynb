{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: image data\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial shows you how to use the basic functioning of *giotto-deep* API for an imagee classfication task.\n",
    "\n",
    "Image classifcation is about associating a picture with a label: for example, you would like your model to be able to associate a label `cat` to the image of a cat and a label `dog`to an image of a dog:\n",
    "\n",
    "![img](./images/image_class.png)\n",
    "\n",
    "It is important to clarify that the set of labels has to be pre-defined and the modls have to be trained with a sufficient amount of data per label.\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. creation of a dataset\n",
    " 2. creation of a model\n",
    " 3. define metrics and losses\n",
    " 4. train your model\n",
    " 5. evaluate your model performances\n",
    " 6. use interpretability tools like heatmaps on the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.models as models\n",
    "from gtda.diagrams import BettiCurve\n",
    "from gtda.plotting import plot_betti_surfaces\n",
    "\n",
    "from gdeep.data.datasets import DatasetBuilder, DataLoaderBuilder\n",
    "from gdeep.models import FFNet\n",
    "from gdeep.visualization import persistence_diagrams_of_activations\n",
    "from gdeep.data.preprocessors import ToTensorImage\n",
    "from gdeep.trainer import Trainer\n",
    "from gdeep.models import ModelExtractor\n",
    "from gdeep.analysis.interpretability import Interpreter\n",
    "from gdeep.visualization import Visualiser\n",
    "from gdeep.search import GiottoSummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the results of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/examples` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training of your model to see all the visualization results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = GiottoSummaryWriter()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset\n",
    "\n",
    "In this example we will be using the `CIFAR10` dataset, which is a dataset of many thousands of images with ten classes.\n",
    "\n",
    "In giotto-deep it is enough to call `DatasetBuilder(name=\"CIFAR10\")` to get the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DatasetBuilder(name=\"CIFAR10\")\n",
    "ds_tr, ds_val, ds_ts = db.build()\n",
    "NUMBER_OF_CLASSES = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The preprocessing steps consist in transformin the imagees to tensors. We have already provided a simple class to do such preprocessing: `ToTensorImage`.\n",
    "\n",
    "The transormation is computed on the fly, hence bring very light on the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "\n",
    "transformation = ToTensorImage((32, 32))\n",
    "transformation.fit_to_dataset(\n",
    "    ds_tr\n",
    ")  # this is useless for this transformation, but in general this is the API\n",
    "\n",
    "transformed_ds_tr = transformation.attach_transform_to_dataset(ds_tr)\n",
    "transformed_ds_val = transformation.attach_transform_to_dataset(ds_val)\n",
    "transformed_ds_ts = transformation.attach_transform_to_dataset(ds_ts)\n",
    "\n",
    "# use only 320 images from cifar10 for training\n",
    "train_indices = list(range(32 * 10))\n",
    "val_indices = list(range(32 * 5))\n",
    "test_indices = list(range(32 * 5))\n",
    "dl_tr, dl_val, dl_ts = DataLoaderBuilder(\n",
    "    (transformed_ds_tr, transformed_ds_val, transformed_ds_ts)\n",
    ").build(\n",
    "    (\n",
    "        {\"batch_size\": 32, \"sampler\": SubsetRandomSampler(train_indices)},\n",
    "        {\"batch_size\": 32, \"sampler\": SubsetRandomSampler(val_indices)},\n",
    "        {\"batch_size\": 32, \"sampler\": SubsetRandomSampler(test_indices)},\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train your model\n",
    "\n",
    "In the next cell we show how simple it is to build a model from `torch`, and use it for training in Giotto-deep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We have prepard te dataset, we have prepared the model... it is now tim to train it!\n",
    "\n",
    "In giotto-deep it is just a matter of initialising the `Trainer` and run it with `train`. All the outputs, the exceptions, cross validation... everything you is just a parameter for the trainer. Have a look at the next cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# initilise the trainer class\n",
    "pipe = Trainer(model, (dl_tr, dl_ts), loss_fn, writer)\n",
    "\n",
    "# train the model\n",
    "pipe.train(\n",
    "    SGD,\n",
    "    3,\n",
    "    False,\n",
    "    {\"lr\": 0.01},\n",
    "    {\"batch_size\": 32, \"sampler\": SubsetRandomSampler(train_indices)},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model\n",
    "\n",
    "In the next section we compute the confusion matrix on the entire training dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.evaluate_classification(NUMBER_OF_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simply use interpretability tools\n",
    "\n",
    "After th training we would like to inspect what the model has learned. This is possible thanks to interpretability tools.\n",
    "\n",
    "Here below we show you how simple it is, in giotto-deep, to use many interpretabnility tools in one single line of code.\n",
    "\n",
    "The example below uses the `GuidedGradCAM` algorithm: given an input `datum`, we compute the gradients of `loss(datum, class_)` via backpropagation of the specified layer `conv2` and then averages over the chanels of such convolutional layer.\n",
    "\n",
    "The result is then displayed as superimposition of the initial image, with a red/green hue to identify which pixes are pushing for and against the classification of teh given image into the selected `class`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the interpreter\n",
    "inter = Interpreter(pipe.model, method=\"GuidedGradCam\")\n",
    "\n",
    "# define a signle datum\n",
    "datum = next(iter(dl_tr))[0][0].reshape(1, 3, 32, 32)\n",
    "\n",
    "# define the layer of which we are interested in displaying the features\n",
    "layer = pipe.model.conv2\n",
    "\n",
    "# we will test against this class\n",
    "class_ = 0\n",
    "\n",
    "# interpret the image\n",
    "output = inter.interpret(datum, class_, layer)\n",
    "\n",
    "# visualise the interpreter\n",
    "vs = Visualiser(pipe)\n",
    "try:\n",
    "    vs.plot_interpreter_image(inter)\n",
    "except AssertionError:\n",
    "    print(\"The heatmap is made of all zeros...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many techniques that we can try out for interpretability: here is an example with **Saliency maps**: the idea is to perturb the input datum and compute the gradients of the loss with repect to the perturbation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now use another model: Saliency maps\n",
    "inter2 = Interpreter(pipe.model, method=\"Saliency\")\n",
    "\n",
    "# interpret the mage\n",
    "output = inter2.interpret(datum, class_)\n",
    "\n",
    "# visualise the results\n",
    "vs = Visualiser(pipe)\n",
    "try:\n",
    "    vs.plot_interpreter_image(inter2)\n",
    "except AssertionError:\n",
    "    print(\"The heatmap is made of all zeros...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive 3d visualization\n",
    "\n",
    "You can visualise the dataset in 3D (choosing dynamically the dimensionality reduction algorithm) the dataset on tehsnorboard. In order to do so, just run teh next cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.plot_3d_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "More advanced stuff ahead!\n",
    "\n",
    "## Extract inner data from your models\n",
    "\n",
    "For deeper analysis of the working of deep learning models, it is possible, in giotto-deep, to easily extract tons of parameters from your models:\n",
    "Here below we show you how to extract:\n",
    " - each layer parameter,\n",
    " - the decision boundary,\n",
    " - the activation functions given an input datum,\n",
    " - the gradients at each node, given an input and a target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "me = ModelExtractor(pipe.model, loss_fn)\n",
    "\n",
    "list_of_layers = me.get_layers_param()\n",
    "\n",
    "for k, item in list_of_layers.items():\n",
    "    print(k, item.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the decision boundary will be available on tensorboard, in the projectors section.\n",
    "x = next(iter(dl_tr))[0][0]\n",
    "if x.dtype is not torch.int64:  # cannot backpropagate on integers!\n",
    "    res = me.get_decision_boundary(x, n_epochs=1)\n",
    "    res.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(dl_tr))[0]\n",
    "list_activations = me.get_activations(x)\n",
    "len(list_activations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl_tr))  # a whole batch!\n",
    "if batch[0].dtype is torch.float:  # cannot backpropagate on integers!\n",
    "    for gradient in me.get_gradients(batch)[1]:\n",
    "        print(gradient.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise activations and other topological aspects of your model\n",
    "\n",
    "Finally, we show you how it is possible to extract, layer per layer, given a certain input batch, the persistence diagrams computed on the activation space. This means that, for a fixed layer, the input batch is representated as a point cloud in the space of activations for that selected layer. Out of this point cloud we compute the persistence diagrams. In giotto-deep this is literally one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the diagrams can be seen on tensorboard!\n",
    "vs.plot_persistence_diagrams(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
