{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: HPO and benchmarking\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This tutorial is focused around **hyperparameter optimisation** (HPO), a rather unique feature of giotto-deep compared to other deep-learning frameworks.\n",
    "\n",
    "## Scope\n",
    "\n",
    "Neural network are very complex beasts and it is not at all intuitive to understand whether the change in some of the structural parameters would lead to an improved performance and higher stability. For example, is it always better to increase the depth of a feed-forward network? The answer is, in general, \"no\": it really depends on the complexity of your problem (*VC dimension and friends*). However, given that it is many time impossible to compute a priori quantities like the [VC dimension](https://en.wikipedia.org/wiki/Vapnikâ€“Chervonenkis_dimension), it is more effective to empirically try out different structural parameters and benchmark the results. This is what HPO is all about.\n",
    "\n",
    "## Plan for the tutorial\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. Creation of a dataset\n",
    " 2. Creation of a model\n",
    " 3. HPO on the initial model and dataset\n",
    " 4. Benchmarking\n",
    " 5. HPO-ing in each benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPUs...\n"
     ]
    }
   ],
   "source": [
    "# imports and notebook auto-reloader\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.models as models\n",
    "from gtda.diagrams import BettiCurve\n",
    "from gtda.plotting import plot_betti_surfaces\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import optuna\n",
    "from torch.optim import SGD, Adam, RMSprop\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from gdeep.data.preprocessors import ToTensorImage\n",
    "from gdeep.trainer import Trainer\n",
    "from gdeep.data.datasets import DatasetBuilder, DataLoaderBuilder\n",
    "\n",
    "# today's protagonists\n",
    "from gdeep.search import Benchmark\n",
    "from gdeep.search import HyperParameterOptimization, GiottoSummaryWriter\n",
    "from gdeep.models import FFNet\n",
    "from gdeep.visualization import persistence_diagrams_of_activations\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the results of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualization results.\n",
    "\n",
    "In this example, we use our modified version of the writer, as we believe it displays better results in the `hparams` dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = GiottoSummaryWriter()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset\n",
    "\n",
    "In the next cell we subsample the [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset and prepare the data loaders. Note that a preprocessing step is required to transformed the images into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/mehdi/Documents/giotto-deep/examples/data/DatasetCloud/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f111de632e42d3a6247677d0d11839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/mehdi/Documents/giotto-deep/examples/data/DatasetCloud/cifar-10-python.tar.gz to /home/mehdi/Documents/giotto-deep/examples/data/DatasetCloud\n"
     ]
    }
   ],
   "source": [
    "# download the dataset\n",
    "bd = DatasetBuilder(name=\"CIFAR10\")\n",
    "ds_tr, _, _ = bd.build(download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing steps\n",
    "\n",
    "transformation = ToTensorImage((32, 32))\n",
    "transformation.fit_to_dataset(ds_tr)  # this is useless for this transformation\n",
    "\n",
    "transformed_ds_tr = transformation.attach_transform_to_dataset(ds_tr)\n",
    "\n",
    "# use only 320 images from cifar10\n",
    "train_indices = list(range(32 * 10))\n",
    "dl_tr, *_ = DataLoaderBuilder((transformed_ds_tr,)).build(\n",
    "    ({\"batch_size\": 32, \"sampler\": SubsetRandomSampler(train_indices)},)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your model\n",
    "\n",
    "In the next section we build a torch model with a `str` parameter. The type of parameter can of course be changed to `int`: the example is to show the potential of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehdi/Documents/giotto-deep/venv/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning:\n",
      "\n",
      "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/mehdi/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349229b8c37d49f5a8d44323e21a7eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parametric model with string value\n",
    "class model2(nn.Module):\n",
    "    def __init__(self, n_nodes=\"100\"):\n",
    "        super(model2, self).__init__()\n",
    "        self.md = nn.Sequential(\n",
    "            nn.Sequential(\n",
    "                models.resnet18(weights=True), nn.Linear(1000, eval(n_nodes))\n",
    "            ),\n",
    "            nn.Linear(eval(n_nodes), 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.md(x)\n",
    "\n",
    "\n",
    "model = model2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training without HPO\n",
    "\n",
    "This step is the normal, non HPO, step that you would do to train your model. Starting from the next section on, we will dive into the HPO framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# initialise pipeline class\n",
    "pipe = Trainer(\n",
    "    model, [dl_tr], loss_fn, writer, k_fold_class=StratifiedKFold(2, shuffle=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehdi/Documents/giotto-deep/venv/lib/python3.8/site-packages/torch/cuda/__init__.py:497: UserWarning:\n",
      "\n",
      "Can't initialize NVML\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Fold  1 **************\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 2.713984 \tEpoch training accuracy: 11.25%                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.00100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehdi/Documents/giotto-deep/gdeep/trainer/trainer.py:665: UserWarning:\n",
      "\n",
      "Cannot store data in the PR curve\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " accuracy: 8.75%,                 Avg loss: 0.483993 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 2.645641 \tEpoch training accuracy: 11.25%                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " accuracy: 11.25%,                 Avg loss: 0.482924 \n",
      "\n",
      "\n",
      "\n",
      "********** Fold  2 **************\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 3.024296 \tEpoch training accuracy: 6.88%                                              \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " accuracy: 11.88%,                 Avg loss: 0.455216 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 2.949973 \tEpoch training accuracy: 7.50%                                              \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " accuracy: 11.25%,                 Avg loss: 0.465376 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.47414982318878174, 11.25)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following is a simple cross-validated training (no HPO)\n",
    "# we also add the n_accumulated_grads=5, which is useful to avoid OOM results when training on the GPU\n",
    "pipe.train(SGD, 2, True, {\"lr\": 0.001}, n_accumulated_grads=5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter Optimization\n",
    "\n",
    "One of the unique features of gotto-deep is the possibility to run advanced hyperparameters searches in a few lines of code: it is enough to define the dictionaries of hyperparameters, initialise the class `HyperParameterOptimization` and run it with the method `start`.\n",
    "We run a search over different hyperparameters: \n",
    " - the learning rate `lr`\n",
    " - the batch size `batch_size`\n",
    " - the network parameter `arch`\n",
    " \n",
    "The scope of the search is to find the optimum set of hyperparameters. The \"optimum\" depends on either the accuracy (or the user-defined metric) or the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-01 11:07:41,954] A new study created in memory with name: Y51KIKGNOIWL6IY9CEZZ\n",
      "/home/mehdi/Documents/giotto-deep/venv/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning:\n",
      "\n",
      "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "\n",
      "/home/mehdi/Documents/giotto-deep/gdeep/search/hpo.py:260: UserWarning:\n",
      "\n",
      "Model cannot be re-initialized. Using existing one.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 2.744484 \tEpoch training accuracy: 11.33%                                               \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.00430034\n",
      "Validation results: \n",
      " accuracy: 12.50%,                 Avg loss: 2.362726 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 2.415736 \tEpoch training accuracy: 17.19%                                               \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.00430034\n",
      "Validation results: \n",
      " accuracy: 14.06%,                 Avg loss: 2.338817 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Batch training loss:  2.103074789047241  \tBatch training accuracy:  23.4375  \t[ 3 / 4 ]                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-01 11:07:45,447] Trial 0 finished with value: 12.5 and parameters: {'optimizer': 'SGD', 'lr': 0.004300341923790494, 'batch_size': 64, 'n_nodes': '200'}. Best is trial 0 with value: 12.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 2.135614 \tEpoch training accuracy: 24.61%                                              \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.00430034\n",
      "Validation results: \n",
      " accuracy: 12.50%,                 Avg loss: 2.308360 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  model2 \n",
      "Model Hyperparameters: {'n_nodes': '200'}\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.004300341923790494\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.004300341923790494}\n",
      "Dataloader parameters: {'batch_size': 64}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: None\n",
      "Best Validation accuracy: 14.0625\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 9.706308 \tEpoch training accuracy: 13.54%                                                         \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.00537470\n",
      "Validation results: \n",
      " accuracy: 14.58%,                 Avg loss: 397.158386 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 16.331150 \tEpoch training accuracy: 9.38%                                                          \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.00537470\n",
      "Validation results: \n",
      " accuracy: 3.12%,                 Avg loss: 8410.487305 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 13.813317 \tEpoch training accuracy: 15.62%                                                         \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.00537470\n",
      "Validation results: \n",
      " accuracy: 6.25%,                 Avg loss: 45.579269 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  model2 \n",
      "Model Hyperparameters: {'n_nodes': '200'}\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.00537469988621401\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.00537469988621401}\n",
      "Dataloader parameters: {'batch_size': 48}\n",
      "LR-scheduler parameters: None\n",
      "Regularizer parameters: None\n",
      "Best Validation accuracy: 14.583333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-01 11:07:49,667] Trial 1 finished with value: 6.25 and parameters: {'optimizer': 'Adam', 'lr': 0.00537469988621401, 'batch_size': 48, 'n_nodes': '200'}. Best is trial 0 with value: 12.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "Number of pruned trials:  0\n",
      "Number of complete trials:  2\n",
      "******************** BEST TRIAL: ********************\n",
      "Metric Value for best trial:  12.5\n",
      "Parameters Values for best trial:  {'optimizer': 'SGD', 'lr': 0.004300341923790494, 'batch_size': 64, 'n_nodes': '200'}\n",
      "DateTime start of the best trial:  2023-12-01 11:07:41.955741\n"
     ]
    }
   ],
   "source": [
    "# initialise gridsearch\n",
    "search = HyperParameterOptimization(pipe, \"accuracy\", 2, best_not_last=True)\n",
    "\n",
    "# if you want to store pickle files of the models instead of the state_dicts\n",
    "search.store_pickle = True\n",
    "\n",
    "# dictionaries of hyperparameters\n",
    "optimizers_params = {\"lr\": [0.001, 0.01]}\n",
    "dataloaders_params = {\"batch_size\": [32, 64, 16]}\n",
    "models_hyperparams = {\"n_nodes\": [\"200\"]}\n",
    "\n",
    "# starting the HPO\n",
    "search.start(\n",
    "    (SGD, Adam),\n",
    "    3,\n",
    "    False,\n",
    "    optimizers_params,\n",
    "    dataloaders_params,\n",
    "    models_hyperparams,\n",
    "    n_accumulated_grads=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the best results we have found so far:  12.5 2.3083600997924805\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"These are the best results we have found so far: \",\n",
    "    search.best_val_acc_gs,\n",
    "    search.best_val_loss_gs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "Number of pruned trials:  0\n",
      "Number of complete trials:  2\n",
      "******************** BEST TRIAL: ********************\n",
      "Metric Value for best trial:  12.5\n",
      "Parameters Values for best trial:  {'optimizer': 'SGD', 'lr': 0.004300341923790494, 'batch_size': 64, 'n_nodes': '200'}\n",
      "DateTime start of the best trial:  2023-12-01 11:07:41.955741\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>n_nodes</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-12-01 11-07-41.955741</td>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>64</td>\n",
       "      <td>200</td>\n",
       "      <td>inf</td>\n",
       "      <td>12.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-12-01 11-07-45.448380</td>\n",
       "      <td>model</td>\n",
       "      <td>dataset</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>48</td>\n",
       "      <td>200</td>\n",
       "      <td>inf</td>\n",
       "      <td>6.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     run_name  model  dataset optimizer        lr  batch_size  \\\n",
       "0  2023-12-01 11-07-41.955741  model  dataset       SGD  0.004300          64   \n",
       "1  2023-12-01 11-07-45.448380  model  dataset      Adam  0.005375          48   \n",
       "\n",
       "  n_nodes  loss  accuracy  \n",
       "0     200   inf     12.50  \n",
       "1     200   inf      6.25  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the results\n",
    "df_res = search._results()\n",
    "df_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line in the dataframe with the top accuracy contains the optimum hyperparameters. You can visualise them interactively in the the `HPARAMS` of the tesorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-01 11:07:50,774] A new study created in memory with name: GPT6JJR0Q0IM8VCYDM6H\n",
      "/home/mehdi/Documents/giotto-deep/venv/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning:\n",
      "\n",
      "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "\n",
      "/home/mehdi/Documents/giotto-deep/gdeep/search/hpo.py:260: UserWarning:\n",
      "\n",
      "Model cannot be re-initialized. Using existing one.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehdi/Documents/giotto-deep/venv/lib/python3.8/site-packages/torch/utils/tensorboard/summary.py:386: DeprecationWarning:\n",
      "\n",
      "using `dtype=` in comparisons is only useful for `dtype=object` (and will do nothing for bool). This operation will fail in the future.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 2.930575 \tEpoch training accuracy: 17.19%                                              \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " accuracy: 20.31%,                 Avg loss: 2.687068 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehdi/Documents/giotto-deep/gdeep/trainer/trainer.py:665: UserWarning:\n",
      "\n",
      "Cannot store data in the PR curve\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch training loss:  1.6590203841527302  \tBatch training accuracy:  51.5625  \t[ 3 / 4 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-12-01 11:07:53,603] Trial 0 failed with parameters: {'optimizer': 'Adam', 'batch_size': 64, 'n_nodes': '200', 'gamma': 0.8580499933713335} because of the following error: RuntimeError('File state_dicts/Adam Parameter Group 0    amsgrad False    betas 0.9, 0.999    capturable False    differentiable False    eps 1e-08    foreach None    fused False    initial_lr 0.001    lr 0.0007362497911245454    maximize False    weight_decay 0-2023-12-01 11-07-50.775679.pth cannot be opened.').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mehdi/Documents/giotto-deep/venv/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/home/mehdi/Documents/giotto-deep/gdeep/search/hpo.py\", line 570, in <lambda>\n",
      "    lambda tr: self._objective(tr, config), n_trials=self.n_trials, timeout=None\n",
      "  File \"/home/mehdi/Documents/giotto-deep/gdeep/search/hpo.py\", line 371, in _objective\n",
      "    save_model_and_optimizer(\n",
      "  File \"/home/mehdi/Documents/giotto-deep/gdeep/utility/utils.py\", line 94, in save_model_and_optimizer\n",
      "    torch.save(\n",
      "  File \"/home/mehdi/Documents/giotto-deep/venv/lib/python3.8/site-packages/torch/serialization.py\", line 422, in save\n",
      "    with _open_zipfile_writer(f) as opened_zipfile:\n",
      "  File \"/home/mehdi/Documents/giotto-deep/venv/lib/python3.8/site-packages/torch/serialization.py\", line 309, in _open_zipfile_writer\n",
      "    return container(name_or_buffer)\n",
      "  File \"/home/mehdi/Documents/giotto-deep/venv/lib/python3.8/site-packages/torch/serialization.py\", line 287, in __init__\n",
      "    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))\n",
      "RuntimeError: File state_dicts/Adam Parameter Group 0    amsgrad False    betas 0.9, 0.999    capturable False    differentiable False    eps 1e-08    foreach None    fused False    initial_lr 0.001    lr 0.0007362497911245454    maximize False    weight_decay 0-2023-12-01 11-07-50.775679.pth cannot be opened.\n",
      "[W 2023-12-01 11:07:53,604] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 1.598727 \tEpoch training accuracy: 47.66%                                               \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.00085805\n",
      "Validation results: \n",
      " accuracy: 25.00%,                 Avg loss: 2.453447 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  model2 \n",
      "Model Hyperparameters: {'n_nodes': '200'}\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.0007362497911245454\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Optimizer parameters: None\n",
      "Dataloader parameters: {'batch_size': 64}\n",
      "LR-scheduler parameters: {'gamma': 0.8580499933713335}\n",
      "Regularizer parameters: None\n",
      "Best Validation accuracy: 25.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File state_dicts/Adam Parameter Group 0    amsgrad False    betas 0.9, 0.999    capturable False    differentiable False    eps 1e-08    foreach None    fused False    initial_lr 0.001    lr 0.0007362497911245454    maximize False    weight_decay 0-2023-12-01 11-07-50.775679.pth cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# starting the gridsearch, this time with a LR scheduler\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# here we wat to grid-search over the LR parameters as well!\u001b[39;00m\n\u001b[1;32m      4\u001b[0m schedulers_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.9\u001b[39m]}\n\u001b[0;32m----> 6\u001b[0m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mSGD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloaders_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloaders_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels_hyperparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels_hyperparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mExponentialLR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschedulers_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedulers_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/giotto-deep/gdeep/search/hpo.py:521\u001b[0m, in \u001b[0;36mHyperParameterOptimization.start\u001b[0;34m(self, optimizers, n_epochs, cross_validation, optimizers_params, dataloaders_params, models_hyperparams, lr_scheduler, schedulers_params, regularization_params, profiling, parallel_tpu, keep_training, store_grad_layer_hist, n_accumulated_grads, writer_tag)\u001b[0m\n\u001b[1;32m    500\u001b[0m config \u001b[38;5;241m=\u001b[39m HPOConfig(\n\u001b[1;32m    501\u001b[0m     optimizers,\n\u001b[1;32m    502\u001b[0m     n_epochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    515\u001b[0m     writer_tag,\n\u001b[1;32m    516\u001b[0m )\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_pipe:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;66;03m# in the __init__, self.model and self.dataloaders are\u001b[39;00m\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;66;03m# already initialized. So they exist also in _objective()\u001b[39;00m\n\u001b[0;32m--> 521\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_optimization_fun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregularizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    526\u001b[0m     _benchmarking_param(\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_optimization_fun,\n\u001b[1;32m    528\u001b[0m         (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    533\u001b[0m         config,\n\u001b[1;32m    534\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/giotto-deep/gdeep/search/hpo.py:569\u001b[0m, in \u001b[0;36mHyperParameterOptimization._inner_optimization_fun\u001b[0;34m(self, model, dataloaders, regularizers, config)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtr\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    571\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results(model_name\u001b[38;5;241m=\u001b[39mmodel[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m], dataset_name\u001b[38;5;241m=\u001b[39mdataloaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/giotto-deep/venv/lib/python3.8/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/giotto-deep/venv/lib/python3.8/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/giotto-deep/venv/lib/python3.8/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/Documents/giotto-deep/venv/lib/python3.8/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/Documents/giotto-deep/venv/lib/python3.8/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m~/Documents/giotto-deep/gdeep/search/hpo.py:570\u001b[0m, in \u001b[0;36mHyperParameterOptimization._inner_optimization_fun.<locals>.<lambda>\u001b[0;34m(tr)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39moptimize(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m tr: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trials, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    571\u001b[0m )\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results(model_name\u001b[38;5;241m=\u001b[39mmodel[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m], dataset_name\u001b[38;5;241m=\u001b[39mdataloaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/giotto-deep/gdeep/search/hpo.py:371\u001b[0m, in \u001b[0;36mHyperParameterOptimization._objective\u001b[0;34m(self, trial, config)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_output()\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# save model and optimizer\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m \u001b[43msave_model_and_optimizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime_start\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstore_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# save results to tensorboard\u001b[39;00m\n\u001b[1;32m    379\u001b[0m model_name, dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_model_and_dataset_name(\n\u001b[1;32m    380\u001b[0m     config\u001b[38;5;241m.\u001b[39mwriter_tag\n\u001b[1;32m    381\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/giotto-deep/gdeep/utility/utils.py:94\u001b[0m, in \u001b[0;36msave_model_and_optimizer\u001b[0;34m(model, model_name, trial_id, optimizer, store_pickle)\u001b[0m\n\u001b[1;32m     89\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m     90\u001b[0m             model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     91\u001b[0m             os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dicts\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m trial_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     92\u001b[0m         )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dicts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    104\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrial_id\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/giotto-deep/venv/lib/python3.8/site-packages/torch/serialization.py:422\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    419\u001b[0m _check_dill_version(pickle_module)\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    423\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/giotto-deep/venv/lib/python3.8/site-packages/torch/serialization.py:309\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/giotto-deep/venv/lib/python3.8/site-packages/torch/serialization.py:287\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_zipfile_writer_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: File state_dicts/Adam Parameter Group 0    amsgrad False    betas 0.9, 0.999    capturable False    differentiable False    eps 1e-08    foreach None    fused False    initial_lr 0.001    lr 0.0007362497911245454    maximize False    weight_decay 0-2023-12-01 11-07-50.775679.pth cannot be opened."
     ]
    }
   ],
   "source": [
    "# starting the gridsearch, this time with a LR scheduler\n",
    "\n",
    "# here we wat to grid-search over the LR parameters as well!\n",
    "schedulers_params = {\"gamma\": [0.5, 0.9]}\n",
    "\n",
    "search.start(\n",
    "    (SGD, Adam),\n",
    "    2,\n",
    "    False,\n",
    "    dataloaders_params=dataloaders_params,\n",
    "    models_hyperparams=models_hyperparams,\n",
    "    lr_scheduler=ExponentialLR,\n",
    "    schedulers_params=schedulers_params,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "\n",
    "Benchmarking means fixing a set of models and a set of datasets and trying all possible pairs of *(model, dataset)*. The most common usecase is actually to also fix the model and to run it over many datasets.\n",
    "\n",
    "Of course, only compatible models with compatiible datasets will be benchmarked.\n",
    "\n",
    "Just to clarify further: at this stage, there is no hyperparameter search involved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing multiple datasets\n",
    "\n",
    "Store your different dataloaders into a dictionary for benchmarking: `dataloaders_dicts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_dicts = []\n",
    "bd = DatasetBuilder(name=\"CIFAR10\")\n",
    "\n",
    "ds_tr, *_ = bd.build()\n",
    "transformation = ToTensorImage((32, 32))\n",
    "\n",
    "transformed_ds_tr = transformation.attach_transform_to_dataset(ds_tr)\n",
    "\n",
    "\n",
    "test_indices = [64 * 5 + x for x in range(32 * 3)]\n",
    "train_indices = [x for x in range(32 * 2)]\n",
    "\n",
    "dl = DataLoaderBuilder((transformed_ds_tr, transformed_ds_tr))\n",
    "dl_tr, dl_val, _ = dl.build(\n",
    "    (\n",
    "        {\"batch_size\": 32, \"sampler\": SubsetRandomSampler(train_indices)},\n",
    "        {\"batch_size\": 32, \"sampler\": SubsetRandomSampler(test_indices)},\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"CIFAR10_1000\"\n",
    "temp_dict[\"dataloaders\"] = (dl_tr, dl_val, _)\n",
    "\n",
    "dataloaders_dicts.append(temp_dict)\n",
    "\n",
    "db = DatasetBuilder(name=\"DoubleTori\")\n",
    "ds_tr, ds_val, _ = db.build()\n",
    "\n",
    "dl_tr, dl_ts, _ = DataLoaderBuilder((ds_tr, ds_val)).build(\n",
    "    ({\"batch_size\": 48}, {\"batch_size\": 32})\n",
    ")\n",
    "\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"double_tori\"\n",
    "temp_dict[\"dataloaders\"] = (dl_tr,)\n",
    "\n",
    "dataloaders_dicts.append(temp_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing multiple models\n",
    "Store your different models into a dictionary for benchmarking: `models_dicts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dicts = []\n",
    "\n",
    "model = model2()\n",
    "\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"resnet18\"\n",
    "temp_dict[\"model\"] = model\n",
    "\n",
    "models_dicts.append(temp_dict)\n",
    "\n",
    "# avoid having exposed paramters that wll not be gridsearched on\n",
    "class model_no_param(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model_no_param, self).__init__()\n",
    "        self.mod = FFNet([3, 5, 5, 2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mod(x)\n",
    "\n",
    "\n",
    "model5 = model_no_param()\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"ffnn\"\n",
    "temp_dict[\"model\"] = model5\n",
    "\n",
    "models_dicts.append(temp_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the benchmarking!\n",
    "\n",
    "After initialising the class with the dictionaries of models and dataloaders, we can run the actual benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the benchmarking class. When we do not specify it, it will use KFold with 5 splits\n",
    "bench = Benchmark(models_dicts, dataloaders_dicts, loss_fn, writer)\n",
    "\n",
    "# start the benchmarking\n",
    "bench.start(SGD, 2, False, {\"lr\": 0.01}, {\"batch_size\": 32}, n_accumulated_grads=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking + HyperParameter Optimization + CV\n",
    "\n",
    "In this last section we consider the possibility of running an HPO within each pair *(model, dataset)*.\n",
    "\n",
    "This can be achieved by initialising a benchmark class and use the benchmark as input for the gridsearch class.\n",
    "\n",
    "With these commands, we are basically looking for the best set of hyperparamets for each pair of *(model, dataset)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard pytorch loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# initialise benchmark\n",
    "bench = Benchmark(\n",
    "    models_dicts, dataloaders_dicts, loss_fn, writer, k_fold_class=KFold(3)\n",
    ")\n",
    "\n",
    "# initialise gridsearch with benchmark instance\n",
    "search2 = HyperParameterOptimization(bench, \"loss\", 2)\n",
    "\n",
    "# yperparameters\n",
    "optimizers_params = {\"lr\": [0.001, 0.01, None, True]}  # to have the log sampler\n",
    "dataloaders_params = {\"batch_size\": [32, 64, 16]}\n",
    "models_hyperparams = {\"n_nodes\": [\"500\", \"200\"]}\n",
    "search2.start(\n",
    "    (SGD, Adam), 2, True, optimizers_params, dataloaders_params, models_hyperparams\n",
    ")\n",
    "\n",
    "writer.close()  # let's not forget to close the tensorboard writer once all is done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Best validation accuracy: \",\n",
    "    search2.best_val_acc_gs,\n",
    "    \"\\nBest validation loss value: \",\n",
    "    search2.best_val_loss_gs,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom pruner and sampler\n",
    "\n",
    "It is possible to pass to the HyperParameterOptimization class a customer `optuna.Pruners` and `optuna.Samplers`.\n",
    "\n",
    "The pruner is used to stop a trial when it is clearly not reaching acceptable performances (hence sparing a bit of computational costs), while a sampler is used to sample the hyperparameter space using different techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# initialise te HPO\n",
    "gs = HyperParameterOptimization(\n",
    "    pipe,\n",
    "    \"accuracy\",\n",
    "    5,\n",
    "    best_not_last=False,\n",
    "    pruner=MedianPruner(\n",
    "        n_startup_trials=2, n_warmup_steps=0, interval_steps=1, n_min_trials=1\n",
    "    ),\n",
    "    sampler=TPESampler(),\n",
    ")\n",
    "\n",
    "# dictionaries of hyperparameters\n",
    "optimizers_params = {\"lr\": [0.001, 0.01]}\n",
    "dataloaders_params = {\"batch_size\": [32, 64, 16]}\n",
    "models_hyperparams = {\"n_nodes\": [\"500\", \"200\"]}\n",
    "\n",
    "# starting the HPO\n",
    "gs.start(\n",
    "    (SGD, Adam),\n",
    "    3,\n",
    "    False,\n",
    "    optimizers_params,\n",
    "    dataloaders_params,\n",
    "    models_hyperparams,\n",
    "    n_accumulated_grads=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
