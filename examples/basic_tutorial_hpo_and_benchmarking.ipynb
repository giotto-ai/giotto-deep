{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: HPO and benchmarking\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This tutorial is focussed around **hyperparameter optimisation** (HPO), a rather unique feature of giotto-deep compared to other deep-learning frameworks.\n",
    "\n",
    "## Scope\n",
    "\n",
    "Neural network are very complex beasts and it is not at all intuitive to understand whether the change in some of the structural parameters would lead to an improved performance and higher stability. For example, is it always better to increase the depth of a feed-forward network? The answer is, in general, \"no\": it really depends on the complexity of your problem (*VC dimension and friends*). However, given that it is many time impossible to compute a priori quantities like the [VC dimension](https://en.wikipedia.org/wiki/Vapnikâ€“Chervonenkis_dimension), it is more effective to empirically try out different structural parameters and benchmark the results. This is what HPO is all about.\n",
    "\n",
    "## Plan for the tutorial\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. creation of a dataset\n",
    " 2. creation of a model\n",
    " 3. HPO on the initial model and dataset\n",
    " 4. benchmarking\n",
    " 5. HPO-ing in each benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and notebook auto-reloader\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.models as models\n",
    "from gtda.diagrams import BettiCurve\n",
    "from gtda.plotting import plot_betti_surfaces\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import optuna\n",
    "from torch.optim import SGD, Adam, RMSprop\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from gdeep.data.preprocessors import ToTensorImage\n",
    "from gdeep.trainer import Trainer\n",
    "from gdeep.data.datasets import DatasetBuilder, DataLoaderBuilder\n",
    "\n",
    "# today's protagonists\n",
    "from gdeep.search import Benchmark\n",
    "from gdeep.search import HyperParameterOptimization, GiottoSummaryWriter\n",
    "from gdeep.models import FFNet\n",
    "from gdeep.visualisation import persistence_diagrams_of_activations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the reuslts of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualisation results.\n",
    "\n",
    "In this example, we use our modified version of the writer, as we believe it displays better results in the `hparams` dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = GiottoSummaryWriter()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset\n",
    "\n",
    "In the next cell we subsample the [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset and prepare the data loaders. Note that a preprocessing step is required to transformed the images into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "bd = DatasetBuilder(name=\"CIFAR10\")\n",
    "ds_tr, _, _ = bd.build(download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing steps\n",
    "\n",
    "transformation = ToTensorImage((32, 32))\n",
    "transformation.fit_to_dataset(ds_tr)  # this is useless for this transformation\n",
    "\n",
    "transformed_ds_tr = transformation.attach_transform_to_dataset(ds_tr)\n",
    "\n",
    "# use only 320 images from cifar10\n",
    "train_indices = list(range(32 * 10))\n",
    "dl_tr, *_ = DataLoaderBuilder((transformed_ds_tr,)).build(\n",
    "    ({\"batch_size\": 32, \"sampler\": SubsetRandomSampler(train_indices)},)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your model\n",
    "\n",
    "In the next section we build a torch model with a `str` parameter. The type of parameter can of course be changed to `int`: the example is to show the potential of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametric model with string value\n",
    "class model2(nn.Module):\n",
    "    def __init__(self, n_nodes=\"100\"):\n",
    "        super(model2, self).__init__()\n",
    "        self.md = nn.Sequential(\n",
    "            nn.Sequential(\n",
    "                models.resnet18(pretrained=True), nn.Linear(1000, eval(n_nodes))\n",
    "            ),\n",
    "            nn.Linear(eval(n_nodes), 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.md(x)\n",
    "\n",
    "\n",
    "model = model2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training without HPO\n",
    "\n",
    "This step is the normal, non HPO, step that you would do to train your model. Starting from the next section on, we will dive into the HPO framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# initialise pipeline class\n",
    "pipe = Trainer(\n",
    "    model, [dl_tr, None], loss_fn, writer, k_fold_class=StratifiedKFold(2, shuffle=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following is a simple cross-validated training (no HPO)\n",
    "# we also add the n_accumulated_grads=5, which is useful to avoid OOM results when training on the GPU\n",
    "pipe.train(SGD, 2, True, {\"lr\": 0.001}, n_accumulated_grads=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter Optimization\n",
    "\n",
    "One of the unique features of gotto-deep is the possibility to run advanced hyperparameters searches in a few lines of code: it is enough to define the dictionaries of hyperparameters, initialise the class `HyperParameterOptimization` and run it with the method `start`.\n",
    "We run a search over different hyperparametrs: \n",
    " - the learning rate `lr`\n",
    " - the batch size `batch_size`\n",
    " - the network parameter `arch`\n",
    " \n",
    "The scope of the search is to find the optimum set of hyperparameters. The \"optimum\" depends on either the accuracy (or the user-defined metric) or the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise gridsearch\n",
    "search = HyperParameterOptimization(pipe, \"accuracy\", 2, best_not_last=True)\n",
    "\n",
    "# if you want to store pickle files of the models instead of the state_dicts\n",
    "search.store_pickle = True\n",
    "\n",
    "# dictionaries of hyperparameters\n",
    "optimizers_params = {\"lr\": [0.001, 0.01]}\n",
    "dataloaders_params = {\"batch_size\": [32, 64, 16]}\n",
    "models_hyperparams = {\"n_nodes\": [\"200\"]}\n",
    "\n",
    "# starting the HPO\n",
    "search.start(\n",
    "    (SGD, Adam),\n",
    "    3,\n",
    "    False,\n",
    "    optimizers_params,\n",
    "    dataloaders_params,\n",
    "    models_hyperparams,\n",
    "    n_accumulated_grads=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"These are the best results we have found so far: \",\n",
    "    search.best_val_acc_gs,\n",
    "    search.best_val_loss_gs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the results\n",
    "df_res = search._results()\n",
    "df_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line in the dataframe with the top accuracy contains the optimum hyperparameters. You can visualise them interactively in the the `HPARAMS` of the tesorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the gridsearch, this time with a LR scheduler\n",
    "\n",
    "# here we wat to grid-search over the LR parameters as well!\n",
    "schedulers_params = {\"gamma\": [0.5, 0.9]}\n",
    "\n",
    "search.start(\n",
    "    (SGD, Adam),\n",
    "    2,\n",
    "    False,\n",
    "    dataloaders_params=dataloaders_params,\n",
    "    models_hyperparams=models_hyperparams,\n",
    "    lr_scheduler=ExponentialLR,\n",
    "    schedulers_params=schedulers_params,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "\n",
    "Benchmarking means fixing a set of models and a set of datasets and trying all possible pairs of *(model, dataset)*. The most common usecase is actually to also fix the model and to run it over many datasets.\n",
    "\n",
    "Of course, only compatible models with compatiible datasets will be benchmarked.\n",
    "\n",
    "Just to clarify further: at this stage, there is no hyperparameter search involved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing multiple datasets\n",
    "\n",
    "Store your different dataloaders into a dictionary for benchmarking: `dataloaders_dicts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_dicts = []\n",
    "bd = DatasetBuilder(name=\"CIFAR10\")\n",
    "\n",
    "ds_tr, *_ = bd.build()\n",
    "transformation = ToTensorImage((32, 32))\n",
    "\n",
    "transformed_ds_tr = transformation.attach_transform_to_dataset(ds_tr)\n",
    "\n",
    "\n",
    "test_indices = [64 * 5 + x for x in range(32 * 3)]\n",
    "train_indices = [x for x in range(32 * 2)]\n",
    "\n",
    "dl = DataLoaderBuilder((transformed_ds_tr, transformed_ds_tr))\n",
    "dl_tr, dl_val, _ = dl.build(\n",
    "    (\n",
    "        {\"batch_size\": 32, \"sampler\": SubsetRandomSampler(train_indices)},\n",
    "        {\"batch_size\": 32, \"sampler\": SubsetRandomSampler(test_indices)},\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"CIFAR10_1000\"\n",
    "temp_dict[\"dataloaders\"] = (dl_tr, dl_val, _)\n",
    "\n",
    "dataloaders_dicts.append(temp_dict)\n",
    "\n",
    "db = DatasetBuilder(name=\"DoubleTori\")\n",
    "ds_tr, ds_val, _ = db.build()\n",
    "\n",
    "dl_tr, dl_ts, _ = DataLoaderBuilder((ds_tr, ds_val)).build(\n",
    "    ({\"batch_size\": 48}, {\"batch_size\": 32})\n",
    ")\n",
    "\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"double_tori\"\n",
    "temp_dict[\"dataloaders\"] = (dl_tr, dl_ts)\n",
    "\n",
    "dataloaders_dicts.append(temp_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing multiple models\n",
    "Store your different models into a dictionary for benchmarking: `models_dicts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dicts = []\n",
    "\n",
    "model = model2()\n",
    "\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"resnet18\"\n",
    "temp_dict[\"model\"] = model\n",
    "\n",
    "models_dicts.append(temp_dict)\n",
    "\n",
    "# avoid having exposed paramters that wll not be gridsearched on\n",
    "class model_no_param(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model_no_param, self).__init__()\n",
    "        self.mod = FFNet([3, 5, 5, 2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mod(x)\n",
    "\n",
    "\n",
    "model5 = model_no_param()\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"ffnn\"\n",
    "temp_dict[\"model\"] = model5\n",
    "\n",
    "models_dicts.append(temp_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the benchmarking!\n",
    "\n",
    "After initialising the class with the dictionaries of models and dataloaders, we can run the actual benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the benchmarking class. When we do not specify it, it will use KFold with 5 splits\n",
    "bench = Benchmark(models_dicts, dataloaders_dicts, loss_fn, writer)\n",
    "\n",
    "# start the benchmarking\n",
    "bench.start(SGD, 2, False, {\"lr\": 0.01}, {\"batch_size\": 32}, n_accumulated_grads=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking + HyperParameter Optimization + CV\n",
    "\n",
    "In this last section we consider the possibility of running an HPO within each pair *(model, dataset)*.\n",
    "\n",
    "This can be achieved by initialising a benchmark class and use the benchmark as input for the gridsearch class.\n",
    "\n",
    "With these commands, we are basically looking for the best set of hyperparamets for each pair of *(model, dataset)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard pytorch loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# initialise benchmark\n",
    "bench = Benchmark(\n",
    "    models_dicts, dataloaders_dicts, loss_fn, writer, k_fold_class=KFold(3)\n",
    ")\n",
    "\n",
    "# initialise gridsearch with benchmark instance\n",
    "search2 = HyperParameterOptimization(bench, \"loss\", 2)\n",
    "\n",
    "# yperparameters\n",
    "optimizers_params = {\"lr\": [0.001, 0.01, None, True]}  # to have the log sampler\n",
    "dataloaders_params = {\"batch_size\": [32, 64, 16]}\n",
    "models_hyperparams = {\"n_nodes\": [\"500\", \"200\"]}\n",
    "search2.start(\n",
    "    (SGD, Adam), 2, True, optimizers_params, dataloaders_params, models_hyperparams\n",
    ")\n",
    "\n",
    "writer.close()  # let's not forget to close the tensorboard writer once all is done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Best validation accuracy: \",\n",
    "    search2.best_val_acc_gs,\n",
    "    \"\\nBest validation loss value: \",\n",
    "    search2.best_val_loss_gs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom pruner and sampler\n",
    "\n",
    "It is possible to pass to the HyperParameterOptimization class a customer `optuna.Pruners` and `optuna.Samplers`.\n",
    "\n",
    "The pruner is used to stop a trial when it is clearly not rach acceptable performances (hence spearing a bit of computational costs), while a sampler is used to sample the hyperparameter space using different techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# initialise te HPO\n",
    "gs = HyperParameterOptimization(\n",
    "    pipe,\n",
    "    \"accuracy\",\n",
    "    5,\n",
    "    best_not_last=False,\n",
    "    pruner=MedianPruner(\n",
    "        n_startup_trials=2, n_warmup_steps=0, interval_steps=1, n_min_trials=1\n",
    "    ),\n",
    "    sampler=TPESampler(),\n",
    ")\n",
    "\n",
    "# dictionaries of hyperparameters\n",
    "optimizers_params = {\"lr\": [0.001, 0.01]}\n",
    "dataloaders_params = {\"batch_size\": [32, 64, 16]}\n",
    "models_hyperparams = {\"n_nodes\": [\"500\", \"200\"]}\n",
    "\n",
    "# starting the HPO\n",
    "gs.start(\n",
    "    (SGD, Adam),\n",
    "    3,\n",
    "    False,\n",
    "    optimizers_params,\n",
    "    dataloaders_params,\n",
    "    models_hyperparams,\n",
    "    n_accumulated_grads=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
